<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kafka on rmoff&#39;s random ramblings</title>
    <link>https://rmoff.github.io/tags/kafka/</link>
    <description>Recent content in Kafka on rmoff&#39;s random ramblings</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Oct 2018 15:13:59 +0000</lastBuildDate>
    
	<atom:link href="https://rmoff.github.io/tags/kafka/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Flatten CDC records in KSQL</title>
      <link>https://rmoff.github.io/2018/10/11/flatten-cdc-records-in-ksql/</link>
      <pubDate>Thu, 11 Oct 2018 15:13:59 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/10/11/flatten-cdc-records-in-ksql/</guid>
      <description>The problem - nested messages in Kafka Data comes into Kafka in many shapes and sizes. Sometimes it&amp;rsquo;s from CDC tools, and may be nested like this:
{ &amp;quot;SCN&amp;quot;: 12206116841348, &amp;quot;SEG_OWNER&amp;quot;: &amp;quot;KFKUSER&amp;quot;, &amp;quot;TABLE_NAME&amp;quot;: &amp;quot;CDCTAB2&amp;quot;, &amp;quot;TIMESTAMP&amp;quot;: 1539162785000, &amp;quot;SQL_REDO&amp;quot;: &amp;quot;insert into \&amp;quot;KFKUSER\&amp;quot;.\&amp;quot;CDCTAB2\&amp;quot;(\&amp;quot;ID\&amp;quot;,\&amp;quot;CITY\&amp;quot;,\&amp;quot;NATIONALITY\&amp;quot;) values (634789,&#39;AHMEDABAD&#39;,&#39;INDIA&#39;)&amp;quot;, &amp;quot;OPERATION&amp;quot;: &amp;quot;INSERT&amp;quot;, &amp;quot;data&amp;quot;: { &amp;quot;value&amp;quot;: { &amp;quot;ID&amp;quot;: 634789, &amp;quot;CITY&amp;quot;: { &amp;quot;string&amp;quot;: &amp;quot;AHMEDABAD&amp;quot; }, &amp;quot;NATIONALITY&amp;quot;: { &amp;quot;string&amp;quot;: &amp;quot;INDIA&amp;quot; } } }, &amp;quot;before&amp;quot;: null }  Note that the &amp;lsquo;payload&amp;rsquo; is nested under data-&amp;gt;value.</description>
    </item>
    
    <item>
      <title>Streaming geopoint data from Kafka to Elasticsearch</title>
      <link>https://rmoff.github.io/2018/10/05/streaming-geopoint-data-from-kafka-to-elasticsearch/</link>
      <pubDate>Fri, 05 Oct 2018 15:22:51 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/10/05/streaming-geopoint-data-from-kafka-to-elasticsearch/</guid>
      <description>Using the Elasticsearch Kafka Connect connector to stream events from a Kafka topic to Elasticsearch.
curl -X &amp;quot;POST&amp;quot; &amp;quot;http://kafka-connect:8083/connectors/&amp;quot; \ -H &amp;quot;Content-Type: application/json&amp;quot; \ -d &#39;{ &amp;quot;name&amp;quot;: &amp;quot;es_sink_ATM_POSSIBLE_FRAUD&amp;quot;, &amp;quot;config&amp;quot;: { &amp;quot;topics&amp;quot;: &amp;quot;ATM_POSSIBLE_FRAUD&amp;quot;, &amp;quot;key.converter&amp;quot;: &amp;quot;org.apache.kafka.connect.storage.StringConverter&amp;quot;, &amp;quot;value.converter&amp;quot;: &amp;quot;org.apache.kafka.connect.json.JsonConverter&amp;quot;, &amp;quot;value.converter.schemas.enable&amp;quot;: false, &amp;quot;connector.class&amp;quot;: &amp;quot;io.confluent.connect.elasticsearch.ElasticsearchSinkConnector&amp;quot;, &amp;quot;key.ignore&amp;quot;: &amp;quot;true&amp;quot;, &amp;quot;schema.ignore&amp;quot;: &amp;quot;true&amp;quot;, &amp;quot;type.name&amp;quot;: &amp;quot;type.name=kafkaconnect&amp;quot;, &amp;quot;topic.index.map&amp;quot;: &amp;quot;ATM_POSSIBLE_FRAUD:atm_possible_fraud&amp;quot;, &amp;quot;connection.url&amp;quot;: &amp;quot;http://elasticsearch:9200&amp;quot; } }&#39;  Dynamic mapping setup in Elasticsearch (before running the Connector) to force columns to a given type:
curl -XPUT &amp;quot;http://elasticsearch:9200/_template/kafkaconnect/&amp;quot; -H &#39;Content-Type: application/json&#39; -d&#39; { &amp;quot;index_patterns&amp;quot;: &amp;quot;*&amp;quot;, &amp;quot;settings&amp;quot;: { &amp;quot;number_of_shards&amp;quot;: 1, &amp;quot;number_of_replicas&amp;quot;: 0 }, &amp;quot;mappings&amp;quot;: { &amp;quot;_default_&amp;quot;: { &amp;quot;dynamic_templates&amp;quot;: [ { &amp;quot;dates&amp;quot;: { &amp;quot;match&amp;quot;: &amp;quot;*TIMESTAMP&amp;quot;, &amp;quot;mapping&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;date&amp;quot; } } }, { &amp;quot;geopoint&amp;quot;: { &amp;quot;match&amp;quot;: &amp;quot;*LOCATION&amp;quot;, &amp;quot;mapping&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;geo_point&amp;quot; } } }, { &amp;quot;geopoint2&amp;quot;: { &amp;quot;match&amp;quot;: &amp;quot;location&amp;quot;, &amp;quot;mapping&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;geo_point&amp;quot; } } }, { &amp;quot;non_analysed_string_template&amp;quot;: { &amp;quot;match&amp;quot;: &amp;quot;account_id, atm, transaction_id&amp;quot;, &amp;quot;match_mapping_type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;mapping&amp;quot;: { &amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot; } } } ] } } }&#39;  Sample JSON message from Kafka: (pretty-printed)</description>
    </item>
    
    <item>
      <title>Exploring JMX with jmxterm</title>
      <link>https://rmoff.github.io/2018/09/19/exploring-jmx-with-jmxterm/</link>
      <pubDate>Wed, 19 Sep 2018 08:11:00 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/09/19/exploring-jmx-with-jmxterm/</guid>
      <description>Check out the jmxterm repository / Download jmxterm from http://wiki.cyclopsgroup.org/jmxterm/
Launch:
java -jar ~/Downloads/jmxterm-1.0.0-uber.jar --url localhost:30002  You can pass the jmx host/port directly, or use the open command once jmxterm launches.
Once connected, use domains to list available domains
$&amp;gt;domains #following domains are available JMImplementation com.sun.management io.confluent.ksql.metrics io.confluent.rest java.lang java.nio java.util.logging kafka.admin.client kafka.consumer kafka.producer kafka.streams [...]  Switch to a particular domain:
$&amp;gt;domain io.confluent.ksql.metrics #domain is set to io.confluent.ksql.metrics  List the available MBeans in a the selected domain (you can also run this without choosing a domain first, to see every MBean, but it&amp;rsquo;s a long list):</description>
    </item>
    
    <item>
      <title>Accessing Kafka Docker containers&#39; JMX from host</title>
      <link>https://rmoff.github.io/2018/09/17/accessing-kafka-docker-containers-jmx-from-host/</link>
      <pubDate>Mon, 17 Sep 2018 15:29:48 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/09/17/accessing-kafka-docker-containers-jmx-from-host/</guid>
      <description>To help future Googlers‚Ä¶ with the Confluent docker images for Kafka, KSQL, Kafka Connect, etc, if you want to access JMX metrics from within, you just need to pass two environment variables:
 KSQL_JMX_HOSTNAME - the hostname/IP of the host machine. This is used by the JMX client to connect back into JMX, so must be accessible from the host machine running the JMX client. If you&amp;rsquo;re just running your JMX client locally on the Docker host, you can set this to 127.</description>
    </item>
    
    <item>
      <title>Sending multiline messages to Kafka</title>
      <link>https://rmoff.github.io/2018/09/04/sending-multiline-messages-to-kafka/</link>
      <pubDate>Tue, 04 Sep 2018 08:26:51 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/09/04/sending-multiline-messages-to-kafka/</guid>
      <description>(SO answer repost)
You can use kafkacat to send messages to Kafka that include line breaks. To do this, use its -D operator to specify a custom message delimiter (in this example /):
kafkacat -b kafka:29092 \ -t test_topic_01 \ -D/ \ -P &amp;lt;&amp;lt;EOF this is a string message with a line break/this is another message with two line breaks! EOF  Note that the delimiter must be a single byte - multi-byte chars will end up getting included in the resulting message See issue #140</description>
    </item>
    
    <item>
      <title>Kafka Listeners - Explained</title>
      <link>https://rmoff.github.io/2018/08/02/kafka-listeners-explained/</link>
      <pubDate>Thu, 02 Aug 2018 19:38:00 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/08/02/kafka-listeners-explained/</guid>
      <description>This question comes up on StackOverflow and such places a lot, so here&amp;rsquo;s something to try and help.
tl;dr : You need to set advertised.listeners (or KAFKA_ADVERTISED_LISTENERS if you&amp;rsquo;re using Docker images) to the external address (host/IP) so that clients can correctly connect to it. Otherwise they&amp;rsquo;ll try to connect to the internal host address‚Äìand if that&amp;rsquo;s not reachable then problems ensue.
In this post I&amp;rsquo;ll talk about why this is necessary, and then show how to do it, based on a couple of scenarios - Docker, and AWS.</description>
    </item>
    
    <item>
      <title>Kafka Listeners - Explained</title>
      <link>https://rmoff.github.io/2018/08/02/kafka-listeners-explained/</link>
      <pubDate>Thu, 02 Aug 2018 19:38:00 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/08/02/kafka-listeners-explained/</guid>
      <description>This question comes up on StackOverflow and such places a lot, so here&amp;rsquo;s something to try and help.
tl;dr : You need to set advertised.listeners (or KAFKA_ADVERTISED_LISTENERS if you&amp;rsquo;re using Docker images) to the external address (host/IP) so that clients can correctly connect to it. Otherwise they&amp;rsquo;ll try to connect to the internal host address‚Äìand if that&amp;rsquo;s not reachable then problems ensue.
In this post I&amp;rsquo;ll talk about why this is necessary, and then show how to do it, based on a couple of scenarios - Docker, and AWS.</description>
    </item>
    
    <item>
      <title>Quick &#39;n Easy Population of Realistic Test Data into Kafka</title>
      <link>https://rmoff.github.io/2018/05/10/quick-n-easy-population-of-realistic-test-data-into-kafka/</link>
      <pubDate>Thu, 10 May 2018 12:56:00 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/05/10/quick-n-easy-population-of-realistic-test-data-into-kafka/</guid>
      <description>tl;dr Use curl to pull data from the Mockaroo REST endpoint, and pipe it into kafkacat, thus:
curl -s &amp;quot;https://api.mockaroo.com/api/d5a195e0?count=2&amp;amp;key=ff7856d0&amp;quot;| \ kafkacat -b localhost:9092 -t purchases -P  Three things I love‚Ä¶Kafka, kafkacat, and Mockaroo. And in this post I get to show all three üòÅ
Mockaroo is a very cool online service that lets you quickly mock up test data. What sets it apart from SELECT RANDOM(100) FROM DUMMY; is that it has lots of different classes of test data for you to choose from.</description>
    </item>
    
    <item>
      <title>Streaming Data from MySQL into Kafka with Kafka Connect and Debezium</title>
      <link>https://rmoff.github.io/2018/03/24/streaming-data-from-mysql-into-kafka-with-kafka-connect-and-debezium/</link>
      <pubDate>Sat, 24 Mar 2018 14:58:14 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/03/24/streaming-data-from-mysql-into-kafka-with-kafka-connect-and-debezium/</guid>
      <description>Debezium is a CDC tool that can stream changes from MySQL, MongoDB, and PostgreSQL into Kafka, using Kafka Connect. In this article we&amp;rsquo;ll see how to set it up and examine the format of the data. A subsequent article will show using this realtime stream of data from a RDBMS and join it to data originating from other sources, using KSQL.
The software versions used here are:
 Confluent Platform 4.</description>
    </item>
    
    <item>
      <title>Streaming data from Kafka into Elasticsearch</title>
      <link>https://rmoff.github.io/2018/03/06/streaming-data-from-kafka-into-elasticsearch/</link>
      <pubDate>Tue, 06 Mar 2018 22:21:00 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/03/06/streaming-data-from-kafka-into-elasticsearch/</guid>
      <description>This article is part of a series exploring Streaming ETL in practice. You can read about setting up the ingest of realtime events from a standard Oracle platform, and building streaming ETL using KSQL.
This post shows how we take data streaming in from an Oracle transactional system into Kafka, and simply stream it onwards into Elasticsearch. This is a common pattern, for enabling rapid search or analytics against data held in systems elsewhere.</description>
    </item>
    
    <item>
      <title>Installing the Python Kafka library from Confluent - troubleshooting some silly errors‚Ä¶</title>
      <link>https://rmoff.github.io/2018/03/06/installing-the-python-kafka-library-from-confluent-troubleshooting-some-silly-errors/</link>
      <pubDate>Tue, 06 Mar 2018 22:18:24 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/03/06/installing-the-python-kafka-library-from-confluent-troubleshooting-some-silly-errors/</guid>
      <description>System:
rmoff@proxmox01:~$ uname -a Linux proxmox01 4.4.6-1-pve #1 SMP Thu Apr 21 11:25:40 CEST 2016 x86_64 GNU/Linux rmoff@proxmox01:~$ head -n1 /etc/os-release PRETTY_NAME=&amp;quot;Debian GNU/Linux 8 (jessie)&amp;quot; rmoff@proxmox01:~$ python --version Python 2.7.9  Following:
 https://www.confluent.io/blog/introduction-to-apache-kafka-for-python-programmers/ https://github.com/confluentinc/confluent-kafka-python  Install librdkafka, which is a pre-req for the Python library:
wget -qO - https://packages.confluent.io/deb/4.0/archive.key | sudo apt-key add - sudo add-apt-repository &amp;quot;deb [arch=amd64] https://packages.confluent.io/deb/4.0 stable main&amp;quot; sudo apt-get install librdkafka-dev python-dev  Setup virtualenv:</description>
    </item>
    
    <item>
      <title>Kafka - AdminClient - Connection to node -1 could not be established. Broker may not be available</title>
      <link>https://rmoff.github.io/2018/01/03/kafka-adminclient-connection-to-node-1-could-not-be-established.-broker-may-not-be-available/</link>
      <pubDate>Wed, 03 Jan 2018 11:26:00 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2018/01/03/kafka-adminclient-connection-to-node-1-could-not-be-established.-broker-may-not-be-available/</guid>
      <description>See also Kafka Listeners - Explained
A short post to help Googlers. On a single-node sandbox Apache Kafka / Confluent Platform installation, I was getting this error from Schema Registry, Connect, etc:
WARN [AdminClient clientId=adminclient-3] Connection to node -1 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)  KSQL was throwing a similar error:
KSQL cannot initialize AdminCLient.  I had correctly set the machine&amp;rsquo;s hostname in my Kafka server.</description>
    </item>
    
    <item>
      <title>Installing Oracle GoldenGate for Big Data 12.3.1 with Kafka Connect and Confluent Platform</title>
      <link>https://rmoff.github.io/2017/11/21/installing-oracle-goldengate-for-big-data-12.3.1-with-kafka-connect-and-confluent-platform/</link>
      <pubDate>Tue, 21 Nov 2017 17:31:00 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2017/11/21/installing-oracle-goldengate-for-big-data-12.3.1-with-kafka-connect-and-confluent-platform/</guid>
      <description>Some notes that I made on installing and configuring Oracle GoldenGate with Confluent Platform. Excuse the brevity, but hopefully useful to share!
I used the Oracle Developer Days VM for this - it&amp;rsquo;s preinstalled with Oracle 12cR2. Big Data Lite is nice but currently has an older version of GoldenGate.
Login to the VM (oracle/oracle) and then install some useful things:
sudo rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum install -y screen htop collectl rlwrap p7zip unzip sysstat perf iotop sudo su - cd /etc/yum.</description>
    </item>
    
    <item>
      <title>Apache Kafka‚Ñ¢ talks at Oracle OpenWorld, JavaOne, and Oak Table World 2017</title>
      <link>https://rmoff.github.io/2017/09/20/apache-kafka-talks-at-oracle-openworld-javaone-and-oak-table-world-2017/</link>
      <pubDate>Wed, 20 Sep 2017 15:46:00 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2017/09/20/apache-kafka-talks-at-oracle-openworld-javaone-and-oak-table-world-2017/</guid>
      <description>There&amp;rsquo;s an impressive 19 sessions that cover Apache Kafka‚Ñ¢ at Oracle OpenWorld, JavaOne, and Oak Table World this year! You can find the full list with speakers in the session catalogs for OOW, JavaOne, and Oak Table World. OTW is an awesome techie conference which is at the same time as OpenWorld, next door to Moscone. Hope to see you there!
Check out the writeup of my previous visit to OOW including useful tips here.</description>
    </item>
    
    <item>
      <title>Configuring Kafka Connect to log REST HTTP messages to a separate file</title>
      <link>https://rmoff.github.io/2017/06/12/configuring-kafka-connect-to-log-rest-http-messages-to-a-separate-file/</link>
      <pubDate>Mon, 12 Jun 2017 15:28:15 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2017/06/12/configuring-kafka-connect-to-log-rest-http-messages-to-a-separate-file/</guid>
      <description>Kafka&amp;rsquo;s Connect API is a wondrous way of easily bringing data in and out of Apache Kafka without having to write a line of code. By choosing a Connector from the many available, it&amp;rsquo;s possible to set up and end-to-end data pipeline with just a few lines of configuration. You can configure this by hand, or you can use the Confluent Control Center, for both management and monitoring:
BUT &amp;hellip; there are times when not all goes well - perhaps your source has gone offline, or one of your targets has been misconfigured.</description>
    </item>
    
    <item>
      <title>kafka.common.KafkaException: No key found on line 1</title>
      <link>https://rmoff.github.io/2017/05/12/kafka.common.kafkaexception-no-key-found-on-line-1/</link>
      <pubDate>Fri, 12 May 2017 00:52:41 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2017/05/12/kafka.common.kafkaexception-no-key-found-on-line-1/</guid>
      <description>A very silly PEBCAK problem this one, but Google hits weren&amp;rsquo;t so helpful so here goes.
Running a console producer, specifying keys:
kafka-console-producer \ --broker-list localhost:9092 \ --topic test_topic \ --property parse.key=true \ --property key.seperator=,  Failed when I entered a key/value:
1,foo kafka.common.KafkaException: No key found on line 1: 1,foo at kafka.tools.ConsoleProducer$LineMessageReader.readMessage(ConsoleProducer.scala:314) at kafka.tools.ConsoleProducer$.main(ConsoleProducer.scala:55) at kafka.tools.ConsoleProducer.main(ConsoleProducer.scala)  kafka.common.KafkaException: No key found on line &amp;hellip; but I specified the key, didn&amp;rsquo;t I?</description>
    </item>
    
    <item>
      <title>kafka-avro-console-producer - Error registering Avro schema / io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException</title>
      <link>https://rmoff.github.io/2016/12/02/kafka-avro-console-producer-error-registering-avro-schema-io.confluent.kafka.schemaregistry.client.rest.exceptions.restclientexception/</link>
      <pubDate>Fri, 02 Dec 2016 11:35:57 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2016/12/02/kafka-avro-console-producer-error-registering-avro-schema-io.confluent.kafka.schemaregistry.client.rest.exceptions.restclientexception/</guid>
      <description>By default, the kafka-avro-console-producer will assume that the schema registry is on port 8081, and happily connect to it. Unfortunately, this can lead to some weird errors if another process happens to be listening on port 8081 already!
[oracle@bigdatalite tmp]$ kafka-avro-console-producer \ &amp;gt; --broker-list localhost:9092 --topic kudu_test \ &amp;gt; --property value.schema=&#39;{&amp;quot;type&amp;quot;:&amp;quot;record&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;myrecord&amp;quot;,&amp;quot;fields&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;id&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;int&amp;quot;},{&amp;quot;name&amp;quot;:&amp;quot;random_field&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;}]}&#39; {&amp;quot;id&amp;quot;: 999, &amp;quot;random_field&amp;quot;: &amp;quot;foo&amp;quot;} org.apache.kafka.common.errors.SerializationException: Error registering Avro schema: {&amp;quot;type&amp;quot;:&amp;quot;record&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;myrecord&amp;quot;,&amp;quot;fields&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;id&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;int&amp;quot;},{&amp;quot;name&amp;quot;:&amp;quot;random_field&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;string&amp;quot;}]} Caused by: io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException: Unexpected character (&#39;&amp;lt;&#39; (code 60)): expected a valid value (number, String, array, object, &#39;true&#39;, &#39;false&#39; or &#39;null&#39;) at [Source: sun.</description>
    </item>
    
    <item>
      <title>Kafka Connect - java.lang.IncompatibleClassChangeError</title>
      <link>https://rmoff.github.io/2016/11/24/kafka-connect-java.lang.incompatibleclasschangeerror/</link>
      <pubDate>Thu, 24 Nov 2016 20:58:44 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2016/11/24/kafka-connect-java.lang.incompatibleclasschangeerror/</guid>
      <description>I hit this error running Kafka Connect HDFS connector from Confluent Platform v3.1.1 on BigDataLite 4.6:
[oracle@bigdatalite ~]$ connect-standalone /etc/schema-registry/connect-avro-standalone.properties /etc/kafka-connect-hdfs/quickstart-hdfs.properties [...] Exception in thread &amp;quot;main&amp;quot; java.lang.IncompatibleClassChangeError: Implementing class at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:467) at java.net.URLClassLoader.access$100(URLClassLoader.java:73) at java.net.URLClassLoader$1.run(URLClassLoader.java:368) at java.net.URLClassLoader$1.run(URLClassLoader.java:362) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:361) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763)  The fix was to unset the CLASSPATH first:
unset CLASSPATH  </description>
    </item>
    
    <item>
      <title>OGG-15051 oracle.goldengate.util.GGException:  Class not found: &#34;kafkahandler&#34;</title>
      <link>https://rmoff.github.io/2016/07/29/ogg-15051-oracle.goldengate.util.ggexception-class-not-found-kafkahandler/</link>
      <pubDate>Fri, 29 Jul 2016 07:47:30 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2016/07/29/ogg-15051-oracle.goldengate.util.ggexception-class-not-found-kafkahandler/</guid>
      <description>Similar to the previous issue, the sample config in the docs causes another snafu:
OGG-15051 Java or JNI exception: oracle.goldengate.util.GGException: Class not found: &amp;quot;kafkahandler&amp;quot;. kafkahandler Class not found: &amp;quot;kafkahandler&amp;quot;. kafkahandler  This time it&amp;rsquo;s in the kafka.props file:
gg.handler.kafkahandler.Type = kafka  Should be
gg.handler.kafkahandler.type = kafka  No capital T in Type!
(Image credit: https://unsplash.com/@vanschneider)</description>
    </item>
    
    <item>
      <title>OGG -  Class not found: &#34;com.company.kafka.CustomProducerRecord&#34;</title>
      <link>https://rmoff.github.io/2016/07/28/ogg-class-not-found-com.company.kafka.customproducerrecord/</link>
      <pubDate>Thu, 28 Jul 2016 16:34:37 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2016/07/28/ogg-class-not-found-com.company.kafka.customproducerrecord/</guid>
      <description>In the documentation for the current release of Oracle GoldenGate for Big Data (12.2.0.1.1.011) there&amp;rsquo;s a helpful sample configuration, which isn&amp;rsquo;t so helpful &amp;hellip;
[...] gg.handler.kafkahandler.ProducerRecordClass = com.company.kafka.CustomProducerRecord [...]  This value for gg.handler.kafkahandler.ProducerRecordClass will cause a failure when you start the replicat:
[...] Class not found: &amp;quot;com.company.kafka.CustomProducerRecord&amp;quot; [...]  If you comment this configuration item out, it&amp;rsquo;ll use the default (oracle.goldengate.handler.kafka.DefaultProducerRecord) and work swimingly!
(Image credit: https://unsplash.com/@vanschneider)</description>
    </item>
    
    <item>
      <title>Kafka Connect JDBC - Oracle - Number of groups must be positive</title>
      <link>https://rmoff.github.io/2016/07/27/kafka-connect-jdbc-oracle-number-of-groups-must-be-positive/</link>
      <pubDate>Wed, 27 Jul 2016 15:23:14 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2016/07/27/kafka-connect-jdbc-oracle-number-of-groups-must-be-positive/</guid>
      <description>There are various reasons for this error, but the one I hit was that the table name is case sensitive, and returned from Oracle by the JDBC driver in uppercase.
If you specify the tablename in your connecter config in lowercase, it won&amp;rsquo;t be matched, and this error is thrown. You can validate this by setting debug logging (edit etc/kafka/connect-log4j.properties to set log4j.rootLogger=DEBUG, stdout), and observe: (I&amp;rsquo;ve truncated some of the output for legibility)</description>
    </item>
    
    <item>
      <title>Kafka Connect - HDFS with Hive Integration - SchemaProjectorException - Schema version required</title>
      <link>https://rmoff.github.io/2016/07/19/kafka-connect-hdfs-with-hive-integration-schemaprojectorexception-schema-version-required/</link>
      <pubDate>Tue, 19 Jul 2016 14:36:52 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2016/07/19/kafka-connect-hdfs-with-hive-integration-schemaprojectorexception-schema-version-required/</guid>
      <description>I&amp;rsquo;ve been doing some noodling around with Confluent&amp;rsquo;s Kafka Connect recently, as part of gaining a wider understanding into Kafka. If you&amp;rsquo;re not familiar with Kafka Connect this page gives a good idea of the thinking behind it.
One issue that I hit defeated my Google-fu so I&amp;rsquo;m recording it here to hopefully help out fellow n00bs.
The pipeline that I&amp;rsquo;d set up looked like this:
 Eneco&amp;rsquo;s Twitter Source streaming tweets to a Kafka topic Confluent&amp;rsquo;s HDFS Sink to stream tweets to HDFS and define Hive table automagically over them  It worked great, but only if I didn&amp;rsquo;t enable the Hive integration part.</description>
    </item>
    
    <item>
      <title>Streaming Data through Oracle GoldenGate to Elasticsearch</title>
      <link>https://rmoff.github.io/2016/04/14/streaming-data-through-oracle-goldengate-to-elasticsearch/</link>
      <pubDate>Thu, 14 Apr 2016 22:51:43 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2016/04/14/streaming-data-through-oracle-goldengate-to-elasticsearch/</guid>
      <description>Recently added to the oracledi project over at java.net is an adaptor enabling Oracle GoldenGate (OGG) to send data to Elasticsearch. This adds a powerful alternative to [micro-]batch extract via JDBC from Oracle to Elasticsearch, which I wrote about recently over at the Elastic blog.
Elasticsearch is a &amp;lsquo;document store&amp;rsquo; widely used for both search and analytics. It&amp;rsquo;s something I&amp;rsquo;ve written a lot about (here and here for archives), as well as spoken about - preaching the good word, as it were, since the Elastic stack as a whole is very very good at what it does and a pleasure to work with.</description>
    </item>
    
    <item>
      <title>Decoupling the Data Pipeline with Kafka - A (Very) Simple Real Life Example</title>
      <link>https://rmoff.github.io/2016/04/12/decoupling-the-data-pipeline-with-kafka-a-very-simple-real-life-example/</link>
      <pubDate>Tue, 12 Apr 2016 21:50:46 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2016/04/12/decoupling-the-data-pipeline-with-kafka-a-very-simple-real-life-example/</guid>
      <description>I&amp;rsquo;ve recently been playing around with the ELK stack (now officially known as the Elastic stack) collecting data from an IRC channel with Elastic&amp;rsquo;s Logstash, storing it in Elasticsearch and analysing it with Kibana. But, this isn&amp;rsquo;t an &amp;ldquo;ELK&amp;rdquo; post - this is a Kafka post! ELK is just some example data manipulation tooling that helps demonstrate the principles.
As I wrote about last year, Apache Kafka provides a handy way to build flexible &amp;ldquo;pipelines&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>Fun and Games with Oracle GoldenGate, Kafka, and Logstash on BigDataLite 4.4</title>
      <link>https://rmoff.github.io/2016/03/16/fun-and-games-with-oracle-goldengate-kafka-and-logstash-on-bigdatalite-4.4/</link>
      <pubDate>Wed, 16 Mar 2016 22:01:00 +0000</pubDate>
      
      <guid>https://rmoff.github.io/2016/03/16/fun-and-games-with-oracle-goldengate-kafka-and-logstash-on-bigdatalite-4.4/</guid>
      <description>The Oracle by Example (ObE) here demonstrating how to use Goldengate to replicate transactions big data targets such as HDFS is written for the BigDataLite 4.2.1, and for me didn&amp;rsquo;t work on the current latest version, 4.4.0.
The OBE (and similar Hands On Lab PDF) assume the presence of pmov.prm and pmov.properties in /u01/ogg/dirprm/. On BDL 4.4 there&amp;rsquo;s only the extract to from Oracle configuration, emov.
Fortunately it&amp;rsquo;s still possible to run this setup out of the box in BDL 4.</description>
    </item>
    
    <item>
      <title>Presentations and Talks</title>
      <link>https://rmoff.github.io/presentations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://rmoff.github.io/presentations/</guid>
      <description>You can find most of my slide decks on Speaker Deck. Below are links to particular talks I&amp;rsquo;ve done recently, along with recordings where available.
Embrace the Anarchy: Apache Kafka&amp;rsquo;s Role in Modern Data Architectures  üìΩ Recording  Devoxx Belgium  Rated 32 out of all the conference talks Scored 4.31&amp;frasl;5, 134 votes  Oracle CODE London  üìñ Slides  Apache Kafka and KSQL in Action : Let‚Äôs Build a Streaming Data Pipeline!</description>
    </item>
    
  </channel>
</rss>