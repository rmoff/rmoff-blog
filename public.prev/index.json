[{"categories":["Kafka Connect"],"content":"By default Kafka Connect sends its output to stdout, so you\u0026#8217;ll see it on the console, Docker logs, or wherever. Sometimes you might want to route it to file, and you can do this by reconfiguring log4j. You can also change the configuration to get more (or less) detail in the logs by changing the log level.\n Finding the log configuration file The configuration file is called connect-log4j.properties and usually found in etc/kafka/connect-log4j.","keywords":null,"title":"Kafka Connect Change Log Level and Write Log to File","uri":"https://rmoff.github.io/post/kafka-connect-change-log-level-and-write-log-to-file/"},{"categories":["bash"],"content":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eA script I\u0026#8217;d batch-run on my Markdown files had inserted a UTF-8 non-breaking-space between Markdown heading indicator and the text, which meant that \u003ccode\u003e\u003cmark\u003e#\u003c/mark\u003e My title\u003c/code\u003e actually got rendered as that, instead of an H3 title.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eLooking at the file contents, I could see it wasn\u0026#8217;t just a space between the \u003ccode\u003e#\u003c/code\u003e and the text, but a non-breaking space.\u003c/p\u003e\n\u003c/div\u003e","keywords":null,"title":"Replacing UTF8 non-breaking-space with bash/sed on the Mac","uri":"https://rmoff.github.io/post/replacing-utf8-non-breaking-space-with-bash-sed-on-the-mac/"},{"categories":["ksql"],"content":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003e\u003ca href=\"https://www.confluent.io/ksql\"\u003eKSQL\u003c/a\u003e is generally case-sensitive. Very sensitive, at times ;-)\u003c/p\u003e\n\u003c/div\u003e","keywords":null,"title":"How KSQL handles case","uri":"https://rmoff.github.io/post/how-ksql-handles-case/"},{"categories":["KSQL","rest"],"content":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eFull reference is \u003ca href=\"https://docs.confluent.io/current/ksql/docs/developer-guide/api.html\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e","keywords":null,"title":"KSQL REST API cheatsheet","uri":"https://rmoff.github.io/post/ksql-rest-api-cheatsheet/"},{"categories":["rest","Schema Registry"],"content":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe \u003ca href=\"https://docs.confluent.io/current/schema-registry/docs/index.html\"\u003eSchema Registry\u003c/a\u003e support a \u003ca href=\"https://docs.confluent.io/current/schema-registry/docs/api.html\"\u003eREST API\u003c/a\u003e for finding out information about the schemas within it. Here\u0026#8217;s a quick cheatsheat with REST calls that I often use.\u003c/p\u003e\n\u003c/div\u003e","keywords":null,"title":"Confluent Schema Registry REST API cheatsheet","uri":"https://rmoff.github.io/post/schema-registry-cheatsheet/"},{"categories":["docker"],"content":"I use Docker and Docker Compose a lot. Like, every day. It\u0026#8217;s a fantastic way to build repeatable demos and examples, that can be torn down and spun up in a repeatable way. But‚Ä¶what happens when the demo that was working is spun up and then tail spins down in a blaze of flames?\n Here\u0026#8217;s the excerpt of a log from my Oracle container:\n $ docker-compose up -d ; docker-compose logs -f oracle Recreating connect-jdbc_oracle_1_4be0ad4479f8 .","keywords":null,"title":"What to Do When Docker on the Mac Runs Out of Space","uri":"https://rmoff.github.io/post/what-to-do-when-docker-runs-out-of-space/"},{"categories":["abstracts","conferences"],"content":"I\u0026#8217;ve reviewed a bunch of abstracts in the last couple of days, here are some common suggestions I made:\n   No need to include your company name in the abstract text. Chances are I\u0026#8217;ve not heard of your company, and even if I have, what does it add to my comprehension of your abstract and what you\u0026#8217;re going to talk about? Possible exception would be the \"hot\" tech companies where people will see a talk just because it\u0026#8217;s Netflix etc","keywords":null,"title":"Quick Thoughts on Not Writing a Crap Abstract","uri":"https://rmoff.github.io/post/quick-thoughts-on-not-writing-a-crap-abstract/"},{"categories":["blogging","ghost","hugo","Markdown","asciidoc"],"content":"Why? I\u0026#8217;ve been blogging for quite a few years now, starting on Blogger, soon onto WordPress, and then to Ghost a couple of years ago. Blogger was fairly lame, WP yucky, but I really do like Ghost. It\u0026#8217;s simple and powerful and was perfect for my needs. My needs being, an outlet for technical content that respected formatting, worked with a markup language (Markdown), and didn\u0026#8217;t f**k things up in the way that WP often would in its WYSIWYG handling of content.","keywords":null,"title":"Moving from Ghost to Hugo","uri":"https://rmoff.github.io/2018/12/17/moving-from-ghost-to-hugo/"},{"categories":["docker"],"content":"Tiny little snippet this one. Given a list of images:\n $ docker images|grep confluent confluentinc/cp-enterprise-kafka 5.0.0 d0c5528d7f99 3 months ago 600MB confluentinc/cp-kafka 5.0.0 373a4e31e02e 3 months ago 558MB confluentinc/cp-zookeeper 5.0.0 3cab14034c43 3 months ago 558MB confluentinc/cp-ksql-server 5.0.0 691bc3c1991f 4 months ago 493MB confluentinc/cp-ksql-cli 5.0.0 e521f3e787d6 4 months ago 488MB ‚Ä¶  Now there\u0026#8217;s a new version available, and you want to pull down all the latest ones for it:","keywords":null,"title":"Pull new version of multiple Docker images","uri":"https://rmoff.github.io/post/docker-pull-new-version/"},{"categories":["docker","Docker Compose","ksql","ksql-cli","ksql-server","kafka connect"],"content":"A few years ago a colleague of mine told me about this thing called Docker, and I must admit I dismissed it as a fad‚Ä¶how wrong was I. Docker, and Docker Compose, are one of my key tools of the trade. With them I can build self-contained environments for tutorials, demos, conference talks etc. Tear it down, run it again, without worrying that somewhere a local config changed and will break things.","keywords":null,"title":"Docker Tips and Tricks with KSQL and Kafka","uri":"https://rmoff.github.io/2018/12/15/docker-tips-and-tricks-with-ksql-and-kafka/"},{"categories":["oracle","cdc","debezium","goldengate","xstream","logminer","flashback","licence","ksql"],"content":"This is a short summary discussing what the options are for integrating Oracle RDBMS into Kafka, as of December 2018. For a more detailed background to why and how at a broader level for all databases (not just Oracle) see this blog and these slides.\nWhat techniques \u0026amp; tools are there? As of December 2018, this is what the line-up looks like:\n Query-based CDC  The JDBC Connector for Kafka Connect, polls the database for new or changed data based on an incrementing ID column and/or update timestamp  Log-based CDC","keywords":null,"title":"Streaming data from Oracle into Kafka (December 2018)","uri":"https://rmoff.github.io/2018/12/12/streaming-data-from-oracle-into-kafka-december-2018/"},{"categories":["ipad pro","apple pencil","apple keyboard","keynote"],"content":"I\u0026rsquo;ve written recently about how I create the diagrams in my blog posts and talks, and from discussions around that, a couple of people were interested more broadly in how I use my iPad Pro. So, on the basis that if two people are interested maybe others are (and if no-one else is, I have a copy-and-paste answer to give to those two people) here we go.\nKit  iPad Pro 10.","keywords":null,"title":"Tools I Use: iPad Pro","uri":"https://rmoff.github.io/2018/12/11/tools-i-use-ipad-pro/"},{"categories":["paper","ios","diagrams","presenting","ipad","tools"],"content":"I write and speak lots about Kafka, and get a fair few questions from this. The most common question is actually nothing to do with Kafka, but instead:\n How do you make those cool diagrams?\n So here\u0026rsquo;s a short, and longer, answer!\ntl;dr An iOS app called Paper, from a company called FiftyThree\nSo, how DO you make those cool diagrams? Disclaimer: This is a style that I have copied straight from my esteemed colleagues at Confluent, including Neha Narkhede and Ben Stopford, as well as others including Martin Kleppmann.","keywords":null,"title":"So how DO you make those cool diagrams?","uri":"https://rmoff.github.io/2018/12/10/so-how-do-you-make-those-cool-diagrams/"},{"categories":["mtr","mac","brew"],"content":" Install Not sure why the brew doesn\u0026rsquo;t work as it used to, but here\u0026rsquo;s how to get it working:\nbrew install mtr sudo ln /usr/local/Cellar/mtr/0.92/sbin/mtr /usr/local/bin/mtr sudo ln /usr/local/Cellar/mtr/0.92/sbin/mtr-packet /usr/local/bin/mtr-packet  (If you don\u0026rsquo;t do the two symbolic links (ln) you\u0026rsquo;ll get mtr: command not found or mtr: Failure to start mtr-packet: Invalid argument)\nRun sudo mtr google.com  ","keywords":null,"title":"Get mtr working on the Mac","uri":"https://rmoff.github.io/2018/12/08/get-mtr-working-on-the-mac/"},{"categories":["kafka connect","bash","jq","peco","xargs","rest api"],"content":"I do lots of work with Kafka Connect, almost entirely in Distributed mode‚Äîeven just with 1 node -\u0026gt; makes scaling out much easier when/if needed. Because I\u0026rsquo;m using Distributed mode, I use the Kafka Connect REST API to configure and manage it. Whilst others might use GUI REST tools like Postman etc, I tend to just use the commandline. Here are some useful snippets that I use all the time.","keywords":null,"title":"Kafka Connect CLI tricks","uri":"https://rmoff.github.io/2018/12/03/kafka-connect-cli-tricks/"},{"categories":["oracle","docker","sudo","root"],"content":"\u003cp\u003etl;dr:\u003c/p\u003e\n\n\u003cdiv class=\"highlight\"\u003e\u003cpre style=\";-moz-tab-size:4;-o-tab-size:4;tab-size:4\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003edocker \u003cspan style=\"color:#008000\"\u003eexec\u003c/span\u003e --interactive \u003cspan style=\"color:#b62;font-weight:bold\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#b62;font-weight:bold\"\u003e\u003c/span\u003e            --tty \u003cspan style=\"color:#b62;font-weight:bold\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#b62;font-weight:bold\"\u003e\u003c/span\u003e            --user root \u003cspan style=\"color:#b62;font-weight:bold\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#b62;font-weight:bold\"\u003e\u003c/span\u003e            --workdir / \u003cspan style=\"color:#b62;font-weight:bold\"\u003e\\\n\u003c/span\u003e\u003cspan style=\"color:#b62;font-weight:bold\"\u003e\u003c/span\u003e            oracle-container-name bash\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","keywords":null,"title":"Logging in as root on Oracle Database Docker image","uri":"https://rmoff.github.io/2018/11/30/logging-in-as-root-on-oracle-database-docker-image/"},{"categories":["docker","docker-compose"],"content":"\u003cp\u003eDoing some funky Docker Compose stuff, including:\u003c/p\u003e","keywords":null,"title":"ERROR: Invalid interpolation format for \u0026ldquo;command\u0026rdquo; option in service‚Ä¶","uri":"https://rmoff.github.io/2018/11/20/error-invalid-interpolation-format-for-command-option-in-service/"},{"categories":["cdc","ksql","kafka","jdbc sink"],"content":"\u003ch3 id=\"the-problem-nested-messages-in-kafka\"\u003eThe problem - nested messages in Kafka\u003c/h3\u003e\n\n\u003cp\u003eData comes into Kafka in many shapes and sizes. Sometimes it\u0026rsquo;s from CDC tools, and may be nested like this:\u003c/p\u003e","keywords":null,"title":"Flatten CDC records in KSQL","uri":"https://rmoff.github.io/2018/10/11/flatten-cdc-records-in-ksql/"},{"categories":["elasticsearch","kafka","kafkaconnect","geopoint"],"content":"\u003cp\u003eUsing the \u003ca href=\"https://www.confluent.io/connector/kafka-connect-elasticsearch/\"\u003eElasticsearch Kafka Connect connector\u003c/a\u003e to stream events from a Kafka topic to Elasticsearch.\u003c/p\u003e","keywords":null,"title":"Streaming geopoint data from Kafka to Elasticsearch","uri":"https://rmoff.github.io/2018/10/05/streaming-geopoint-data-from-kafka-to-elasticsearch/"},{"categories":["kafka","ksql","jmx","jmxterm"],"content":"\u003cp\u003eCheck out the \u003ca href=\"https://github.com/jiaqi/jmxterm/\"\u003ejmxterm repository\u003c/a\u003e / Download jmxterm from \u003ca href=\"http://wiki.cyclopsgroup.org/jmxterm/\"\u003ehttp://wiki.cyclopsgroup.org/jmxterm/\u003c/a\u003e\u003c/p\u003e","keywords":null,"title":"Exploring JMX with jmxterm","uri":"https://rmoff.github.io/2018/09/19/exploring-jmx-with-jmxterm/"},{"categories":["jmx","kafka","docker"],"content":"See also docs.\nTo help future Googlers‚Ä¶ with the Confluent docker images for Kafka, KSQL, Kafka Connect, etc, if you want to access JMX metrics from within, you just need to pass two environment variables: \u0026lt;x\u0026gt;_JMX_HOSTNAME and \u0026lt;x\u0026gt;_JMX_PORT, prefixed by a component name.\n \u0026lt;x\u0026gt;_JMX_HOSTNAME - the hostname/IP of the JMX host machine, as accessible from the JMX Client.\nThis is used by the JMX client to connect back into JMX, so must be accessible from the host machine running the JMX client.","keywords":null,"title":"Accessing Kafka Docker containers\u0026rsquo; JMX from host","uri":"https://rmoff.github.io/2018/09/17/accessing-kafka-docker-containers-jmx-from-host/"},{"categories":["kafkacat","kafka","multiline"],"content":"(SO answer repost)\nYou can use kafkacat to send messages to Kafka that include line breaks. To do this, use its -D operator to specify a custom message delimiter (in this example /):\nkafkacat -b kafka:29092 \\ -t test_topic_01 \\ -D/ \\ -P \u0026lt;\u0026lt;EOF this is a string message with a line break/this is another message with two line breaks! EOF  Note that the delimiter must be a single byte - multi-byte chars will end up getting included in the resulting message See issue #140","keywords":null,"title":"Sending multiline messages to Kafka","uri":"https://rmoff.github.io/2018/09/04/sending-multiline-messages-to-kafka/"},{"categories":["ksql","window","aggregate","timestamp","elasticsearch","kibana"],"content":"KSQL provides the ability to create windowed aggregations. For example, count the number of messages in a 1 minute window, grouped by a particular column:\nCREATE TABLE RATINGS_BY_CLUB_STATUS AS \\ SELECT CLUB_STATUS, COUNT(*) AS RATING_COUNT \\ FROM RATINGS_WITH_CUSTOMER_DATA \\ WINDOW TUMBLING (SIZE 1 MINUTES) \\ GROUP BY CLUB_STATUS;  How KSQL, and Kafka Streams, stores the window timestamp associated with an aggregate, has recently changed. See #1497 for details.\nWhereas previously the Kafka message timestamp (accessible through the KSQL ROWTIME system column) stored the start of the window for which the aggregate had been calculated, this changed in July 2018 to instead be the timestamp of the latest message to update that aggregate value.","keywords":null,"title":"Window Timestamps in KSQL / Integration with Elasticsearch","uri":"https://rmoff.github.io/2018/09/03/window-timestamps-in-ksql-integration-with-elasticsearch/"},{"categories":["adoc","asciidoc","ms word","docx","pandoc","markdown"],"content":"Short and sweet this one. I\u0026rsquo;ve written in the past how I love Markdown but I\u0026rsquo;ve actually moved on from that and now firmly throw my hat in the AsciiDoc ring. I\u0026rsquo;ll write another post another time explaining why in more detail, but in short it\u0026rsquo;s just more powerful whilst still simple and readable without compilation.\nSo anyway, I use AsciiDoc (ADOC) for all my technical (and often non-technical) writing now, and from there usually dump it out to HTML which I can share with people as needed:","keywords":null,"title":"Converting from AsciiDoc to MS Word","uri":"https://rmoff.github.io/2018/08/22/converting-from-asciidoc-to-ms-word/"},{"categories":["speaking","schedule","calendar","javazone","lisa","usenix","devoxx"],"content":" There\u0026rsquo;s lots going on in the next few months :-)\nI\u0026rsquo;m particularly excited to be speaking at several notable conferences for the first time, including JavaZone, USENIX LISA, and Devoxx.\nAs always, if you\u0026rsquo;re nearby then hope to see you there, and let me know if you want to meet for a coffee or beer!\nSeptember üá™üá∏ Madrid, Spain  6th Sept: Madrid Kafka Meetup  üá≥üá¥ Oslo, Norway  10th Sept: Oslo Kafka Meetup 11th Sept: JavaZone  üáßüá™ Antwerp/Brussels, Belgium  25th Sept: Brussels Kafka Meetup  üá™üá∏ Barcelona, Spain  26th Sept: Barcelona Kafka Meetup  October üá¨üáß Leeds, UK  4th Oct: The JVM Thing meetup  üá∫üá∏ Nashville (TN), USA  31st Oct: LISA18  November üá©üá™ M√ºnich, Germany  7th Nov: W-JAX  üáßüá™ Antwerp, Belgium  13th Nov: Devoxx Belgium  üáµüá± Krakow, Poland  26th Nov: CoreDump  December üá¨üáß Liverpool, UK  4th Dec: UKOUG TECH 18  üá©üá™ Frankfurt, Germany  10th Dec: Apache Kafka Meetup 11th Dec: IT Days  ","keywords":null,"title":"Where I\u0026rsquo;m speaking in the rest of 2018","uri":"https://rmoff.github.io/2018/08/21/where-im-speaking-in-the-rest-of-2018/"},{"categories":["apache kafka","kafka","docker","advertised.listeners","listeners","aws","ec2","KAFKA_ADVERTISED_LISTENERS"],"content":"This question comes up on StackOverflow and such places a lot, so here\u0026rsquo;s something to try and help.\ntl;dr : You need to set advertised.listeners (or KAFKA_ADVERTISED_LISTENERS if you\u0026rsquo;re using Docker images) to the external address (host/IP) so that clients can correctly connect to it. Otherwise they\u0026rsquo;ll try to connect to the internal host address‚Äìand if that\u0026rsquo;s not reachable then problems ensue.\nIn this post I\u0026rsquo;ll talk about why this is necessary, and then show how to do it, based on a couple of scenarios - Docker, and AWS.","keywords":null,"title":"Kafka Listeners - Explained","uri":"https://rmoff.github.io/2018/08/02/kafka-listeners-explained/"},{"categories":["apache kafka","kafka","docker","advertised.listeners","listeners","aws","ec2","KAFKA_ADVERTISED_LISTENERS"],"content":"This question comes up on StackOverflow and such places a lot, so here\u0026rsquo;s something to try and help.\ntl;dr : You need to set advertised.listeners (or KAFKA_ADVERTISED_LISTENERS if you\u0026rsquo;re using Docker images) to the external address (host/IP) so that clients can correctly connect to it. Otherwise they\u0026rsquo;ll try to connect to the internal host address‚Äìand if that\u0026rsquo;s not reachable then problems ensue.\nIn this post I\u0026rsquo;ll talk about why this is necessary, and then show how to do it, based on a couple of scenarios - Docker, and AWS.","keywords":null,"title":"Kafka Listeners - Explained","uri":"https://rmoff.github.io/2018/08/02/kafka-listeners-explained/"},{"categories":["pygments","pygmentize","jq","syntax highlighting","keynote","presenting","asciinema"],"content":"So you\u0026rsquo;ve got a code sample you want to share in a presentation, but whilst it looks beautiful in your text-editor with syntax highlighting, it\u0026rsquo;s fugly in Keynote? You could screenshot it and paste the image into your slide, but you just know that you\u0026rsquo;ll want to change that code, and end up re-snapshotting it‚Ä¶what a PITA.\nBetter to have a nicely syntax-highlighted code snippet that you can paste as formatted text into Keynote and amend from there as needed.","keywords":null,"title":"Syntax highlighting code for presentation slides","uri":"https://rmoff.github.io/2018/06/20/syntax-highlighting-code-for-presentation-slides/"},{"categories":["elasticsearch","ksql","apache kafka","ubiquiti","espressi","slack","python","stream processing"],"content":"In this article I demonstrated how to use KSQL to filter streams of network event data. As well as filtering, KSQL can be used to easily enrich streams. In this article we\u0026rsquo;ll see how this enriched data can be used to drive analysis in Elasticsearch and Kibana‚Äîand how KSQL again came into use for building some stream processing as a result of the discovery made.\nThe data came from my home Ubiquiti router, and took two forms:","keywords":null,"title":"Analysing Network Data with Apache Kafka, KSQL, and Elasticsearch","uri":"https://rmoff.github.io/2018/06/17/analysing-network-data-with-apache-kafka-ksql-and-elasticsearch/"},{"categories":["diff","patch"],"content":"Hacky way to keep config files in sync when there\u0026rsquo;s a new version of some software.\nCaveat : probably completely wrong, may not pick up config entries added in the new version, etc. But, works for me right here right now ;-)\nSo let\u0026rsquo;s say we have two folders:\nconfluent-4.1.0 confluent-4.1.1  Same structures, different versions. 4.1.0 was set up with our local config in ./etc, that we want to preserve.","keywords":null,"title":"Compare and apply a diff / patch recursively","uri":"https://rmoff.github.io/2018/06/07/compare-and-apply-a-diff-patch-recursively/"},{"categories":["kafka connect","oracle","number","timestamp"],"content":"The Kafka Connect JDBC Connector by default does not cope so well with:\n NUMBER columns with no defined precision/scale. You may end up with apparent junk (bytes) in the output, or just errors. TIMESTAMP WITH LOCAL TIME ZONE. Throws JDBC type -102 not currently supported warning in the log.  Read more about NUMBER data type in the Oracle docs.\ntl;dr : How do I make it work? There are several options:","keywords":null,"title":"Kafka Connect and Oracle data types","uri":"https://rmoff.github.io/2018/05/21/kafka-connect-and-oracle-data-types/"},{"categories":["ksql","stream","table","join"],"content":"(preserving this StackOverflow answer for posterity and future Googlers)\ntl;dr When doing a stream-table join, your table messages must already exist (and must be timestamped) before the stream messages. If you re-emit your source stream messages, after the table topic is populated, the join will succeed.\nExample data Use kafakcat to populate topics:\nkafkacat -b localhost:9092 -P -t sessionDetails \u0026lt;\u0026lt;EOF {\u0026quot;Media\u0026quot;:\u0026quot;Foo\u0026quot;,\u0026quot;SessionIdTime\u0026quot;:\u0026quot;2018-05-17 11:25:33 BST\u0026quot;,\u0026quot;SessionIdSeq\u0026quot;:1} {\u0026quot;Media\u0026quot;:\u0026quot;Foo\u0026quot;,\u0026quot;SessionIdTime\u0026quot;:\u0026quot;2018-05-17 11:26:33 BST\u0026quot;,\u0026quot;SessionIdSeq\u0026quot;:2} EOF kafkacat -b localhost:9092 -P -t voipDetails \u0026lt;\u0026lt;EOF {\u0026quot;SessionIdTime\u0026quot;:\u0026quot;2018-05-17 11:25:33 BST\u0026quot;,\u0026quot;SessionIdSeq\u0026quot;:1,\u0026quot;Details\u0026quot;:\u0026quot;Bar1a\u0026quot;} {\u0026quot;SessionIdTime\u0026quot;:\u0026quot;2018-05-17 11:25:33 BST\u0026quot;,\u0026quot;SessionIdSeq\u0026quot;:1,\u0026quot;Details\u0026quot;:\u0026quot;Bar1b\u0026quot;} {\u0026quot;SessionIdTime\u0026quot;:\u0026quot;2018-05-17 11:26:33 BST\u0026quot;,\u0026quot;SessionIdSeq\u0026quot;:2,\u0026quot;Details\u0026quot;:\u0026quot;Bar2\u0026quot;} EOF  Validate topic contents:","keywords":null,"title":"Stream-Table Joins in KSQL: Stream events must be timestamped after the Table messages","uri":"https://rmoff.github.io/2018/05/17/stream-table-joins-in-ksql-stream-events-must-be-timestamped-after-the-table-messages/"},{"categories":["kafka","kafkacat","mockaroo","testing"],"content":"tl;dr Use curl to pull data from the Mockaroo REST endpoint, and pipe it into kafkacat, thus:\ncurl -s \u0026quot;https://api.mockaroo.com/api/d5a195e0?count=2\u0026amp;key=ff7856d0\u0026quot;| \\ kafkacat -b localhost:9092 -t purchases -P  Three things I love‚Ä¶Kafka, kafkacat, and Mockaroo. And in this post I get to show all three üòÅ\nMockaroo is a very cool online service that lets you quickly mock up test data. What sets it apart from SELECT RANDOM(100) FROM DUMMY; is that it has lots of different classes of test data for you to choose from.","keywords":null,"title":"Quick \u0026lsquo;n Easy Population of Realistic Test Data into Kafka","uri":"https://rmoff.github.io/2018/05/10/quick-n-easy-population-of-realistic-test-data-into-kafka/"},{"categories":["mongodb","debezium","kafka connect","apache kafka","replica set"],"content":"Disclaimer: I am not a MongoDB person. These steps may or may not be appropriate and proper. But they worked for me :) Feel free to post in comments if I\u0026rsquo;m doing something wrong\nMongoDB config - enabling replica sets For Debezium to be able to stream changes from MongoDB, Mongo needs to have replication configured:\nDocs: Replication / Convert a Standalone to a Replica Set\nStop Mongo:\nrmoff@proxmox01 ~\u0026gt; sudo service mongod stop  Add replica set config to /etc/mongod.","keywords":null,"title":"Streaming Data from MongoDB into Kafka with Kafka Connect and Debezium","uri":"https://rmoff.github.io/2018/03/27/streaming-data-from-mongodb-into-kafka-with-kafka-connect-and-debezium/"},{"categories":["mongodb","ubiquiti","ubnt","mongorestore","mongodump"],"content":"DISCLAIMER: I am not a MongoDB person (even if it is Web Scale X-D) - below instructions may work for you, they may not. Use with care!\nFor some work I\u0026rsquo;ve been doing I wanted to access the data in Ubiquiti\u0026rsquo;s Unifi controller which it stores in MongoDB. Because I didn\u0026rsquo;t want to risk my actual Unifi device by changing local settings to enable remote access, and also because the version of MongoDB on it is older than ideal, I wanted to clone the data elsewhere.","keywords":null,"title":"Cloning Ubiquiti\u0026rsquo;s MongoDB instance to a separate server","uri":"https://rmoff.github.io/2018/03/27/cloning-ubiquitis-mongodb-instance-to-a-separate-server/"},{"categories":["debezium","kafka","kafka connect","mysql"],"content":"Debezium is a CDC tool that can stream changes from MySQL, MongoDB, and PostgreSQL into Kafka, using Kafka Connect. In this article we\u0026rsquo;ll see how to set it up and examine the format of the data. A subsequent article will show using this realtime stream of data from a RDBMS and join it to data originating from other sources, using KSQL.\nThe software versions used here are:\n Confluent Platform 4.","keywords":null,"title":"Streaming Data from MySQL into Kafka with Kafka Connect and Debezium","uri":"https://rmoff.github.io/2018/03/24/streaming-data-from-mysql-into-kafka-with-kafka-connect-and-debezium/"},{"categories":null,"content":"io.confluent.ksql.exception.KafkaTopicException: Topic 'KSQL_NOTIFY' does not conform to the requirements Partitions:1 v 4. Replication: 1 v 1  Why? Because the topic KSQL creates to underpin a CREATE STREAM AS SELECT or CREATE TABLE AS SELECT already exists, and doesn\u0026rsquo;t match what it expects. By default it will create partitions \u0026amp; replicas based on the same values of the input topic.\nOptions:\n Use a different topic, via the WITH (KAFKA_TOPIC='FOO') syntax, e.","keywords":null,"title":"KSQL: Topic ‚Ä¶ does not conform to the requirements","uri":"https://rmoff.github.io/2018/03/06/ksql-topic-does-not-conform-to-the-requirements/"},{"categories":["kafka connect","elasticsearch","kafka","oracle","streaming etl"],"content":"This article is part of a series exploring Streaming ETL in practice. You can read about setting up the ingest of realtime events from a standard Oracle platform, and building streaming ETL using KSQL.\nThis post shows how we take data streaming in from an Oracle transactional system into Kafka, and simply stream it onwards into Elasticsearch. This is a common pattern, for enabling rapid search or analytics against data held in systems elsewhere.","keywords":null,"title":"Streaming data from Kafka into Elasticsearch","uri":"https://rmoff.github.io/2018/03/06/streaming-data-from-kafka-into-elasticsearch/"},{"categories":["kafka","confluent","python","apt-get"],"content":"System:\nrmoff@proxmox01:~$ uname -a Linux proxmox01 4.4.6-1-pve #1 SMP Thu Apr 21 11:25:40 CEST 2016 x86_64 GNU/Linux rmoff@proxmox01:~$ head -n1 /etc/os-release PRETTY_NAME=\u0026quot;Debian GNU/Linux 8 (jessie)\u0026quot; rmoff@proxmox01:~$ python --version Python 2.7.9  Following:\n https://www.confluent.io/blog/introduction-to-apache-kafka-for-python-programmers/ https://github.com/confluentinc/confluent-kafka-python  Install librdkafka, which is a pre-req for the Python library:\nwget -qO - https://packages.confluent.io/deb/4.0/archive.key | sudo apt-key add - sudo add-apt-repository \u0026quot;deb [arch=amd64] https://packages.confluent.io/deb/4.0 stable main\u0026quot; sudo apt-get install librdkafka-dev python-dev  Setup virtualenv:","keywords":null,"title":"Installing the Python Kafka library from Confluent - troubleshooting some silly errors‚Ä¶","uri":"https://rmoff.github.io/2018/03/06/installing-the-python-kafka-library-from-confluent-troubleshooting-some-silly-errors/"},{"categories":["streaming","etl","apache kafka","confluent platform"],"content":"(This is an expanded version of the intro to an article I posted over on the Confluent blog. Here I get to be as verbose as I like ;))\nMy first job from university was building a datawarehouse for a retailer in the UK. Back then, it was writing COBOL jobs to load tables in DB2. We waited for all the shops to close and do their end of day system processing, and send their data back to the central mainframe.","keywords":null,"title":"Why Do We Need Streaming ETL?","uri":"https://rmoff.github.io/2018/03/06/why-do-we-need-streaming-etl/"},{"categories":["apache kafka","schema registry","swingbench","goldengate","oracle"],"content":"This is the detailed step-by-step if you want to recreate the process I describe in the Confluent blog here\nI used Oracle\u0026rsquo;s Oracle Developer Days VM, which comes preinstalled with Oracle 12cR2. You can see the notes on how to do this here. These notes take you through installing and configuring:\n Swingbench, to create a sample \u0026ldquo;Order Entry\u0026rdquo; schema and simulate events on the Oracle database Oracle GoldenGate (OGG, forthwith) and Oracle GoldenGate for Big Data (OGG-BD, forthwith)  I\u0026rsquo;m using Oracle GoldenGate 12.","keywords":null,"title":"HOWTO: Oracle GoldenGate + Apache Kafka + Schema Registry + Swingbench","uri":"https://rmoff.github.io/2018/02/01/howto-oracle-goldengate-apache-kafka-schema-registry-swingbench/"},{"categories":["kafka","adminclient","networking"],"content":"See also Kafka Listeners - Explained\nA short post to help Googlers. On a single-node sandbox Apache Kafka / Confluent Platform installation, I was getting this error from Schema Registry, Connect, etc:\nWARN [AdminClient clientId=adminclient-3] Connection to node -1 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)  KSQL was throwing a similar error:\nKSQL cannot initialize AdminCLient.  I had correctly set the machine\u0026rsquo;s hostname in my Kafka server.","keywords":null,"title":"Kafka - AdminClient - Connection to node -1 could not be established. Broker may not be available","uri":"https://rmoff.github.io/2018/01/03/kafka-adminclient-connection-to-node-1-could-not-be-established.-broker-may-not-be-available/"},{"categories":["goldengate","oracle","kafka","confluent platform","swingbench"],"content":"Some notes that I made on installing and configuring Oracle GoldenGate with Confluent Platform. Excuse the brevity, but hopefully useful to share!\nI used the Oracle Developer Days VM for this - it\u0026rsquo;s preinstalled with Oracle 12cR2. Big Data Lite is nice but currently has an older version of GoldenGate.\nLogin to the VM (oracle/oracle) and then install some useful things:\nsudo rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo yum install -y screen htop collectl rlwrap p7zip unzip sysstat perf iotop sudo su - cd /etc/yum.","keywords":null,"title":"Installing Oracle GoldenGate for Big Data 12.3.1 with Kafka Connect and Confluent Platform","uri":"https://rmoff.github.io/2017/11/21/installing-oracle-goldengate-for-big-data-12.3.1-with-kafka-connect-and-confluent-platform/"},{"categories":["oracle","openworld","oaktable world"],"content":"Here\u0026rsquo;s where I\u0026rsquo;ll be!\n If you use Google Calendar you can click on individual entries above and select copy to my calendar - which of course you\u0026rsquo;ll want to do for all the ones I\u0026rsquo;ve marked as [SPEAKING] :-)\nHere\u0026rsquo;s a list of all the Apache Kafka talks at OpenWorld and JavaOne, most of which I\u0026rsquo;ll be trying to get to.","keywords":null,"title":"Where will I be at OpenWorld / Oak Table World?","uri":"https://rmoff.github.io/2017/09/29/where-will-i-be-at-openworld-oak-table-world/"},{"categories":["kafka","openworld","javaone","oow","san francisco","oaktable world"],"content":"There\u0026rsquo;s an impressive 19 sessions that cover Apache Kafka‚Ñ¢ at Oracle OpenWorld, JavaOne, and Oak Table World this year! You can find the full list with speakers in the session catalogs for OOW, JavaOne, and Oak Table World. OTW is an awesome techie conference which is at the same time as OpenWorld, next door to Moscone. Hope to see you there!\nCheck out the writeup of my previous visit to OOW including useful tips here.","keywords":null,"title":"Apache Kafka‚Ñ¢ talks at Oracle OpenWorld, JavaOne, and Oak Table World 2017","uri":"https://rmoff.github.io/2017/09/20/apache-kafka-talks-at-oracle-openworld-javaone-and-oak-table-world-2017/"},{"categories":["goldengate","kafka connect","avro","schema registry","oracle"],"content":"The Replicat was kapput:\nGGSCI (localhost.localdomain) 3\u0026gt; info rkconnoe REPLICAT RKCONNOE Last Started 2017-09-12 17:06 Status ABENDED Checkpoint Lag 00:00:00 (updated 00:46:34 ago) Log Read Checkpoint File /u01/app/ogg/dirdat/oe000000 First Record RBA 0  So checking the OGG error log ggserr.log showed\n2017-09-12T17:06:17.572-0400 ERROR OGG-15051 Oracle GoldenGate Delivery, rkconnoe.prm: Java or JNI exception: oracle.goldengate.util.GGException: Error detected handling operation added event. 2017-09-12T17:06:17.572-0400 ERROR OGG-01668 Oracle GoldenGate Delivery, rkconnoe.prm: PROCESS ABENDING.  So checking the replicat log dirrpt/RKCONNOE_info_log4j.","keywords":null,"title":"Oracle GoldenGate / Kafka Connect Handler troubleshooting","uri":"https://rmoff.github.io/2017/09/12/oracle-goldengate-kafka-connect-handler-troubleshooting/"},{"categories":["markdown","marked2","emacs","vi"],"content":"Markdown is a plain-text formatting syntax. It enables you write documents in plain text, readable by others in plain text, and optionally rendered into nicely formatted PDF, HTML, DOCX etc.\nIt\u0026rsquo;s used widely in software documentation, particularly open-source, because it enables richer formatting than plain-text alone, but without constraining authors or readers to a given software platform.\nPlatforms such as github natively support Markdown rendering - so you write your README etc in markdown, and when viewed on github it is automagically rendered - without you needing to actually do anything.","keywords":null,"title":"What is Markdown, and Why is it Awesome?","uri":"https://rmoff.github.io/2017/09/12/what-is-markdown-and-why-is-it-awesome/"},{"categories":["conferences","meetups","speaking","doag","ukoug","oow"],"content":"I\u0026rsquo;m excited to be speaking at several conferences and meetups over the next few months. Unsurprisingly, the topic will be Apache Kafka!\nIf you\u0026rsquo;re at any of these, please do come and say hi :)\nApache Kafka Meetup - London My first time talking at the London Apache Kafka Meetup - always a sold-out crowd, this will be fun!\n September 20th, 19:00 : Look Ma, no Code! Building Streaming Data Pipelines with Apache Kafka  Slides are available here   Oracle OpenWorld - San Francisco This will be my second time at OOW - I wrote up my previous trip here","keywords":null,"title":"Conferences \u0026amp; Meetups at which I\u0026rsquo;ll be speaking - 2017","uri":"https://rmoff.github.io/2017/09/11/conferences-meetups-at-which-ill-be-speaking-2017/"},{"categories":["kafka connect","JsonDeserializer"],"content":"An error that I see coming up frequently in the Kafka Connect community (e.g. mailing list, Slack group, StackOverflow) is:\nJsonDeserializer with schemas.enable requires \u0026quot;schema\u0026quot; and \u0026quot;payload\u0026quot; fields and may not contain additional fields  or\nNo fields found using key and value schemas for table: foo-bar  You can see an explanation, and solution, for the issue in my StackOverflow answer here: https://stackoverflow.com/a/45940013/350613\nIf you\u0026rsquo;re using schemas.enable in the Connector configuration, you must have schema and payload as the root-level elements of your JSON message ( Which is pretty much verbatim what the error says üòÅ), like this:","keywords":null,"title":"Kafka Connect - JsonDeserializer with schemas.enable requires \u0026ldquo;schema\u0026rdquo; and \u0026ldquo;payload\u0026rdquo; fields","uri":"https://rmoff.github.io/2017/09/06/kafka-connect-jsondeserializer-with-schemas.enable-requires-schema-and-payload-fields/"},{"categories":["curl","grafana","bash","jq"],"content":" Grafana API Reference\nExport all Grafana data sources to data_sources folder mkdir -p data_sources \u0026amp;\u0026amp; curl -s \u0026quot;http://localhost:3000/api/datasources\u0026quot; -u admin:admin|jq -c -M '.[]'|split -l 1 - data_sources/  This exports each data source to a separate JSON file in the data_sources folder.\nLoad data sources back in from folder This submits every file that exists in the data_sources folder to Grafana as a new data source definition.\nfor i in data_sources/*; do \\ curl -X \u0026quot;POST\u0026quot; \u0026quot;http://localhost:3000/api/datasources\u0026quot; \\ -H \u0026quot;Content-Type: application/json\u0026quot; \\ --user admin:admin \\ --data-binary @$i done  ","keywords":null,"title":"Simple export/import of Data Sources in Grafana","uri":"https://rmoff.github.io/2017/08/08/simple-export-import-of-data-sources-in-grafana/"},{"categories":["lsblk","uas","usb","mount"],"content":"Usually connecting external disks in Linux is easy. Plug it in, run fdisk -l or lsblk | grep disk to identify the device ID, and then mount it.\nUnfortunately in this instance, plugging in my Seagate 2TB wasn\u0026rsquo;t so simple. The server is running Proxmox:\n# uname -a Linux proxmox01 4.4.6-1-pve #1 SMP Thu Apr 21 11:25:40 CEST 2016 x86_64 GNU/Linux  No device showed up on lsblk or fdisk -l.","keywords":null,"title":"Linux - USB disk connection problems - uas: probe failed with error -12","uri":"https://rmoff.github.io/2017/06/21/linux-usb-disk-connection-problems-uas-probe-failed-with-error-12/"},{"categories":["kafka","log4j","kafka connect"],"content":"Kafka\u0026rsquo;s Connect API is a wondrous way of easily bringing data in and out of Apache Kafka without having to write a line of code. By choosing a Connector from the many available, it\u0026rsquo;s possible to set up and end-to-end data pipeline with just a few lines of configuration. You can configure this by hand, or you can use the Confluent Control Center, for both management and monitoring:\nBUT \u0026hellip; there are times when not all goes well - perhaps your source has gone offline, or one of your targets has been misconfigured.","keywords":null,"title":"Configuring Kafka Connect to log REST HTTP messages to a separate file","uri":"https://rmoff.github.io/2017/06/12/configuring-kafka-connect-to-log-rest-http-messages-to-a-separate-file/"},{"categories":["kafka","key","spelling","pebcak"],"content":"A very silly PEBCAK problem this one, but Google hits weren\u0026rsquo;t so helpful so here goes.\nRunning a console producer, specifying keys:\nkafka-console-producer \\ --broker-list localhost:9092 \\ --topic test_topic \\ --property parse.key=true \\ --property key.seperator=,  Failed when I entered a key/value:\n1,foo kafka.common.KafkaException: No key found on line 1: 1,foo at kafka.tools.ConsoleProducer$LineMessageReader.readMessage(ConsoleProducer.scala:314) at kafka.tools.ConsoleProducer$.main(ConsoleProducer.scala:55) at kafka.tools.ConsoleProducer.main(ConsoleProducer.scala)  kafka.common.KafkaException: No key found on line \u0026hellip; but I specified the key, didn\u0026rsquo;t I?","keywords":null,"title":"kafka.common.KafkaException: No key found on line 1","uri":"https://rmoff.github.io/2017/05/12/kafka.common.kafkaexception-no-key-found-on-line-1/"},{"categories":["reading","books","twitter","podcasts","knowledge","data","career"],"content":"How do you try and stay current on technical affairs, given only 24 hours in a day and a job to do as well? Here\u0026rsquo;s my take on it\u0026hellip;\nOne of the many things that has changed perceptibly since the beginning of this century when I started working in IT is the amount of information freely available, and being created all the time. Back then, printed books and manuals were still the primary source of definitive information about a piece of software.","keywords":null,"title":"Keeping Up with the Deluge","uri":"https://rmoff.github.io/2017/03/11/keeping-up-with-the-deluge/"},{"categories":["qemu","aws","ec2","centos","rhel"],"content":"Mucking about with virtual disks, I wanted to install qemu on a AWS EC2 instance in order to use qemu-img.\nNot finding it in a yum repo, I built it from scratch:\n$ uname -a Linux ip-10-0-1-238 4.4.41-36.55.amzn1.x86_64 #1 SMP Wed Jan 18 01:03:26 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux  Steps:\nsudo yum install -y ghc-glib-devel ghc-glib autoconf autogen intltool libtool wget http://download.qemu-project.org/qemu-2.8.0.tar.xz tar xvJf qemu-2.8.0.tar.xz cd qemu-2.8.0 ./configure make sudo make install  I hit a few errors, recorded here for passing Googlers:","keywords":null,"title":"Install qemu on AWS EC2 Amazon Linux","uri":"https://rmoff.github.io/2017/03/11/install-qemu-on-aws-ec2-amazon-linux/"},{"categories":["vmdk","vgscan","lvm","mount","raw","img","aws","losetup"],"content":"So you\u0026rsquo;ve got a Linux VM that you want to access the contents of in EC2 - how do you do it? Let\u0026rsquo;s see how. First up, convert the VMDK to raw image file. If you\u0026rsquo;ve got a ova/ovf then just untar it first (tar -xvf my_vm.ova), from which you should get the VMDK. With that, convert it using qemu-img:\n$ time qemu-img convert -f vmdk -O raw SampleAppv607p-appliance-disk1.vmdk SampleAppv607p-appliance-disk1.raw real 16m36.","keywords":null,"title":"Mount VMDK/OVF/OVA on Amazon Web Services (AWS) EC2","uri":"https://rmoff.github.io/2017/03/11/mount-vmdk-ovf-ova-on-amazon-web-services-aws-ec2/"},{"categories":["headphones","microphone","headset","wireless","logitech","microsoft","apple","strava","running","podcasts","apple watch"],"content":"Wireless Headset for VOIP With No 30-Minute Dalek Timebomb A lot of my work is done remotely, with colleagues and customers. Five years ago I bought a Microsoft LifeChat LX-3000 which plugged into the USB port on my Mac. It did the job kinda fine, with two gripes:\n it wasn\u0026rsquo;t wireless. I like to wander whilst I chat, and I didn\u0026rsquo;t like being tethered. But this in itself wasn\u0026rsquo;t a reason to ditch it After c.","keywords":null,"title":"Little Technology Wins","uri":"https://rmoff.github.io/2017/03/11/little-technology-wins/"},{"categories":["career"],"content":"After 5 years at Rittman Mead, 126 blog posts, 16 conferences, four published OTN articles, an Oracle ACE award - not to mention, of course, a whole heap of interesting and challenging client work - I\u0026rsquo;ve decided that it\u0026rsquo;s time to do something different.\nLater this month I\u0026rsquo;ll be joining Confluent as a Partner Technology Evangelist, helping spread the good word of Apache Kafka and the Confluent platform.\nAs always you can find me on Twitter @rmoff, for beer tweets, fried breakfast pics - and lots of Apache Kafka!","keywords":null,"title":"Time For a Change","uri":"https://rmoff.github.io/2017/03/10/time-for-a-change/"},{"categories":["hbase","bigdatalite","virtualbox","ova"],"content":"I use BigDataLite for a lot of my sandboxing work. This is a OVA provided by Oracle which can be run on VirtualBox, VMWare, etc and has the Cloudera Hadoop platform (CDH) along with all of Oracle\u0026rsquo;s Big Data goodies including Big Data Discovery and Big Data Spatial and Graph (BDSG).\nSomething that kept tripping me up during my work with BDSG was that HBase would become unavailable. Not being an HBase expert and simply using it as a data store for my property graph data, I wrote it off as mistakes on my part.","keywords":null,"title":"HBase crash after resuming suspended VM","uri":"https://rmoff.github.io/2017/01/20/hbase-crash-after-resuming-suspended-vm/"},{"categories":["kibana","timelion","holt"],"content":"Using the holt function in Timelion to do anomaly detection on Metricbeat data in Kibana:\nExpression:\n$thres=0.02, .es(index='metricbeat*',metric='max:system.cpu.user.pct').lines(1).if(eq, 0, null).holt(0.9, 0.1, 0.9, 0.5h).color(#eee).lines(10).label('Prediction'), .es(index='metricbeat*',metric='max:system.cpu.user.pct').color(#666).lines(1).label(Actual), .es(index='metricbeat*',metric='max:system.cpu.user.pct').lines(1).if(eq, 0, null).holt(0.9, 0.1, 0.9, 0.5h).subtract(.es(index='metricbeat*',metric='max:system.cpu.user.pct')).abs().if(lt, $thres, null, .es(index='metricbeat*',metric='max:system.cpu.user.pct')).points(10,3,0).color(#c66).label('Anomaly').title('max:system.cpu.user.pct / @rmoff')  References:\n https://twitter.com/rashidkpc/status/762754396111327232 https://github.com/elastic/timelion/issues/87 https://github.com/elastic/timelion/blob/master/FUNCTIONS.md  ","keywords":null,"title":"Kibana Timelion - Anomaly Detection","uri":"https://rmoff.github.io/2017/01/18/kibana-timelion-anomaly-detection/"},{"categories":null,"content":"The world beyond batch: Streaming 101 The world beyond batch: Streaming 102 Data architectures for streaming applications SE-Radio Episode 272: Frances Perry on Apache Beam  (img credit)","keywords":null,"title":"Streaming / Unbounded Data - Resources","uri":"https://rmoff.github.io/2017/01/16/streaming-unbounded-data-resources/"},{"categories":["kafka","schema registry","kafka-avro-console-producer"],"content":"By default, the kafka-avro-console-producer will assume that the schema registry is on port 8081, and happily connect to it. Unfortunately, this can lead to some weird errors if another process happens to be listening on port 8081 already!\n[oracle@bigdatalite tmp]$ kafka-avro-console-producer \\ \u0026gt; --broker-list localhost:9092 --topic kudu_test \\ \u0026gt; --property value.schema='{\u0026quot;type\u0026quot;:\u0026quot;record\u0026quot;,\u0026quot;name\u0026quot;:\u0026quot;myrecord\u0026quot;,\u0026quot;fields\u0026quot;:[{\u0026quot;name\u0026quot;:\u0026quot;id\u0026quot;,\u0026quot;type\u0026quot;:\u0026quot;int\u0026quot;},{\u0026quot;name\u0026quot;:\u0026quot;random_field\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;}]}' {\u0026quot;id\u0026quot;: 999, \u0026quot;random_field\u0026quot;: \u0026quot;foo\u0026quot;} org.apache.kafka.common.errors.SerializationException: Error registering Avro schema: {\u0026quot;type\u0026quot;:\u0026quot;record\u0026quot;,\u0026quot;name\u0026quot;:\u0026quot;myrecord\u0026quot;,\u0026quot;fields\u0026quot;:[{\u0026quot;name\u0026quot;:\u0026quot;id\u0026quot;,\u0026quot;type\u0026quot;:\u0026quot;int\u0026quot;},{\u0026quot;name\u0026quot;:\u0026quot;random_field\u0026quot;,\u0026quot;type\u0026quot;:\u0026quot;string\u0026quot;}]} Caused by: io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException: Unexpected character ('\u0026lt;' (code 60)): expected a valid value (number, String, array, object, 'true', 'false' or 'null') at [Source: sun.","keywords":null,"title":"kafka-avro-console-producer - Error registering Avro schema / io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException","uri":"https://rmoff.github.io/2016/12/02/kafka-avro-console-producer-error-registering-avro-schema-io.confluent.kafka.schemaregistry.client.rest.exceptions.restclientexception/"},{"categories":["ogg","kafka connect","avro"],"content":"tl;dr Make sure that key.converter.schema.registry.url and value.converter.schema.registry.url are specified, and that there are no trailing whitespaces.\nI\u0026rsquo;ve been building on previous work I\u0026rsquo;ve done with Oracle GoldenGate and Kafka Connect, looking at how to have the change records from the Oracle database come through to Kafka in Avro format rather than the default JSON that the sample configuration gives.\nSimply changing the Kafka Connect OGG configuration file (confluent.properties) from\nvalue.converter=org.apache.kafka.connect.json.JsonConverter key.","keywords":null,"title":"Oracle GoldenGate -\u0026gt; Kafka Connect - \u0026ldquo;Failed to serialize Avro data\u0026rdquo;","uri":"https://rmoff.github.io/2016/11/29/oracle-goldengate-kafka-connect-failed-to-serialize-avro-data/"},{"categories":["kafka","kafka connect","IncompatibleClassChangeError","classpath"],"content":"I hit this error running Kafka Connect HDFS connector from Confluent Platform v3.1.1 on BigDataLite 4.6:\n[oracle@bigdatalite ~]$ connect-standalone /etc/schema-registry/connect-avro-standalone.properties /etc/kafka-connect-hdfs/quickstart-hdfs.properties [...] Exception in thread \u0026quot;main\u0026quot; java.lang.IncompatibleClassChangeError: Implementing class at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:467) at java.net.URLClassLoader.access$100(URLClassLoader.java:73) at java.net.URLClassLoader$1.run(URLClassLoader.java:368) at java.net.URLClassLoader$1.run(URLClassLoader.java:362) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:361) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763)  The fix was to unset the CLASSPATH first:\nunset CLASSPATH  ","keywords":null,"title":"Kafka Connect - java.lang.IncompatibleClassChangeError","uri":"https://rmoff.github.io/2016/11/24/kafka-connect-java.lang.incompatibleclasschangeerror/"},{"categories":["boto","s3","aws","python"],"content":"Presented without comment, warranty, or context - other than these might help a wandering code hacker.\nWhen using SigV4, you must specify a \u0026lsquo;host\u0026rsquo; parameter boto.s3.connection.HostRequiredError: BotoClientError: When using SigV4, you must specify a 'host' parameter.  To fix, switch\nconn_s3 = boto.connect_s3()  for\nconn_s3 = boto.connect_s3(host='s3.amazonaws.com')  You can see a list of endpoints here.\nboto.exception.S3ResponseError: S3ResponseError: 400 Bad Request Make sure you\u0026rsquo;re specifying the correct hostname (see above) for the bucket\u0026rsquo;s region.","keywords":null,"title":"boto / S3 errors","uri":"https://rmoff.github.io/2016/10/14/boto-s3-errors/"},{"categories":["kafka","goldengate","ogg","ogg-15051"],"content":"Similar to the previous issue, the sample config in the docs causes another snafu:\nOGG-15051 Java or JNI exception: oracle.goldengate.util.GGException: Class not found: \u0026quot;kafkahandler\u0026quot;. kafkahandler Class not found: \u0026quot;kafkahandler\u0026quot;. kafkahandler  This time it\u0026rsquo;s in the kafka.props file:\ngg.handler.kafkahandler.Type = kafka  Should be\ngg.handler.kafkahandler.type = kafka  No capital T in Type!\n(Image credit: https://unsplash.com/@vanschneider)","keywords":null,"title":"OGG-15051 oracle.goldengate.util.GGException:  Class not found: \u0026ldquo;kafkahandler\u0026rdquo;","uri":"https://rmoff.github.io/2016/07/29/ogg-15051-oracle.goldengate.util.ggexception-class-not-found-kafkahandler/"},{"categories":["ogg","goldengate","kafka"],"content":"In the documentation for the current release of Oracle GoldenGate for Big Data (12.2.0.1.1.011) there\u0026rsquo;s a helpful sample configuration, which isn\u0026rsquo;t so helpful \u0026hellip;\n[...] gg.handler.kafkahandler.ProducerRecordClass = com.company.kafka.CustomProducerRecord [...]  This value for gg.handler.kafkahandler.ProducerRecordClass will cause a failure when you start the replicat:\n[...] Class not found: \u0026quot;com.company.kafka.CustomProducerRecord\u0026quot; [...]  If you comment this configuration item out, it\u0026rsquo;ll use the default (oracle.goldengate.handler.kafka.DefaultProducerRecord) and work swimingly!\n(Image credit: https://unsplash.com/@vanschneider)","keywords":null,"title":"OGG -  Class not found: \u0026ldquo;com.company.kafka.CustomProducerRecord\u0026rdquo;","uri":"https://rmoff.github.io/2016/07/28/ogg-class-not-found-com.company.kafka.customproducerrecord/"},{"categories":["kafka","kafka connect","jdbc","oracle","log4j"],"content":"There are various reasons for this error, but the one I hit was that the table name is case sensitive, and returned from Oracle by the JDBC driver in uppercase.\nIf you specify the tablename in your connecter config in lowercase, it won\u0026rsquo;t be matched, and this error is thrown. You can validate this by setting debug logging (edit etc/kafka/connect-log4j.properties to set log4j.rootLogger=DEBUG, stdout), and observe: (I\u0026rsquo;ve truncated some of the output for legibility)","keywords":null,"title":"Kafka Connect JDBC - Oracle - Number of groups must be positive","uri":"https://rmoff.github.io/2016/07/27/kafka-connect-jdbc-oracle-number-of-groups-must-be-positive/"},{"categories":["kafka","avro","kafka connect","SchemaProjectorException"],"content":"I\u0026rsquo;ve been doing some noodling around with Confluent\u0026rsquo;s Kafka Connect recently, as part of gaining a wider understanding into Kafka. If you\u0026rsquo;re not familiar with Kafka Connect this page gives a good idea of the thinking behind it.\nOne issue that I hit defeated my Google-fu so I\u0026rsquo;m recording it here to hopefully help out fellow n00bs.\nThe pipeline that I\u0026rsquo;d set up looked like this:\n Eneco\u0026rsquo;s Twitter Source streaming tweets to a Kafka topic Confluent\u0026rsquo;s HDFS Sink to stream tweets to HDFS and define Hive table automagically over them  It worked great, but only if I didn\u0026rsquo;t enable the Hive integration part.","keywords":null,"title":"Kafka Connect - HDFS with Hive Integration - SchemaProjectorException - Schema version required","uri":"https://rmoff.github.io/2016/07/19/kafka-connect-hdfs-with-hive-integration-schemaprojectorexception-schema-version-required/"},{"categories":["apcupsd","ups"],"content":"With my new server I bought a UPS, partly just as a Good Thing, but also because I suspect a powercut fried the motherboard on a previous machine that I had, and this baby is too precious to lose ;)\nThe idea is that the UPS will smooth out the power supply to my server, protecting it from surges or temporarily blips in power loss. If there\u0026rsquo;s a proper power cut, the UPS is connected to my server and can initiate a graceful shutdown instead of system crash.","keywords":null,"title":"Configuring UPS/apcupsd","uri":"https://rmoff.github.io/2016/07/18/configuring-ups-apcupsd/"},{"categories":["spark","sparksql","json"],"content":"Trying to use SparkSQL to read a JSON file, from either pyspark or spark-shell, I got this error:\njava.io.IOException: No input paths specified in job  scala\u0026gt; sqlContext.read.json(\u0026quot;/u02/custom/twitter/twitter.json\u0026quot;) java.io.IOException: No input paths specified in job at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:202)  Despite the reference articles that I found using this local path syntax (/u02/custom/twitter/twitter.json), it turned out that I needed to prefix it with file://:\nscala\u0026gt; sqlContext.read.json(\u0026quot;file:///u02/custom/twitter/twitter.json\u0026quot;) res3: org.apache.spark.sql.DataFrame = [@timestamp: string, @version: string, contributors: string, coordinates: string, created_at: string, entities: struct\u0026lt;hashtags:array\u0026lt;struct\u0026lt;indices:array\u0026lt;bigint\u0026gt;,text:string\u0026gt;\u0026gt;,media:array\u0026lt;struct\u0026lt;display_url:string,expanded_url:string,id:bigint,id_str:string,indices:array\u0026lt;bigint\u0026gt;,media_url:string,media_url_https:string,sizes:struct\u0026lt;large:struct\u0026lt;h:bigint,resize:string,w:bigint\u0026gt;,medium:struct\u0026lt;h:bigint,resize:string,w:bigint\u0026gt;,small:struct\u0026lt;h:bigint,resize:string,w:bigint\u0026gt;,thumb:struct\u0026lt;h:bigint,resize:string,w:bigint\u0026gt;\u0026gt;,source_status_id:bigint,source_status_id_str:string,source_user_id:bigint,source_user_id_str:string,type:string,url:string\u0026gt;\u0026gt;,symbols:array\u0026lt;struct\u0026lt;indices:array\u0026lt;bigint\u0026gt;,text:string\u0026gt;\u0026gt;,urls:array\u0026lt;struct\u0026lt;display_url:string,expanded_url:string.","keywords":null,"title":"Spark sqlContext.read.json - java.io.IOException: No input paths specified in job","uri":"https://rmoff.github.io/2016/07/13/spark-sqlcontext.read.json-java.io.ioexception-no-input-paths-specified-in-job/"},{"categories":["proxmox","cidr","networking","lxc"],"content":"TL;DR When defining networking on Proxmox 4 LXC containers, use an appropriate CIDR suffix (e.g. 24) - don\u0026rsquo;t use 32!\nOn my Proxmox 4 server I\u0026rsquo;m running a whole load of lovely LXC containers. Unfortunately, I had trouble connecting to them. From a client machine, I got the error\nssh_exchange_identification: read: Connection reset by peer  On the server I was connecting to (which I could get a console for through the Proxmox GUI, or a session on using pct enter from the Proxmox host) I ran a SSHD process with debug to see what was happening:","keywords":null,"title":"Proxmox 4 Containers - ssh - ssh_exchange_identification: read: Connection reset by peer","uri":"https://rmoff.github.io/2016/07/05/proxmox-4-containers-ssh-ssh_exchange_identification-read-connection-reset-by-peer/"},{"categories":["hue","django","python","cdh"],"content":"(Ref)\nThe bit that caught me out was this kept failing with\nError: Password not present  and a Python stack trace that ended with\nsubprocess.CalledProcessError: Command '/var/run/cloudera-scm-agent/process/78-hue-HUE_SERVER/altscript.sh sec-1-secret_key' returned non-zero exit status 1  The answer (it seems) is to ensure that HUE_SECRET_KEY is set (to any value!)\nLaunch shell:\nexport HUE_SECRET_KEY=foobar /opt/cloudera/parcels/CDH-5.7.1-1.cdh5.7.1.p0.11/lib/hue/build/env/bin/hue shell  Reset password for hue, activate account and make it superuser\nfrom django.contrib.auth.models import User user = User.","keywords":null,"title":"Reset Hue password","uri":"https://rmoff.github.io/2016/07/05/reset-hue-password/"},{"categories":["apache drill"],"content":"Vanilla download of Apache Drill 1.6, attempting to follow the Followed the Drill in 10 Minutes tutorial - but kept just getting the error No current connection. Here\u0026rsquo;s an example:\n[oracle@bigdatalite apache-drill-1.6.0]$ ./bin/drill-embedded Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0 com.fasterxml.jackson.databind.JavaType.isReferenceType()Z apache drill 1.6.0 \u0026quot;the only truly happy people are children, the creative minority and drill users\u0026quot; 0: jdbc:drill:zk=local\u0026gt; SELECT version FROM sys.version; No current connection 0: jdbc:drill:zk=local\u0026gt;  Whether SELECT version FROM sys.","keywords":null,"title":"Apache Drill - conflicting jar problem - \u0026ldquo;No current connection\u0026rdquo;","uri":"https://rmoff.github.io/2016/06/20/apache-drill-conflicting-jar-problem-no-current-connection/"},{"categories":["mogodb","hive","jar","classnotfoundexception"],"content":"I wasted literally two hours on this one, so putting down a note to hopefully help future Googlers.\nSymptom Here\u0026rsquo;s all the various errors that I got in the hive-server2.log during my attempts to get a CREATE EXTERNABLE TABLE to work against a MongoDB table in Hive:\nCaused by: java.lang.ClassNotFoundException: com.mongodb.hadoop.io.BSONWritable Caused by: java.lang.ClassNotFoundException: com.mongodb.util.JSON Caused by: java.lang.ClassNotFoundException: org.bson.conversions.Bson Caused by: java.lang.ClassNotFoundException: org.bson.io.OutputBuffer  Whilst Hive would throw errors along the lines of:","keywords":null,"title":"ClassNotFoundException with MongoDB-Hadoop in Hive","uri":"https://rmoff.github.io/2016/06/15/classnotfoundexception-with-mongodb-hadoop-in-hive/"},{"categories":["lxc","proxmox","swapfree","cdh","cloudera","yarn","readProcMemInfoFile","/proc/meminfo"],"content":"Installing CDH 5.7 on Linux Containers (LXC) hosted on Proxmox 4. Everything was going well until Cluster Setup, and which point it failed on Start YARN (MR2 included)\nCompleted only 0/1 steps. First failure: Failed to execute command Start on service YARN (MR2 Included)  Log /var/log/hadoop-yarn/hadoop-cmf-yarn-NODEMANAGER-cdh57-01-node-02.moffatt.me.log.out showed:\norg.apache.hadoop.service.AbstractService: Service containers-monitor failed in state INITED; cause: java.lang.NumberFormatException: For input string: \u0026quot;18446744073709550364\u0026quot; java.lang.NumberFormatException: For input string: \u0026quot;18446744073709550364\u0026quot;  Looking down the stack trace, this came from org.","keywords":null,"title":"Erroneous SwapFree on LXC causes problems with CDH install","uri":"https://rmoff.github.io/2016/06/15/erroneous-swapfree-on-lxc-causes-problems-with-cdh-install/"},{"categories":["edgemax","erl","edgerouter lite","tftp","rj45","screen","squashfs","router","networking"],"content":"I\u0026rsquo;ve got an EdgeRouter LITE (ERL) which I used as my home router until a powercut fried it a while ago (looks like I\u0026rsquo;m not the only one to have this issue). The symptoms were it powering on but not giving any DHCP addresses, or after a factory reset responding on the default IP of 192.168.1.1. It was a real shame, because it had been a great bit of kit up until then.","keywords":null,"title":"Reviving a bricked EdgeRouter Lite (ERL) from a Mac","uri":"https://rmoff.github.io/2016/06/08/reviving-a-bricked-edgerouter-lite-erl-from-a-mac/"},{"categories":["docker","proxmox","bittorrent sync","dropbox"],"content":"(Previously, previously, previously)\nSince Proxmox 4 has a recent Linux kernel and mainline one at that, it means that Docker can be run on it. I\u0026rsquo;ve yet to really dig into Docker and work out when it makes sense in place of Linux Containers (LXC), so this is going to be a learning experience for me.\nTo install Docker, add Backports repo to apt:\nroot@proxmox01:~# cat /etc/apt/sources.list.d/backports.list deb http://ftp.debian.org/debian jessie-backports main  And then install:","keywords":null,"title":"Running a Docker Container on Proxmox for BitTorrent Sync","uri":"https://rmoff.github.io/2016/06/07/running-a-docker-container-on-proxmox-for-bittorrent-sync/"},{"categories":["proxmox","vmware","virtualbox","qcow2"],"content":"(Previously, previously)\nI\u0026rsquo;ve got a bunch of existing VirtualBox and VMWare VMs that I want to run on Proxmox. Eventually I\u0026rsquo;ll migrate them to containers, but for the time being run them as \u0026ldquo;fat\u0026rdquo; VMs using Proxmox\u0026rsquo;s KVM virtualisation. After copying the OVA files that I had to the server, I uncompressed them:\nroot@proxmox01:/data04/vms/bdl44-biwa# cd ../bdl44 root@proxmox01:/data04/vms/bdl44# ll total 27249328 -rw------- 1 root root 27903306752 Jun 1 10:14 BigDataLite440.ova root@proxmox01:/data04/vms/bdl44# tar -xf BigDataLite440.","keywords":null,"title":"Importing VMWare and VirtualBox VMs to Proxmox","uri":"https://rmoff.github.io/2016/06/07/importing-vmware-and-virtualbox-vms-to-proxmox/"},{"categories":["proxmox","ext4","e2label","lsblk","hdiutil","bootable usb","memtest"],"content":"(Previously)\nWith my server in place, I ran a memtest on it \u0026hellip; which with 128G took a while ;)\nAnd then installed Proxmox 4, using a bootable USB that I\u0026rsquo;d created on my Mac from the ISO downloaded from Proxmox\u0026rsquo;s website. To create the bootable USB, create the img file:\nhdiutil convert -format UDRW -o target.img source.iso  and then burn it to USB:\nsudo dd if=target.img of=/dev/rdiskN bs=1m  Replace N with the correct device based on diskutil list output.","keywords":null,"title":"Commissioning my Proxmox Server - OS and filesystems","uri":"https://rmoff.github.io/2016/06/07/commissioning-my-proxmox-server-os-and-filesystems/"},{"categories":["proxmox","home server","lxc","docker","vmware esxi"],"content":"After a long and painful delivery, I\u0026rsquo;m delighted to announce the arrival of a new addition to my household \u0026hellip; :\nThis custom-build from Scan 3XS is sat in my study quietly humming away. I\u0026rsquo;m going to use it for hosting VMs for R\u0026amp;D on OBIEE, Big Data Lite, Elastic, InfluxDB, Kafka, etc. I\u0026rsquo;ll blog various installations that I\u0026rsquo;ve done on it as a reference for myself, and anyone else interested.","keywords":null,"title":"A New Arrival","uri":"https://rmoff.github.io/2016/06/07/a-new-arrival/"},{"categories":["oracle","bigdatalite","vm"],"content":"Oracle\u0026rsquo;s excellent Big Data Lite VM has been updated, to version 4.5.\nDownload it here\nChanges:\n CDH 5.5 -\u0026gt; 5.7 Big Data Spatial and Graph 1.1 -\u0026gt; 1.2 Big Data Discovery 1.1 -\u0026gt; 1.2 Oracle Big Data Connectors 4.4 -\u0026gt; 4.5 Oracle NoSQL 3.5 -\u0026gt; 4.0 GoldenGate 12.2.0.1 -\u0026gt; 12.2.0.1.1  ","keywords":null,"title":"New version of BigDataLite VM from Oracle","uri":"https://rmoff.github.io/2016/06/06/new-version-of-bigdatalite-vm-from-oracle/"},{"categories":["obiee12c"],"content":"I\u0026rsquo;ve been spending some interesting hours digging into OBIEE 12c recently, with some interesting blog posts to show for it. Some of it is just curiosities discovered along the way, but the real meaty stuff is the in the RESTful APIs - lots of potential here for cool integrations I think\u0026hellip;\n Lifting the Lid on OBIEE 12c Web Services - Part 1 Lifting the Lid on OBIEE 12c Web Services - Part 2 Extended Subject Areas (XSA) and the Data Set Service  Changes in BI Server Cache Behaviour in OBIEE 12c : OBIS_REFRESH_CACHE  Dynamic Naming of OBIEE 12c Service Instance Exports OBIEE 12c - \u0026ldquo;Add Data Source\u0026rdquo; in Answers  (Photo credit: https://unsplash.","keywords":null,"title":"OBIEE 12c blog posts","uri":"https://rmoff.github.io/2016/06/01/obiee-12c-blog-posts/"},{"categories":["sawserver","logging","diagnostics"],"content":"Presentation Services can provide some very detailed logs, useful for troubleshooting, performance tracing, and general poking around. See here for details.\nThere\u0026rsquo;s no bi-init.sh in 12c, so need to set up the LD_LIBRARY_PATH ourselves:\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/app/oracle/biee/bi/bifoundation/web/bin/:/app/oracle/biee/bi/lib/:/app/oracle/biee/lib/:/app/oracle/biee/bi/bifoundation/odbc/lib/  Run sawserver with flag to list all log sources\n/app/oracle/biee/bi/bifoundation/web/bin/sawserver -logsources \u0026gt; saw_logsources_12.2.1.txt  Full list: https://gist.github.com/rmoff/e3be9009da6130839c71181cb58509a0","keywords":null,"title":"Presentation Services Logsources in OBIEE 12c","uri":"https://rmoff.github.io/2016/06/01/presentation-services-logsources-in-obiee-12c/"},{"categories":["obiee","obiee12c","rpd","downloadrpd","uploadrpd","data-model-cmd","web service","rest","curl","sysdig"],"content":"In OBIEE 12c data-model-cmd is a wrapper for some java code which ultimately calls an internal RESTful web service in OBIEE 12c, bi-lcm. We saw in the previous post how these internal web services can be opened up slightly, and we\u0026rsquo;re going to do the same again here. Which means, time for the same caveat:\nNone of these Web Services are documented, and they should therefore be assumed to be completely unsupported by Oracle.","keywords":null,"title":"Lifting the Lid on OBIEE 12c Web Services - Part 2","uri":"https://rmoff.github.io/2016/05/28/lifting-the-lid-on-obiee-12c-web-services-part-2/"},{"categories":["obiee","obiee12c","exportserviceinstance","wlst"],"content":"exportServiceInstance will export the RPD, Presentation Catalog, and Security model (application roles \u0026amp; policies etc \u0026ndash; but not WLS LDAP) into a single .bar file, from which they can be imported to another environment, or restored to the same one at a later date (e.g. for backup/restore).\nTo run exportServiceInstance you need to launch WLST first. The following demonstrates how to call it, and embeds the current timestamp \u0026amp; machine details in the backup (useful info, and also makes the backup name unique each time).","keywords":null,"title":"Dynamic Naming of OBIEE 12c Service Instance Exports","uri":"https://rmoff.github.io/2016/05/27/dynamic-naming-of-obiee-12c-service-instance-exports/"},{"categories":["obiee","obiee12c","xsa","dataset","datasetsvc"],"content":"So this had me scratching my head for a good hour today. Comparing SampleApp v511 against a vanilla OBIEE 12c install I\u0026rsquo;d done, one had \u0026ldquo;Add Data Source\u0026rdquo; as an option in Answers, the other didn\u0026rsquo;t. The strange thing was that the option wasn\u0026rsquo;t there in SampleApp \u0026ndash; and usually that has all the bells and whistles enabled.\nAfter checking and re-checking the Manage Privileges option, and even the Application Policy grants, and the manual, I hit MoS - and turned up Doc ID 2093886.","keywords":null,"title":"OBIEE 12c - \u0026ldquo;Add Data Source\u0026rdquo; in Answers","uri":"https://rmoff.github.io/2016/05/27/obiee-12c-add-data-source-in-answers/"},{"categories":["fullenglish","fryup","york","black pudding","fried slice"],"content":"I had the pleasure of not one but two fry-ups in York, UK last weekend.\nThe first was courtesy of Bill\u0026rsquo;s Restaurant\nOverall, pretty good, and I\u0026rsquo;ve had much worse. All the ingredients seemed decent. The black pudding was overcooked and almost biscuit-like, but that\u0026rsquo;s my only serious grumble. The bacon was cooked well. That black pudding, beans and the mashed/friend potato thing were each extra charges annoyed me. Particularly with a hangover, I just want to be able to order a full english, without playing Mastermind to work out what\u0026rsquo;s in or not.","keywords":null,"title":"York Fry Ups","uri":"https://rmoff.github.io/2016/05/24/york-fry-ups/"},{"categories":["obiee","obiee12c","rest","paw","postman","api","visual analyzer","process substitution","command substitution","sysdig"],"content":"Architecturally, OBIEE 12c is - on the surface - pretty similar to OBIEE 11g. Sure, we\u0026rsquo;ve lost OPMN in favour of Node Manager, but all the old favourites are there - WebLogic Servers, BI Server (nqsserver / OBIS), Presentation Services (sawserver / OBIPS), and so on.\nBut, scratch beneath the surface, or have a gander at slide decks such as this one from BIWA this year, and you realise that change is afoot.","keywords":null,"title":"Lifting the Lid on OBIEE 12c Web Services - Part 1","uri":"https://rmoff.github.io/2016/05/24/lifting-the-lid-on-obiee-12c-web-services-part-1/"},{"categories":["timelion","kibana","offset","formatting","bar","line"],"content":"I wrote recently about Kibana\u0026rsquo;s excellent Timelion feature, which brings time-series visualisations to Kibana. In the comments Ben Huang asked:\n do you know how to show whats the difference between this Friday and last Friday by Timelion?\n So I thought I\u0026rsquo;d answer properly here.\nTimelion includes mathematical functions including add and subtract, as well as the ability to show data offset by an amount of time. So to answer Ben\u0026rsquo;s query, we combine the two.","keywords":null,"title":"Kibana Timelion - Series Calculations - Difference from One Week Ago","uri":"https://rmoff.github.io/2016/05/23/kibana-timelion-series-calculations-difference-from-one-week-ago/"},{"categories":["obiee","obiee12c","hang","boot.properties","start.cmd"],"content":"Running the OBIEE 12c startup on Windows:\nC:\\app\\oracle\\fmw\\user_projects\\domains\\bi\\bitools\\bin\\start.cmd  Just hangs at:\nStarting AdminServer ...  No CPU being consumed, very odd. But then \u0026hellip; looking at DOMAIN_HOME\\servers\\AdminServer\\logs\\AdminServer.out shows the last log entry was:\nEnter username to boot WebLogic server:  And that\u0026rsquo;s bad news, cos that\u0026rsquo;s an interactive prompt, but not echo\u0026rsquo;d to the console output of the startup command, and there\u0026rsquo;s no way to interact with it.\nThe start.","keywords":null,"title":"OBIEE 12c hangs at startup - Starting AdminServer \u0026hellip;","uri":"https://rmoff.github.io/2016/05/20/obiee-12c-hangs-at-startup-starting-adminserver-.../"},{"categories":["obiee","obiee12c","bar"],"content":"Another quick note on OBIEE 12c, this time on the importServiceInstance command. If you run it with a BAR file that doesn\u0026rsquo;t exist, it\u0026rsquo;ll fail (obviously), but the error at the end of the stack trace is slightly confusing:\noracle.bi.bar.exceptions.UnSupportedBarException: The Bar file provided as input is not supported in this BI Platfrom release.  Scrolling back up the stack trace does show the error message:\nSEVERE: Failed in reading bar file.","keywords":null,"title":"oracle.bi.bar.exceptions.UnSupportedBarException: The Bar file provided as input is not supported in this BI Platfrom release.","uri":"https://rmoff.github.io/2016/05/19/oracle.bi.bar.exceptions.unsupportedbarexception-the-bar-file-provided-as-input-is-not-supported-in-this-bi-platfrom-release./"},{"categories":["obiee","bvt","regression testing","baseline validation tool","obiee12c"],"content":"Interesting quirk in running Baseline Validation Tool for OBIEE here. If you invoke obibvt from the bin folder, it errors with Parameter \u0026lsquo;directory\u0026rsquo; is not a directory\nPS C:\\OracleBI-BVT\u0026gt; cd bin PS C:\\OracleBI-BVT\\bin\u0026gt; .\\obibvt -config C:\\OracleBI-BVT\\bin\\bvt-config.xml -deployment current INFO: Result folder: Results\\current Throwable: Parameter 'directory' is not a directory Thread[main,5,main] SEVERE: Unhandled Exception SEVERE: java.lang.IllegalArgumentException: Parameter 'directory' is not a directory at org.apache.commons.io.FileUtils.validateListFilesParameters(FileUtils.java:545) at org.apache.commons.io.FileUtils.listFiles(FileUtils.java:521) at org.apache.commons.io.FileUtils.listFiles(FileUtils.java:691) at com.oracle.biee.bvt.UpgradeTool.loadPlugins(UpgradeTool.java:537) at com.","keywords":null,"title":"OBIEE Baseline Validation Tool - Parameter \u0026lsquo;directory\u0026rsquo; is not a directory","uri":"https://rmoff.github.io/2016/05/18/obiee-baseline-validation-tool-parameter-directory-is-not-a-directory/"},{"categories":["logstash","timelion","kibana","elasticsearch","monitoring","ingest"],"content":"Yesterday I wrote about Monitoring Logstash Ingest Rates with InfluxDB and Grafana, in which InfluxDB provided the data store for the ingest rate data, and Grafana the frontend.\nMark Walkom reminded me on twitter that the next release of Logstash will add more functionality in this area - and that it\u0026rsquo;ll integrate back into the Elastic stack:\n@rmoff nice, LS 5.0 will have APIs exposing metrics too. they‚Äôll be integrated back into Marvel/Monitoring!","keywords":null,"title":"Monitoring Logstash Ingest Rates with Elasticsearch, Kibana, and Timelion","uri":"https://rmoff.github.io/2016/05/13/monitoring-logstash-ingest-rates-with-elasticsearch-kibana-and-timelion/"},{"categories":["influxdb","grafana","logstash","graphite","monitoring","ingest"],"content":"In this article I\u0026rsquo;m going to show you how to easily monitor the rate at which Logstash is ingesting data, as well as in future articles the rate at which Elasticsearch is indexing it. It\u0026rsquo;s a nice little touch to add to any project involving Logstash, and it\u0026rsquo;s easy to do.\nLogstash is powerful tool for data ingest, processing, and distribution. It originated as simply the pipe to slurp at log files and put them into Elasticsearch, but has evolved into a whole bunch more.","keywords":null,"title":"Monitoring Logstash Ingest Rates with InfluxDB and Grafana","uri":"https://rmoff.github.io/2016/05/12/monitoring-logstash-ingest-rates-with-influxdb-and-grafana/"},{"categories":["conferences","speaking","user groups","abstract"],"content":"Here\u0026rsquo;s a collection of useful articles that I\u0026rsquo;ve found over the years that give good advice on writing a good abstract, mistakes to avoid, etc:\n Adam Machanic - Capturing Attention: Writing Great Session Descriptions Gwen Shapira - Concrete Advice for Abstract Writers Kellyn Pot‚ÄôVin-Gorman - Abstracts, Reviews and Conferences, Oh My! Russ Unger - Conference Proposals that Don‚Äôt Suck Martin Widlake - Tips on Submitting an Abstract to Conference Bridget Kromhout - give actionable takeaways  (post photo courtesy of Calum MacAulay on https://unsplash.","keywords":null,"title":"Collection of Articles on How to Write a Good Conference Abstract","uri":"https://rmoff.github.io/2016/05/05/collection-of-articles-on-how-to-write-a-good-conference-abstract/"},{"categories":["R","dft","kibana","elasticsearch","dplyr","lubridate","wrangling","elastic"],"content":"Kibana is a tool from Elastic that makes analysis of data held in Elasticsearch really easy and very powerful. Because Elasticsearch has very loose schema that can evolve on demand it makes it very quick to get up and running with some cool visualisations and analysis on any set of data. I demonstrated this in a blog post last year, taking a CSV file and loading it into Elasticsearch via Logstash.","keywords":null,"title":"Using R to Denormalise Data for Analysis in Kibana","uri":"https://rmoff.github.io/2016/04/24/using-r-to-denormalise-data-for-analysis-in-kibana/"},{"categories":["obiee"],"content":"Two vulns for OBIEE in the latest critical patch update (CPU): http://www.oracle.com/technetwork/security-advisory/cpuapr2016v3-2985753.html?elq_mid=45463\u0026amp;sh=91225181314122121267715271910\u0026amp;cmid=WWMK10067711MPP001C140\nPatches is bundle patch .160419:\n 12.2.1: https://support.oracle.com/epmos/faces/ui/patch/PatchDetail.jspx?parent=DOCUMENT\u0026amp;sourceId=2102148.1\u0026amp;patchId=22734181 11.1.1.9: https://support.oracle.com/epmos/faces/ui/patch/PatchDetail.jspx?parent=DOCUMENT\u0026amp;sourceId=2102148.1\u0026amp;patchId=22393988 11.1.1.7: https://support.oracle.com/epmos/faces/ui/patch/PatchDetail.jspx?parent=DOCUMENT\u0026amp;sourceId=2102148.1\u0026amp;patchId=22225110  Note that April 2016 is the last regular patchset for 11.1.1.7, ref: https://support.oracle.com/epmos/faces/DocumentDisplay?id=2102148.1#mozTocId410847. If you\u0026rsquo;re still on it, or earlier, time to upgrade!\n(Photo credit: https://unsplash.com/@jenlittlebirdie)","keywords":null,"title":"OBIEE security patches, and FINAL 11.1.1.7 patchset release","uri":"https://rmoff.github.io/2016/04/18/obiee-security-patches-and-final-11.1.1.7-patchset-release/"},{"categories":["elasticsearch","goldengate","kafka","logstash","oracle"],"content":"Recently added to the oracledi project over at java.net is an adaptor enabling Oracle GoldenGate (OGG) to send data to Elasticsearch. This adds a powerful alternative to [micro-]batch extract via JDBC from Oracle to Elasticsearch, which I wrote about recently over at the Elastic blog.\nElasticsearch is a \u0026lsquo;document store\u0026rsquo; widely used for both search and analytics. It\u0026rsquo;s something I\u0026rsquo;ve written a lot about (here and here for archives), as well as spoken about - preaching the good word, as it were, since the Elastic stack as a whole is very very good at what it does and a pleasure to work with.","keywords":null,"title":"Streaming Data through Oracle GoldenGate to Elasticsearch","uri":"https://rmoff.github.io/2016/04/14/streaming-data-through-oracle-goldengate-to-elasticsearch/"},{"categories":["apache kafka","kafka","logstash","elastic","elasticsearch","kibana","elastic v5","zookeeper"],"content":"I\u0026rsquo;ve recently been playing around with the ELK stack (now officially known as the Elastic stack) collecting data from an IRC channel with Elastic\u0026rsquo;s Logstash, storing it in Elasticsearch and analysing it with Kibana. But, this isn\u0026rsquo;t an \u0026ldquo;ELK\u0026rdquo; post - this is a Kafka post! ELK is just some example data manipulation tooling that helps demonstrate the principles.\nAs I wrote about last year, Apache Kafka provides a handy way to build flexible \u0026ldquo;pipelines\u0026rdquo;.","keywords":null,"title":"Decoupling the Data Pipeline with Kafka - A (Very) Simple Real Life Example","uri":"https://rmoff.github.io/2016/04/12/decoupling-the-data-pipeline-with-kafka-a-very-simple-real-life-example/"},{"categories":["lardy cake","dorset","devon","gyle 59","palmers","beer","fryup","fullenglish"],"content":"On a family holiday in South Devon last week I had some good food experiences. Top of the pile was a Lardy Cake, a delicacy new to me but which I\u0026rsquo;ll be sure to be searching out again. It reminded me of an Eccles cake, but bigger and lardier!\nI got mine from Vinnicombes on the High Street in Sidmouth.\nJust down the coast from Sidmouth is a village called Beer.","keywords":null,"title":"Food Pr0n 02 - Devon \u0026amp; Dorset","uri":"https://rmoff.github.io/2016/04/11/food-pr0n-02-devon-dorset/"},{"categories":["kibana","timelion","quandl","topbeat"],"content":"Timelion was released in November 2015 and with the 4.4.2 release of Kibana is available as a native visualisation once installed. It adds some powerful capabilities to Kibana as an timeseries analysis tool, using its own data manipulation language.\nInstalling Timelion is a piece of cake:\n./bin/kibana plugin -i kibana/timelion  After restarting Kibana, you\u0026rsquo;ll see it as an option from the application picker\nThere\u0026rsquo;s a bit of a learning curve with Timelion, but it\u0026rsquo;s worth it.","keywords":null,"title":"Experiments with Kibana Timelion","uri":"https://rmoff.github.io/2016/03/29/experiments-with-kibana-timelion/"},{"categories":["obiee","jdbc","jisql","logical sql"],"content":"OBIEE supports JDBC as a connection protocol, using the driver available on all installations of OBIEE, bijdbc.jar. This makes connecting to OBIEE from custom or third-party applications very easy. Once connected, you issue \u0026ldquo;Logical SQL\u0026rdquo; against the \u0026ldquo;tables\u0026rdquo; of the Presentation Layer. An example of logical SQL is:\nSELECT \u0026quot;Time\u0026quot;.\u0026quot;T05 Per Name Year\u0026quot; saw_0 FROM \u0026quot;A - Sample Sales\u0026quot;  To find more Logical SQL simply inspect your nqquery.log (obis-query.log in 12c), or Usage Tracking.","keywords":null,"title":"Connecting to OBIEE via JDBC - with jisql","uri":"https://rmoff.github.io/2016/03/28/connecting-to-obiee-via-jdbc-with-jisql/"},{"categories":["logstash","kibana","elasticsearch","irc","obihackers"],"content":"OK, maybe that\u0026rsquo;s not entirely true. But my read-only client, certainly.\nI was perusing the Logstash input plugins recently when I noticed that there was one for IRC. Being a fan of IRC and a regular on the #obihackers channel, I thought this could be fun and yet another great example of how easy the Elastic stack is to work with.\nInstallation is a piece of cake:\nwget https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/zip/elasticsearch/2.2.1/elasticsearch-2.2.1.zip wget https://download.","keywords":null,"title":"My latest IRC client : Kibana","uri":"https://rmoff.github.io/2016/03/24/my-latest-irc-client-kibana/"},{"categories":["food","fullenglish","hawksmoor","burger","indian","black pudding","suet pudding","cubano"],"content":"One of the perks of my job is that I get to travel to some pretty nice places (hi, San Francisco, Bergen, √Öland Islands), and get to eat some pretty good food too. If you\u0026rsquo;re looking for some techie content, then move along and go and read about Kafka, but if you enjoy food pr0n then stay put.\nI was working for a client in the centre of Manchester this week, staying as usual at a Premier Inn.","keywords":null,"title":"Food Pr0n - 01","uri":"https://rmoff.github.io/2016/03/19/food-pr0n-01/"},{"categories":["obiee","installation","jps-06514","jdk","environment variables","wls","keystore"],"content":"I got this lovely failure during a fresh install of OBIEE 11.1.1.9. I emphasise that it was during the install because there\u0026rsquo;s other causes for this error on an existing system to do with corrupted credential stores etc \u0026ndash; not the case here.\nThe install had copied in the binaries and was in the process of building the domain. During the early stages of this where it starts configuring and restarting the AdminServer it failed, with the AdminServer.","keywords":null,"title":"OBIEE 11.1.1.9 installation - JPS-06514: Opening of file based keystore failed","uri":"https://rmoff.github.io/2016/03/18/obiee-11.1.1.9-installation-jps-06514-opening-of-file-based-keystore-failed/"},{"categories":["logstash","kafka","goldengate","avro","elasticsearch"],"content":"The Oracle by Example (ObE) here demonstrating how to use Goldengate to replicate transactions big data targets such as HDFS is written for the BigDataLite 4.2.1, and for me didn\u0026rsquo;t work on the current latest version, 4.4.0.\nThe OBE (and similar Hands On Lab PDF) assume the presence of pmov.prm and pmov.properties in /u01/ogg/dirprm/. On BDL 4.4 there\u0026rsquo;s only the extract to from Oracle configuration, emov.\nFortunately it\u0026rsquo;s still possible to run this setup out of the box in BDL 4.","keywords":null,"title":"Fun and Games with Oracle GoldenGate, Kafka, and Logstash on BigDataLite 4.4","uri":"https://rmoff.github.io/2016/03/16/fun-and-games-with-oracle-goldengate-kafka-and-logstash-on-bigdatalite-4.4/"},{"categories":null,"content":"I\u0026rsquo;ve always defaulted to Slideshare for hosting slides from presentations that I\u0026rsquo;ve given, but it\u0026rsquo;s become more and more crap-infested. The UI is messy, and the UX sucks - for example, I want to download a slide deck, I most definitely 100% am not interested in \u0026ldquo;clipping\u0026rdquo; it\u0026hellip;even if you ask me every. damn. time:\nLooking around it seems the other popular option is Speakerdeck. The UI is clean and simple, and I as both a user and uploader I feel like I\u0026rsquo;m there to read and share slides rather than be monetised as an eyeball on the site.","keywords":null,"title":"Presentation Slides‚Ä¶ bye-bye Slideshare, hello Speakerdeck","uri":"https://rmoff.github.io/2016/03/09/presentation-slides-bye-bye-slideshare-hello-speakerdeck/"},{"categories":null,"content":"#obihackers There\u0026rsquo;s a #obihackers IRC channel on freenode, where a dozen or so of us have hung out for several years now. Chat is usually OBIEE, Oracle, ODI, and general geek out.\nBear in mind this is the equivalent of us hanging out in a bar; if you wanna shoot the shit with a geeky question about OBIEE go ahead, but if you\u0026rsquo;ve come to get help with your homework without even buying a round, you\u0026rsquo;ll probably get short shrift\u0026hellip; ;-)","keywords":null,"title":"#obihackers IRC channel","uri":"https://rmoff.github.io/2016/03/03/obihackers-irc-channel/"},{"categories":["influxdb","metrics","bash","awk","sed","du","curl","grafana"],"content":"InfluxDB is a great time series database, that\u0026rsquo;s recently been rebranded as part of the \u0026ldquo;TICK\u0026rdquo; stack, including data collectors, visualisation, and ETL/Alerting. I\u0026rsquo;ve yet to really look at the other components, but InfluxDB alone works just great with my favourite visualisation/analysis tool for time series metrics, Grafana.\nGetting data into InfluxDB is easy, with many tools supporting the native InfluxDB line input protocol, and those that don\u0026rsquo;t often supporting the carbon protocol (from Graphite), which InfluxDB also supports (along with others).","keywords":null,"title":"Streaming data to InfluxDB from any bash command","uri":"https://rmoff.github.io/2016/02/27/streaming-data-to-influxdb-from-any-bash-command/"},{"categories":["food","fullenglish"],"content":"Thanks to the power of twitter, I can look back on all the many and varied Full English breakfasts that I\u0026rsquo;ve (mostly) enjoyed: https://twitter.com/search?q=rmoff%20%23fullenglish\u0026amp;src=typd\nWhat makes a good Full English?\n Good ingredients, cooked well. Nothing worse than a limp pink sausage, as it were. Sausage, standard pork or Cumberland at most. Definitely no daft apricot and guava bean with a hint of foie gras nonsense. Must be cooked right well, crispy skin, almost burnt.","keywords":null,"title":"#FullEnglish","uri":"https://rmoff.github.io/2016/02/26/fullenglish/"},{"categories":["obiee","dms","metrics","jmanage","jmx"],"content":"It struck me today when I was writing my most recent blog over at Rittman Mead that I\u0026rsquo;ve been playing with visualising OBIEE metrics for years now.\n Back in 2009 I wrote about using something called JManage to pull metrics out of OBIEE 10g via JMX:\n Still with OBIEE 10g in 2011, I was using rrdtool and some horrible-looking tcl hacking to get the metrics out through jmx :","keywords":null,"title":"Visualising OBIEE DMS Metrics over the years","uri":"https://rmoff.github.io/2016/02/26/visualising-obiee-dms-metrics-over-the-years/"},{"categories":null,"content":"Robin Moffatt is a Developer Advocate at Confluent, and Oracle Groundbreaker Ambassador. He also likes writing about himself in the third person, eating good breakfasts, and drinking good beer.","keywords":null,"title":"","uri":"https://rmoff.github.io/authors/robin-moffatt/"},{"categories":null,"content":"","keywords":null,"title":"/Proc/Meminfo","uri":"https://rmoff.github.io/categories/proc-meminfo/"},{"categories":null,"content":"","keywords":null,"title":"/Proc/Meminfo","uri":"https://rmoff.github.io/tag/proc-meminfo/"},{"categories":null,"content":"My primary blog writing is elsewhere (previously). I write at rmoff.net for random tech scribblings and notes, and pictures of fried breakfasts.\nwhoami  Robin is a Developer Advocate at Confluent, the company founded by the creators of Apache Kafka, as well as an Oracle Groundbreaker Ambassador and ACE Director (alumnus). His career has always involved data, from the old worlds of COBOL and DB2, through the worlds of Oracle and Hadoop, and into the current world with Kafka.","keywords":null,"title":"About Me","uri":"https://rmoff.github.io/about-me/"},{"categories":null,"content":"","keywords":null,"title":"Abstract","uri":"https://rmoff.github.io/categories/abstract/"},{"categories":null,"content":"","keywords":null,"title":"Abstract","uri":"https://rmoff.github.io/tag/abstract/"},{"categories":null,"content":"","keywords":null,"title":"Abstracts","uri":"https://rmoff.github.io/categories/abstracts/"},{"categories":null,"content":"","keywords":null,"title":"Adminclient","uri":"https://rmoff.github.io/categories/adminclient/"},{"categories":null,"content":"","keywords":null,"title":"Adminclient","uri":"https://rmoff.github.io/tag/adminclient/"},{"categories":null,"content":"","keywords":null,"title":"Adoc","uri":"https://rmoff.github.io/categories/adoc/"},{"categories":null,"content":"","keywords":null,"title":"Adoc","uri":"https://rmoff.github.io/tag/adoc/"},{"categories":null,"content":"","keywords":null,"title":"Advertised.listeners","uri":"https://rmoff.github.io/categories/advertised.listeners/"},{"categories":null,"content":"","keywords":null,"title":"Advertised.listeners","uri":"https://rmoff.github.io/tag/advertised.listeners/"},{"categories":null,"content":"","keywords":null,"title":"Aggregate","uri":"https://rmoff.github.io/categories/aggregate/"},{"categories":null,"content":"","keywords":null,"title":"Aggregate","uri":"https://rmoff.github.io/tag/aggregate/"},{"categories":null,"content":"","keywords":null,"title":"Apache Drill","uri":"https://rmoff.github.io/categories/apache-drill/"},{"categories":null,"content":"","keywords":null,"title":"Apache Drill","uri":"https://rmoff.github.io/tag/apache-drill/"},{"categories":null,"content":"","keywords":null,"title":"Apache Kafka","uri":"https://rmoff.github.io/categories/apache-kafka/"},{"categories":null,"content":"","keywords":null,"title":"Apache Kafka","uri":"https://rmoff.github.io/tag/apache-kafka/"},{"categories":null,"content":"","keywords":null,"title":"Apcupsd","uri":"https://rmoff.github.io/categories/apcupsd/"},{"categories":null,"content":"","keywords":null,"title":"Apcupsd","uri":"https://rmoff.github.io/tag/apcupsd/"},{"categories":null,"content":"","keywords":null,"title":"Api","uri":"https://rmoff.github.io/categories/api/"},{"categories":null,"content":"","keywords":null,"title":"Api","uri":"https://rmoff.github.io/tag/api/"},{"categories":null,"content":"","keywords":null,"title":"Apple","uri":"https://rmoff.github.io/categories/apple/"},{"categories":null,"content":"","keywords":null,"title":"Apple","uri":"https://rmoff.github.io/tag/apple/"},{"categories":null,"content":"","keywords":null,"title":"Apple Keyboard","uri":"https://rmoff.github.io/categories/apple-keyboard/"},{"categories":null,"content":"","keywords":null,"title":"Apple Keyboard","uri":"https://rmoff.github.io/tag/apple-keyboard/"},{"categories":null,"content":"","keywords":null,"title":"Apple Pencil","uri":"https://rmoff.github.io/categories/apple-pencil/"},{"categories":null,"content":"","keywords":null,"title":"Apple Pencil","uri":"https://rmoff.github.io/tag/apple-pencil/"},{"categories":null,"content":"","keywords":null,"title":"Apple Watch","uri":"https://rmoff.github.io/categories/apple-watch/"},{"categories":null,"content":"","keywords":null,"title":"Apple Watch","uri":"https://rmoff.github.io/tag/apple-watch/"},{"categories":null,"content":"","keywords":null,"title":"Apt Get","uri":"https://rmoff.github.io/categories/apt-get/"},{"categories":null,"content":"","keywords":null,"title":"Apt Get","uri":"https://rmoff.github.io/tag/apt-get/"},{"categories":null,"content":"","keywords":null,"title":"Asciidoc","uri":"https://rmoff.github.io/categories/asciidoc/"},{"categories":null,"content":"","keywords":null,"title":"Asciidoc","uri":"https://rmoff.github.io/tag/asciidoc/"},{"categories":null,"content":"","keywords":null,"title":"Asciinema","uri":"https://rmoff.github.io/categories/asciinema/"},{"categories":null,"content":"","keywords":null,"title":"Asciinema","uri":"https://rmoff.github.io/tag/asciinema/"},{"categories":null,"content":"","keywords":null,"title":"Authors","uri":"https://rmoff.github.io/authors/"},{"categories":null,"content":"","keywords":null,"title":"Avro","uri":"https://rmoff.github.io/categories/avro/"},{"categories":null,"content":"","keywords":null,"title":"Avro","uri":"https://rmoff.github.io/tag/avro/"},{"categories":null,"content":"","keywords":null,"title":"Awk","uri":"https://rmoff.github.io/categories/awk/"},{"categories":null,"content":"","keywords":null,"title":"Awk","uri":"https://rmoff.github.io/tag/awk/"},{"categories":null,"content":"","keywords":null,"title":"Aws","uri":"https://rmoff.github.io/categories/aws/"},{"categories":null,"content":"","keywords":null,"title":"Aws","uri":"https://rmoff.github.io/tag/aws/"},{"categories":null,"content":"","keywords":null,"title":"Bar","uri":"https://rmoff.github.io/categories/bar/"},{"categories":null,"content":"","keywords":null,"title":"Bar","uri":"https://rmoff.github.io/tag/bar/"},{"categories":null,"content":"","keywords":null,"title":"Baseline Validation Tool","uri":"https://rmoff.github.io/categories/baseline-validation-tool/"},{"categories":null,"content":"","keywords":null,"title":"Baseline Validation Tool","uri":"https://rmoff.github.io/tag/baseline-validation-tool/"},{"categories":null,"content":"","keywords":null,"title":"Bash","uri":"https://rmoff.github.io/categories/bash/"},{"categories":null,"content":"","keywords":null,"title":"Bash","uri":"https://rmoff.github.io/tag/bash/"},{"categories":null,"content":"","keywords":null,"title":"Beer","uri":"https://rmoff.github.io/categories/beer/"},{"categories":null,"content":"","keywords":null,"title":"Beer","uri":"https://rmoff.github.io/tag/beer/"},{"categories":null,"content":"","keywords":null,"title":"Bigdatalite","uri":"https://rmoff.github.io/categories/bigdatalite/"},{"categories":null,"content":"","keywords":null,"title":"Bigdatalite","uri":"https://rmoff.github.io/tag/bigdatalite/"},{"categories":null,"content":"","keywords":null,"title":"Bittorrent Sync","uri":"https://rmoff.github.io/categories/bittorrent-sync/"},{"categories":null,"content":"","keywords":null,"title":"Bittorrent Sync","uri":"https://rmoff.github.io/tag/bittorrent-sync/"},{"categories":null,"content":"","keywords":null,"title":"Black Pudding","uri":"https://rmoff.github.io/categories/black-pudding/"},{"categories":null,"content":"","keywords":null,"title":"Black Pudding","uri":"https://rmoff.github.io/tag/black-pudding/"},{"categories":null,"content":"","keywords":null,"title":"Blogging","uri":"https://rmoff.github.io/categories/blogging/"},{"categories":null,"content":"","keywords":null,"title":"Blogging","uri":"https://rmoff.github.io/tag/blogging/"},{"categories":null,"content":"","keywords":null,"title":"Books","uri":"https://rmoff.github.io/categories/books/"},{"categories":null,"content":"","keywords":null,"title":"Books","uri":"https://rmoff.github.io/tag/books/"},{"categories":null,"content":"","keywords":null,"title":"Boot.properties","uri":"https://rmoff.github.io/categories/boot.properties/"},{"categories":null,"content":"","keywords":null,"title":"Boot.properties","uri":"https://rmoff.github.io/tag/boot.properties/"},{"categories":null,"content":"","keywords":null,"title":"Bootable Usb","uri":"https://rmoff.github.io/categories/bootable-usb/"},{"categories":null,"content":"","keywords":null,"title":"Bootable Usb","uri":"https://rmoff.github.io/tag/bootable-usb/"},{"categories":null,"content":"","keywords":null,"title":"Boto","uri":"https://rmoff.github.io/categories/boto/"},{"categories":null,"content":"","keywords":null,"title":"Boto","uri":"https://rmoff.github.io/tag/boto/"},{"categories":null,"content":"","keywords":null,"title":"Brew","uri":"https://rmoff.github.io/categories/brew/"},{"categories":null,"content":"","keywords":null,"title":"Brew","uri":"https://rmoff.github.io/tag/brew/"},{"categories":null,"content":"","keywords":null,"title":"Burger","uri":"https://rmoff.github.io/categories/burger/"},{"categories":null,"content":"","keywords":null,"title":"Burger","uri":"https://rmoff.github.io/tag/burger/"},{"categories":null,"content":"","keywords":null,"title":"Bvt","uri":"https://rmoff.github.io/categories/bvt/"},{"categories":null,"content":"","keywords":null,"title":"Bvt","uri":"https://rmoff.github.io/tag/bvt/"},{"categories":null,"content":"","keywords":null,"title":"Calendar","uri":"https://rmoff.github.io/categories/calendar/"},{"categories":null,"content":"","keywords":null,"title":"Calendar","uri":"https://rmoff.github.io/tag/calendar/"},{"categories":null,"content":"","keywords":null,"title":"Career","uri":"https://rmoff.github.io/categories/career/"},{"categories":null,"content":"","keywords":null,"title":"Career","uri":"https://rmoff.github.io/tag/career/"},{"categories":null,"content":"","keywords":null,"title":"Categories","uri":"https://rmoff.github.io/categories/"},{"categories":null,"content":"","keywords":null,"title":"Cdc","uri":"https://rmoff.github.io/categories/cdc/"},{"categories":null,"content":"","keywords":null,"title":"Cdc","uri":"https://rmoff.github.io/tag/cdc/"},{"categories":null,"content":"","keywords":null,"title":"Cdh","uri":"https://rmoff.github.io/categories/cdh/"},{"categories":null,"content":"","keywords":null,"title":"Cdh","uri":"https://rmoff.github.io/tag/cdh/"},{"categories":null,"content":"","keywords":null,"title":"Centos","uri":"https://rmoff.github.io/categories/centos/"},{"categories":null,"content":"","keywords":null,"title":"Centos","uri":"https://rmoff.github.io/tag/centos/"},{"categories":null,"content":"","keywords":null,"title":"Cidr","uri":"https://rmoff.github.io/categories/cidr/"},{"categories":null,"content":"","keywords":null,"title":"Cidr","uri":"https://rmoff.github.io/tag/cidr/"},{"categories":null,"content":"","keywords":null,"title":"Classnotfoundexception","uri":"https://rmoff.github.io/categories/classnotfoundexception/"},{"categories":null,"content":"","keywords":null,"title":"Classnotfoundexception","uri":"https://rmoff.github.io/tag/classnotfoundexception/"},{"categories":null,"content":"","keywords":null,"title":"Classpath","uri":"https://rmoff.github.io/categories/classpath/"},{"categories":null,"content":"","keywords":null,"title":"Classpath","uri":"https://rmoff.github.io/tag/classpath/"},{"categories":null,"content":"","keywords":null,"title":"Cloudera","uri":"https://rmoff.github.io/categories/cloudera/"},{"categories":null,"content":"","keywords":null,"title":"Cloudera","uri":"https://rmoff.github.io/tag/cloudera/"},{"categories":null,"content":"","keywords":null,"title":"Command Substitution","uri":"https://rmoff.github.io/categories/command-substitution/"},{"categories":null,"content":"","keywords":null,"title":"Command Substitution","uri":"https://rmoff.github.io/tag/command-substitution/"},{"categories":null,"content":"","keywords":null,"title":"Conferences","uri":"https://rmoff.github.io/categories/conferences/"},{"categories":null,"content":"","keywords":null,"title":"Conferences","uri":"https://rmoff.github.io/tag/conferences/"},{"categories":null,"content":"","keywords":null,"title":"Confluent","uri":"https://rmoff.github.io/categories/confluent/"},{"categories":null,"content":"","keywords":null,"title":"Confluent","uri":"https://rmoff.github.io/tag/confluent/"},{"categories":null,"content":"","keywords":null,"title":"Confluent Platform","uri":"https://rmoff.github.io/categories/confluent-platform/"},{"categories":null,"content":"","keywords":null,"title":"Confluent Platform","uri":"https://rmoff.github.io/tag/confluent-platform/"},{"categories":null,"content":"","keywords":null,"title":"Cubano","uri":"https://rmoff.github.io/categories/cubano/"},{"categories":null,"content":"","keywords":null,"title":"Cubano","uri":"https://rmoff.github.io/tag/cubano/"},{"categories":null,"content":"","keywords":null,"title":"Curl","uri":"https://rmoff.github.io/categories/curl/"},{"categories":null,"content":"","keywords":null,"title":"Curl","uri":"https://rmoff.github.io/tag/curl/"},{"categories":null,"content":"","keywords":null,"title":"Data","uri":"https://rmoff.github.io/categories/data/"},{"categories":null,"content":"","keywords":null,"title":"Data","uri":"https://rmoff.github.io/tag/data/"},{"categories":null,"content":"","keywords":null,"title":"Data Model Cmd","uri":"https://rmoff.github.io/categories/data-model-cmd/"},{"categories":null,"content":"","keywords":null,"title":"Data Model Cmd","uri":"https://rmoff.github.io/tag/data-model-cmd/"},{"categories":null,"content":"","keywords":null,"title":"Dataset","uri":"https://rmoff.github.io/categories/dataset/"},{"categories":null,"content":"","keywords":null,"title":"Dataset","uri":"https://rmoff.github.io/tag/dataset/"},{"categories":null,"content":"","keywords":null,"title":"Datasetsvc","uri":"https://rmoff.github.io/categories/datasetsvc/"},{"categories":null,"content":"","keywords":null,"title":"Datasetsvc","uri":"https://rmoff.github.io/tag/datasetsvc/"},{"categories":null,"content":"","keywords":null,"title":"Debezium","uri":"https://rmoff.github.io/categories/debezium/"},{"categories":null,"content":"","keywords":null,"title":"Debezium","uri":"https://rmoff.github.io/tag/debezium/"},{"categories":null,"content":"","keywords":null,"title":"Devon","uri":"https://rmoff.github.io/categories/devon/"},{"categories":null,"content":"","keywords":null,"title":"Devon","uri":"https://rmoff.github.io/tag/devon/"},{"categories":null,"content":"","keywords":null,"title":"Devoxx","uri":"https://rmoff.github.io/categories/devoxx/"},{"categories":null,"content":"","keywords":null,"title":"Devoxx","uri":"https://rmoff.github.io/tag/devoxx/"},{"categories":null,"content":"","keywords":null,"title":"Dft","uri":"https://rmoff.github.io/categories/dft/"},{"categories":null,"content":"","keywords":null,"title":"Dft","uri":"https://rmoff.github.io/tag/dft/"},{"categories":null,"content":"","keywords":null,"title":"Diagnostics","uri":"https://rmoff.github.io/categories/diagnostics/"},{"categories":null,"content":"","keywords":null,"title":"Diagnostics","uri":"https://rmoff.github.io/tag/diagnostics/"},{"categories":null,"content":"","keywords":null,"title":"Diagrams","uri":"https://rmoff.github.io/categories/diagrams/"},{"categories":null,"content":"","keywords":null,"title":"Diagrams","uri":"https://rmoff.github.io/tag/diagrams/"},{"categories":null,"content":"","keywords":null,"title":"Diff","uri":"https://rmoff.github.io/categories/diff/"},{"categories":null,"content":"","keywords":null,"title":"Diff","uri":"https://rmoff.github.io/tag/diff/"},{"categories":null,"content":"","keywords":null,"title":"Django","uri":"https://rmoff.github.io/categories/django/"},{"categories":null,"content":"","keywords":null,"title":"Django","uri":"https://rmoff.github.io/tag/django/"},{"categories":null,"content":"","keywords":null,"title":"Dms","uri":"https://rmoff.github.io/categories/dms/"},{"categories":null,"content":"","keywords":null,"title":"Dms","uri":"https://rmoff.github.io/tag/dms/"},{"categories":null,"content":"","keywords":null,"title":"Doag","uri":"https://rmoff.github.io/categories/doag/"},{"categories":null,"content":"","keywords":null,"title":"Doag","uri":"https://rmoff.github.io/tag/doag/"},{"categories":null,"content":"","keywords":null,"title":"Docker","uri":"https://rmoff.github.io/categories/docker/"},{"categories":null,"content":"","keywords":null,"title":"Docker","uri":"https://rmoff.github.io/tag/docker/"},{"categories":null,"content":"","keywords":null,"title":"Docker Compose","uri":"https://rmoff.github.io/categories/docker-compose/"},{"categories":null,"content":"","keywords":null,"title":"Docker Compose","uri":"https://rmoff.github.io/tag/docker-compose/"},{"categories":null,"content":"","keywords":null,"title":"Docx","uri":"https://rmoff.github.io/categories/docx/"},{"categories":null,"content":"","keywords":null,"title":"Docx","uri":"https://rmoff.github.io/tag/docx/"},{"categories":null,"content":"","keywords":null,"title":"Dorset","uri":"https://rmoff.github.io/categories/dorset/"},{"categories":null,"content":"","keywords":null,"title":"Dorset","uri":"https://rmoff.github.io/tag/dorset/"},{"categories":null,"content":"","keywords":null,"title":"Downloadrpd","uri":"https://rmoff.github.io/categories/downloadrpd/"},{"categories":null,"content":"","keywords":null,"title":"Downloadrpd","uri":"https://rmoff.github.io/tag/downloadrpd/"},{"categories":null,"content":"","keywords":null,"title":"Dplyr","uri":"https://rmoff.github.io/categories/dplyr/"},{"categories":null,"content":"","keywords":null,"title":"Dplyr","uri":"https://rmoff.github.io/tag/dplyr/"},{"categories":null,"content":"","keywords":null,"title":"Dropbox","uri":"https://rmoff.github.io/categories/dropbox/"},{"categories":null,"content":"","keywords":null,"title":"Dropbox","uri":"https://rmoff.github.io/tag/dropbox/"},{"categories":null,"content":"","keywords":null,"title":"Du","uri":"https://rmoff.github.io/categories/du/"},{"categories":null,"content":"","keywords":null,"title":"Du","uri":"https://rmoff.github.io/tag/du/"},{"categories":null,"content":"","keywords":null,"title":"E2label","uri":"https://rmoff.github.io/categories/e2label/"},{"categories":null,"content":"","keywords":null,"title":"E2label","uri":"https://rmoff.github.io/tag/e2label/"},{"categories":null,"content":"","keywords":null,"title":"Ec2","uri":"https://rmoff.github.io/categories/ec2/"},{"categories":null,"content":"","keywords":null,"title":"Ec2","uri":"https://rmoff.github.io/tag/ec2/"},{"categories":null,"content":"","keywords":null,"title":"Edgemax","uri":"https://rmoff.github.io/categories/edgemax/"},{"categories":null,"content":"","keywords":null,"title":"Edgemax","uri":"https://rmoff.github.io/tag/edgemax/"},{"categories":null,"content":"","keywords":null,"title":"Edgerouter Lite","uri":"https://rmoff.github.io/categories/edgerouter-lite/"},{"categories":null,"content":"","keywords":null,"title":"Edgerouter Lite","uri":"https://rmoff.github.io/tag/edgerouter-lite/"},{"categories":null,"content":"","keywords":null,"title":"Elastic","uri":"https://rmoff.github.io/categories/elastic/"},{"categories":null,"content":"","keywords":null,"title":"Elastic","uri":"https://rmoff.github.io/tag/elastic/"},{"categories":null,"content":"","keywords":null,"title":"Elastic V5","uri":"https://rmoff.github.io/categories/elastic-v5/"},{"categories":null,"content":"","keywords":null,"title":"Elastic V5","uri":"https://rmoff.github.io/tag/elastic-v5/"},{"categories":null,"content":"","keywords":null,"title":"Elasticsearch","uri":"https://rmoff.github.io/categories/elasticsearch/"},{"categories":null,"content":"","keywords":null,"title":"Elasticsearch","uri":"https://rmoff.github.io/tag/elasticsearch/"},{"categories":null,"content":"","keywords":null,"title":"Emacs","uri":"https://rmoff.github.io/categories/emacs/"},{"categories":null,"content":"","keywords":null,"title":"Emacs","uri":"https://rmoff.github.io/tag/emacs/"},{"categories":null,"content":"","keywords":null,"title":"Environment Variables","uri":"https://rmoff.github.io/categories/environment-variables/"},{"categories":null,"content":"","keywords":null,"title":"Environment Variables","uri":"https://rmoff.github.io/tag/environment-variables/"},{"categories":null,"content":"","keywords":null,"title":"Erl","uri":"https://rmoff.github.io/categories/erl/"},{"categories":null,"content":"","keywords":null,"title":"Erl","uri":"https://rmoff.github.io/tag/erl/"},{"categories":null,"content":"","keywords":null,"title":"Espressi","uri":"https://rmoff.github.io/categories/espressi/"},{"categories":null,"content":"","keywords":null,"title":"Espressi","uri":"https://rmoff.github.io/tag/espressi/"},{"categories":null,"content":"","keywords":null,"title":"Etl","uri":"https://rmoff.github.io/categories/etl/"},{"categories":null,"content":"","keywords":null,"title":"Etl","uri":"https://rmoff.github.io/tag/etl/"},{"categories":null,"content":"","keywords":null,"title":"Exportserviceinstance","uri":"https://rmoff.github.io/categories/exportserviceinstance/"},{"categories":null,"content":"","keywords":null,"title":"Exportserviceinstance","uri":"https://rmoff.github.io/tag/exportserviceinstance/"},{"categories":null,"content":"","keywords":null,"title":"Ext4","uri":"https://rmoff.github.io/categories/ext4/"},{"categories":null,"content":"","keywords":null,"title":"Ext4","uri":"https://rmoff.github.io/tag/ext4/"},{"categories":null,"content":"","keywords":null,"title":"Flashback","uri":"https://rmoff.github.io/categories/flashback/"},{"categories":null,"content":"","keywords":null,"title":"Flashback","uri":"https://rmoff.github.io/tag/flashback/"},{"categories":null,"content":"","keywords":null,"title":"Food","uri":"https://rmoff.github.io/categories/food/"},{"categories":null,"content":"","keywords":null,"title":"Food","uri":"https://rmoff.github.io/tag/food/"},{"categories":null,"content":"","keywords":null,"title":"Formatting","uri":"https://rmoff.github.io/categories/formatting/"},{"categories":null,"content":"","keywords":null,"title":"Formatting","uri":"https://rmoff.github.io/tag/formatting/"},{"categories":null,"content":"","keywords":null,"title":"Fried Slice","uri":"https://rmoff.github.io/categories/fried-slice/"},{"categories":null,"content":"","keywords":null,"title":"Fried Slice","uri":"https://rmoff.github.io/tag/fried-slice/"},{"categories":null,"content":"","keywords":null,"title":"Fryup","uri":"https://rmoff.github.io/categories/fryup/"},{"categories":null,"content":"","keywords":null,"title":"Fryup","uri":"https://rmoff.github.io/tag/fryup/"},{"categories":null,"content":"","keywords":null,"title":"Fullenglish","uri":"https://rmoff.github.io/categories/fullenglish/"},{"categories":null,"content":"","keywords":null,"title":"Fullenglish","uri":"https://rmoff.github.io/tag/fullenglish/"},{"categories":null,"content":"","keywords":null,"title":"Geopoint","uri":"https://rmoff.github.io/categories/geopoint/"},{"categories":null,"content":"","keywords":null,"title":"Geopoint","uri":"https://rmoff.github.io/tag/geopoint/"},{"categories":null,"content":"","keywords":null,"title":"Ghost","uri":"https://rmoff.github.io/categories/ghost/"},{"categories":null,"content":"","keywords":null,"title":"Ghost","uri":"https://rmoff.github.io/tag/ghost/"},{"categories":null,"content":"","keywords":null,"title":"Goldengate","uri":"https://rmoff.github.io/categories/goldengate/"},{"categories":null,"content":"","keywords":null,"title":"Goldengate","uri":"https://rmoff.github.io/tag/goldengate/"},{"categories":null,"content":"","keywords":null,"title":"Grafana","uri":"https://rmoff.github.io/categories/grafana/"},{"categories":null,"content":"","keywords":null,"title":"Grafana","uri":"https://rmoff.github.io/tag/grafana/"},{"categories":null,"content":"","keywords":null,"title":"Graphite","uri":"https://rmoff.github.io/categories/graphite/"},{"categories":null,"content":"","keywords":null,"title":"Graphite","uri":"https://rmoff.github.io/tag/graphite/"},{"categories":null,"content":"","keywords":null,"title":"Gyle 59","uri":"https://rmoff.github.io/categories/gyle-59/"},{"categories":null,"content":"","keywords":null,"title":"Gyle 59","uri":"https://rmoff.github.io/tag/gyle-59/"},{"categories":null,"content":"","keywords":null,"title":"Hang","uri":"https://rmoff.github.io/categories/hang/"},{"categories":null,"content":"","keywords":null,"title":"Hang","uri":"https://rmoff.github.io/tag/hang/"},{"categories":null,"content":"","keywords":null,"title":"Hawksmoor","uri":"https://rmoff.github.io/categories/hawksmoor/"},{"categories":null,"content":"","keywords":null,"title":"Hawksmoor","uri":"https://rmoff.github.io/tag/hawksmoor/"},{"categories":null,"content":"","keywords":null,"title":"Hbase","uri":"https://rmoff.github.io/categories/hbase/"},{"categories":null,"content":"","keywords":null,"title":"Hbase","uri":"https://rmoff.github.io/tag/hbase/"},{"categories":null,"content":"","keywords":null,"title":"Hdiutil","uri":"https://rmoff.github.io/categories/hdiutil/"},{"categories":null,"content":"","keywords":null,"title":"Hdiutil","uri":"https://rmoff.github.io/tag/hdiutil/"},{"categories":null,"content":"","keywords":null,"title":"Headphones","uri":"https://rmoff.github.io/categories/headphones/"},{"categories":null,"content":"","keywords":null,"title":"Headphones","uri":"https://rmoff.github.io/tag/headphones/"},{"categories":null,"content":"","keywords":null,"title":"Headset","uri":"https://rmoff.github.io/categories/headset/"},{"categories":null,"content":"","keywords":null,"title":"Headset","uri":"https://rmoff.github.io/tag/headset/"},{"categories":null,"content":"","keywords":null,"title":"Hive","uri":"https://rmoff.github.io/categories/hive/"},{"categories":null,"content":"","keywords":null,"title":"Hive","uri":"https://rmoff.github.io/tag/hive/"},{"categories":null,"content":"","keywords":null,"title":"Holt","uri":"https://rmoff.github.io/categories/holt/"},{"categories":null,"content":"","keywords":null,"title":"Holt","uri":"https://rmoff.github.io/tag/holt/"},{"categories":null,"content":"","keywords":null,"title":"Home Server","uri":"https://rmoff.github.io/categories/home-server/"},{"categories":null,"content":"","keywords":null,"title":"Home Server","uri":"https://rmoff.github.io/tag/home-server/"},{"categories":null,"content":"","keywords":null,"title":"Hue","uri":"https://rmoff.github.io/categories/hue/"},{"categories":null,"content":"","keywords":null,"title":"Hue","uri":"https://rmoff.github.io/tag/hue/"},{"categories":null,"content":"","keywords":null,"title":"Hugo","uri":"https://rmoff.github.io/categories/hugo/"},{"categories":null,"content":"","keywords":null,"title":"Hugo","uri":"https://rmoff.github.io/tag/hugo/"},{"categories":null,"content":"","keywords":null,"title":"Img","uri":"https://rmoff.github.io/categories/img/"},{"categories":null,"content":"","keywords":null,"title":"Img","uri":"https://rmoff.github.io/tag/img/"},{"categories":null,"content":"","keywords":null,"title":"Incompatibleclasschangeerror","uri":"https://rmoff.github.io/categories/incompatibleclasschangeerror/"},{"categories":null,"content":"","keywords":null,"title":"Incompatibleclasschangeerror","uri":"https://rmoff.github.io/tag/incompatibleclasschangeerror/"},{"categories":null,"content":"","keywords":null,"title":"Indian","uri":"https://rmoff.github.io/categories/indian/"},{"categories":null,"content":"","keywords":null,"title":"Indian","uri":"https://rmoff.github.io/tag/indian/"},{"categories":null,"content":"","keywords":null,"title":"Influxdb","uri":"https://rmoff.github.io/categories/influxdb/"},{"categories":null,"content":"","keywords":null,"title":"Influxdb","uri":"https://rmoff.github.io/tag/influxdb/"},{"categories":null,"content":"","keywords":null,"title":"Ingest","uri":"https://rmoff.github.io/categories/ingest/"},{"categories":null,"content":"","keywords":null,"title":"Ingest","uri":"https://rmoff.github.io/tag/ingest/"},{"categories":null,"content":"","keywords":null,"title":"Installation","uri":"https://rmoff.github.io/categories/installation/"},{"categories":null,"content":"","keywords":null,"title":"Installation","uri":"https://rmoff.github.io/tag/installation/"},{"categories":null,"content":"","keywords":null,"title":"Ios","uri":"https://rmoff.github.io/categories/ios/"},{"categories":null,"content":"","keywords":null,"title":"Ios","uri":"https://rmoff.github.io/tag/ios/"},{"categories":null,"content":"","keywords":null,"title":"Ipad","uri":"https://rmoff.github.io/categories/ipad/"},{"categories":null,"content":"","keywords":null,"title":"Ipad","uri":"https://rmoff.github.io/tag/ipad/"},{"categories":null,"content":"","keywords":null,"title":"Ipad Pro","uri":"https://rmoff.github.io/categories/ipad-pro/"},{"categories":null,"content":"","keywords":null,"title":"Ipad Pro","uri":"https://rmoff.github.io/tag/ipad-pro/"},{"categories":null,"content":"","keywords":null,"title":"Irc","uri":"https://rmoff.github.io/categories/irc/"},{"categories":null,"content":"","keywords":null,"title":"Irc","uri":"https://rmoff.github.io/tag/irc/"},{"categories":null,"content":"","keywords":null,"title":"Jar","uri":"https://rmoff.github.io/categories/jar/"},{"categories":null,"content":"","keywords":null,"title":"Jar","uri":"https://rmoff.github.io/tag/jar/"},{"categories":null,"content":"","keywords":null,"title":"Javaone","uri":"https://rmoff.github.io/categories/javaone/"},{"categories":null,"content":"","keywords":null,"title":"Javaone","uri":"https://rmoff.github.io/tag/javaone/"},{"categories":null,"content":"","keywords":null,"title":"Javazone","uri":"https://rmoff.github.io/categories/javazone/"},{"categories":null,"content":"","keywords":null,"title":"Javazone","uri":"https://rmoff.github.io/tag/javazone/"},{"categories":null,"content":"","keywords":null,"title":"Jdbc","uri":"https://rmoff.github.io/categories/jdbc/"},{"categories":null,"content":"","keywords":null,"title":"Jdbc","uri":"https://rmoff.github.io/tag/jdbc/"},{"categories":null,"content":"","keywords":null,"title":"Jdbc Sink","uri":"https://rmoff.github.io/categories/jdbc-sink/"},{"categories":null,"content":"","keywords":null,"title":"Jdbc Sink","uri":"https://rmoff.github.io/tag/jdbc-sink/"},{"categories":null,"content":"","keywords":null,"title":"Jdk","uri":"https://rmoff.github.io/categories/jdk/"},{"categories":null,"content":"","keywords":null,"title":"Jdk","uri":"https://rmoff.github.io/tag/jdk/"},{"categories":null,"content":"","keywords":null,"title":"Jisql","uri":"https://rmoff.github.io/categories/jisql/"},{"categories":null,"content":"","keywords":null,"title":"Jisql","uri":"https://rmoff.github.io/tag/jisql/"},{"categories":null,"content":"","keywords":null,"title":"Jmanage","uri":"https://rmoff.github.io/categories/jmanage/"},{"categories":null,"content":"","keywords":null,"title":"Jmanage","uri":"https://rmoff.github.io/tag/jmanage/"},{"categories":null,"content":"","keywords":null,"title":"Jmx","uri":"https://rmoff.github.io/categories/jmx/"},{"categories":null,"content":"","keywords":null,"title":"Jmx","uri":"https://rmoff.github.io/tag/jmx/"},{"categories":null,"content":"","keywords":null,"title":"Jmxterm","uri":"https://rmoff.github.io/categories/jmxterm/"},{"categories":null,"content":"","keywords":null,"title":"Jmxterm","uri":"https://rmoff.github.io/tag/jmxterm/"},{"categories":null,"content":"","keywords":null,"title":"Join","uri":"https://rmoff.github.io/categories/join/"},{"categories":null,"content":"","keywords":null,"title":"Join","uri":"https://rmoff.github.io/tag/join/"},{"categories":null,"content":"","keywords":null,"title":"Jps 06514","uri":"https://rmoff.github.io/categories/jps-06514/"},{"categories":null,"content":"","keywords":null,"title":"Jps 06514","uri":"https://rmoff.github.io/tag/jps-06514/"},{"categories":null,"content":"","keywords":null,"title":"Jq","uri":"https://rmoff.github.io/categories/jq/"},{"categories":null,"content":"","keywords":null,"title":"Jq","uri":"https://rmoff.github.io/tag/jq/"},{"categories":null,"content":"","keywords":null,"title":"Json","uri":"https://rmoff.github.io/categories/json/"},{"categories":null,"content":"","keywords":null,"title":"Json","uri":"https://rmoff.github.io/tag/json/"},{"categories":null,"content":"","keywords":null,"title":"Jsondeserializer","uri":"https://rmoff.github.io/categories/jsondeserializer/"},{"categories":null,"content":"","keywords":null,"title":"Jsondeserializer","uri":"https://rmoff.github.io/tag/jsondeserializer/"},{"categories":null,"content":"","keywords":null,"title":"Kafka","uri":"https://rmoff.github.io/categories/kafka/"},{"categories":null,"content":"","keywords":null,"title":"Kafka","uri":"https://rmoff.github.io/tag/kafka/"},{"categories":null,"content":"","keywords":null,"title":"Kafka Avro Console Producer","uri":"https://rmoff.github.io/categories/kafka-avro-console-producer/"},{"categories":null,"content":"","keywords":null,"title":"Kafka Avro Console Producer","uri":"https://rmoff.github.io/tag/kafka-avro-console-producer/"},{"categories":null,"content":"","keywords":null,"title":"Kafka Connect","uri":"https://rmoff.github.io/categories/kafka-connect/"},{"categories":null,"content":"","keywords":null,"title":"Kafka Connect","uri":"https://rmoff.github.io/tag/kafka-connect/"},{"categories":null,"content":"","keywords":null,"title":"Kafka_advertised_listeners","uri":"https://rmoff.github.io/categories/kafka_advertised_listeners/"},{"categories":null,"content":"","keywords":null,"title":"Kafka_advertised_listeners","uri":"https://rmoff.github.io/tag/kafka_advertised_listeners/"},{"categories":null,"content":"","keywords":null,"title":"Kafkacat","uri":"https://rmoff.github.io/categories/kafkacat/"},{"categories":null,"content":"","keywords":null,"title":"Kafkacat","uri":"https://rmoff.github.io/tag/kafkacat/"},{"categories":null,"content":"","keywords":null,"title":"Kafkaconnect","uri":"https://rmoff.github.io/categories/kafkaconnect/"},{"categories":null,"content":"","keywords":null,"title":"Kafkaconnect","uri":"https://rmoff.github.io/tag/kafkaconnect/"},{"categories":null,"content":"","keywords":null,"title":"Key","uri":"https://rmoff.github.io/categories/key/"},{"categories":null,"content":"","keywords":null,"title":"Key","uri":"https://rmoff.github.io/tag/key/"},{"categories":null,"content":"","keywords":null,"title":"Keynote","uri":"https://rmoff.github.io/categories/keynote/"},{"categories":null,"content":"","keywords":null,"title":"Keynote","uri":"https://rmoff.github.io/tag/keynote/"},{"categories":null,"content":"","keywords":null,"title":"Keystore","uri":"https://rmoff.github.io/categories/keystore/"},{"categories":null,"content":"","keywords":null,"title":"Keystore","uri":"https://rmoff.github.io/tag/keystore/"},{"categories":null,"content":"","keywords":null,"title":"Kibana","uri":"https://rmoff.github.io/categories/kibana/"},{"categories":null,"content":"","keywords":null,"title":"Kibana","uri":"https://rmoff.github.io/tag/kibana/"},{"categories":null,"content":"","keywords":null,"title":"Knowledge","uri":"https://rmoff.github.io/categories/knowledge/"},{"categories":null,"content":"","keywords":null,"title":"Knowledge","uri":"https://rmoff.github.io/tag/knowledge/"},{"categories":null,"content":"","keywords":null,"title":"Ksql","uri":"https://rmoff.github.io/categories/ksql/"},{"categories":null,"content":"","keywords":null,"title":"Ksql","uri":"https://rmoff.github.io/tag/ksql/"},{"categories":null,"content":"","keywords":null,"title":"Ksql Cli","uri":"https://rmoff.github.io/categories/ksql-cli/"},{"categories":null,"content":"","keywords":null,"title":"Ksql Cli","uri":"https://rmoff.github.io/tag/ksql-cli/"},{"categories":null,"content":"","keywords":null,"title":"Ksql Server","uri":"https://rmoff.github.io/categories/ksql-server/"},{"categories":null,"content":"","keywords":null,"title":"Ksql Server","uri":"https://rmoff.github.io/tag/ksql-server/"},{"categories":null,"content":"","keywords":null,"title":"Lardy Cake","uri":"https://rmoff.github.io/categories/lardy-cake/"},{"categories":null,"content":"","keywords":null,"title":"Lardy Cake","uri":"https://rmoff.github.io/tag/lardy-cake/"},{"categories":null,"content":"","keywords":null,"title":"Licence","uri":"https://rmoff.github.io/categories/licence/"},{"categories":null,"content":"","keywords":null,"title":"Licence","uri":"https://rmoff.github.io/tag/licence/"},{"categories":null,"content":"","keywords":null,"title":"Line","uri":"https://rmoff.github.io/categories/line/"},{"categories":null,"content":"","keywords":null,"title":"Line","uri":"https://rmoff.github.io/tag/line/"},{"categories":null,"content":"","keywords":null,"title":"Lisa","uri":"https://rmoff.github.io/categories/lisa/"},{"categories":null,"content":"","keywords":null,"title":"Lisa","uri":"https://rmoff.github.io/tag/lisa/"},{"categories":null,"content":"","keywords":null,"title":"Listeners","uri":"https://rmoff.github.io/categories/listeners/"},{"categories":null,"content":"","keywords":null,"title":"Listeners","uri":"https://rmoff.github.io/tag/listeners/"},{"categories":null,"content":"","keywords":null,"title":"Log4j","uri":"https://rmoff.github.io/categories/log4j/"},{"categories":null,"content":"","keywords":null,"title":"Log4j","uri":"https://rmoff.github.io/tag/log4j/"},{"categories":null,"content":"","keywords":null,"title":"Logging","uri":"https://rmoff.github.io/categories/logging/"},{"categories":null,"content":"","keywords":null,"title":"Logging","uri":"https://rmoff.github.io/tag/logging/"},{"categories":null,"content":"","keywords":null,"title":"Logical Sql","uri":"https://rmoff.github.io/categories/logical-sql/"},{"categories":null,"content":"","keywords":null,"title":"Logical Sql","uri":"https://rmoff.github.io/tag/logical-sql/"},{"categories":null,"content":"","keywords":null,"title":"Logitech","uri":"https://rmoff.github.io/categories/logitech/"},{"categories":null,"content":"","keywords":null,"title":"Logitech","uri":"https://rmoff.github.io/tag/logitech/"},{"categories":null,"content":"","keywords":null,"title":"Logminer","uri":"https://rmoff.github.io/categories/logminer/"},{"categories":null,"content":"","keywords":null,"title":"Logminer","uri":"https://rmoff.github.io/tag/logminer/"},{"categories":null,"content":"","keywords":null,"title":"Logstash","uri":"https://rmoff.github.io/categories/logstash/"},{"categories":null,"content":"","keywords":null,"title":"Logstash","uri":"https://rmoff.github.io/tag/logstash/"},{"categories":null,"content":"","keywords":null,"title":"Losetup","uri":"https://rmoff.github.io/categories/losetup/"},{"categories":null,"content":"","keywords":null,"title":"Losetup","uri":"https://rmoff.github.io/tag/losetup/"},{"categories":null,"content":"","keywords":null,"title":"Lsblk","uri":"https://rmoff.github.io/categories/lsblk/"},{"categories":null,"content":"","keywords":null,"title":"Lsblk","uri":"https://rmoff.github.io/tag/lsblk/"},{"categories":null,"content":"","keywords":null,"title":"Lubridate","uri":"https://rmoff.github.io/categories/lubridate/"},{"categories":null,"content":"","keywords":null,"title":"Lubridate","uri":"https://rmoff.github.io/tag/lubridate/"},{"categories":null,"content":"","keywords":null,"title":"Lvm","uri":"https://rmoff.github.io/categories/lvm/"},{"categories":null,"content":"","keywords":null,"title":"Lvm","uri":"https://rmoff.github.io/tag/lvm/"},{"categories":null,"content":"","keywords":null,"title":"Lxc","uri":"https://rmoff.github.io/categories/lxc/"},{"categories":null,"content":"","keywords":null,"title":"Lxc","uri":"https://rmoff.github.io/tag/lxc/"},{"categories":null,"content":"","keywords":null,"title":"Mac","uri":"https://rmoff.github.io/categories/mac/"},{"categories":null,"content":"","keywords":null,"title":"Mac","uri":"https://rmoff.github.io/tag/mac/"},{"categories":null,"content":"","keywords":null,"title":"Markdown","uri":"https://rmoff.github.io/categories/markdown/"},{"categories":null,"content":"","keywords":null,"title":"Markdown","uri":"https://rmoff.github.io/tag/markdown/"},{"categories":null,"content":"","keywords":null,"title":"Marked2","uri":"https://rmoff.github.io/categories/marked2/"},{"categories":null,"content":"","keywords":null,"title":"Marked2","uri":"https://rmoff.github.io/tag/marked2/"},{"categories":null,"content":"","keywords":null,"title":"Meetups","uri":"https://rmoff.github.io/categories/meetups/"},{"categories":null,"content":"","keywords":null,"title":"Meetups","uri":"https://rmoff.github.io/tag/meetups/"},{"categories":null,"content":"","keywords":null,"title":"Memtest","uri":"https://rmoff.github.io/categories/memtest/"},{"categories":null,"content":"","keywords":null,"title":"Memtest","uri":"https://rmoff.github.io/tag/memtest/"},{"categories":null,"content":"","keywords":null,"title":"Metrics","uri":"https://rmoff.github.io/categories/metrics/"},{"categories":null,"content":"","keywords":null,"title":"Metrics","uri":"https://rmoff.github.io/tag/metrics/"},{"categories":null,"content":"","keywords":null,"title":"Microphone","uri":"https://rmoff.github.io/categories/microphone/"},{"categories":null,"content":"","keywords":null,"title":"Microphone","uri":"https://rmoff.github.io/tag/microphone/"},{"categories":null,"content":"","keywords":null,"title":"Microsoft","uri":"https://rmoff.github.io/categories/microsoft/"},{"categories":null,"content":"","keywords":null,"title":"Microsoft","uri":"https://rmoff.github.io/tag/microsoft/"},{"categories":null,"content":"","keywords":null,"title":"Mockaroo","uri":"https://rmoff.github.io/categories/mockaroo/"},{"categories":null,"content":"","keywords":null,"title":"Mockaroo","uri":"https://rmoff.github.io/tag/mockaroo/"},{"categories":null,"content":"","keywords":null,"title":"Mogodb","uri":"https://rmoff.github.io/categories/mogodb/"},{"categories":null,"content":"","keywords":null,"title":"Mogodb","uri":"https://rmoff.github.io/tag/mogodb/"},{"categories":null,"content":"","keywords":null,"title":"Mongodb","uri":"https://rmoff.github.io/categories/mongodb/"},{"categories":null,"content":"","keywords":null,"title":"Mongodb","uri":"https://rmoff.github.io/tag/mongodb/"},{"categories":null,"content":"","keywords":null,"title":"Mongodump","uri":"https://rmoff.github.io/categories/mongodump/"},{"categories":null,"content":"","keywords":null,"title":"Mongodump","uri":"https://rmoff.github.io/tag/mongodump/"},{"categories":null,"content":"","keywords":null,"title":"Mongorestore","uri":"https://rmoff.github.io/categories/mongorestore/"},{"categories":null,"content":"","keywords":null,"title":"Mongorestore","uri":"https://rmoff.github.io/tag/mongorestore/"},{"categories":null,"content":"","keywords":null,"title":"Monitoring","uri":"https://rmoff.github.io/categories/monitoring/"},{"categories":null,"content":"","keywords":null,"title":"Monitoring","uri":"https://rmoff.github.io/tag/monitoring/"},{"categories":null,"content":"","keywords":null,"title":"Mount","uri":"https://rmoff.github.io/categories/mount/"},{"categories":null,"content":"","keywords":null,"title":"Mount","uri":"https://rmoff.github.io/tag/mount/"},{"categories":null,"content":"","keywords":null,"title":"Ms Word","uri":"https://rmoff.github.io/categories/ms-word/"},{"categories":null,"content":"","keywords":null,"title":"Ms Word","uri":"https://rmoff.github.io/tag/ms-word/"},{"categories":null,"content":"","keywords":null,"title":"Mtr","uri":"https://rmoff.github.io/categories/mtr/"},{"categories":null,"content":"","keywords":null,"title":"Mtr","uri":"https://rmoff.github.io/tag/mtr/"},{"categories":null,"content":"","keywords":null,"title":"Multiline","uri":"https://rmoff.github.io/categories/multiline/"},{"categories":null,"content":"","keywords":null,"title":"Multiline","uri":"https://rmoff.github.io/tag/multiline/"},{"categories":["rickroll"],"content":" ","keywords":null,"title":"My Favourite Song","uri":"https://rmoff.github.io/my-favourite-song/"},{"categories":null,"content":"","keywords":null,"title":"Mysql","uri":"https://rmoff.github.io/categories/mysql/"},{"categories":null,"content":"","keywords":null,"title":"Mysql","uri":"https://rmoff.github.io/tag/mysql/"},{"categories":null,"content":"","keywords":null,"title":"Networking","uri":"https://rmoff.github.io/categories/networking/"},{"categories":null,"content":"","keywords":null,"title":"Networking","uri":"https://rmoff.github.io/tag/networking/"},{"categories":null,"content":"","keywords":null,"title":"Number","uri":"https://rmoff.github.io/categories/number/"},{"categories":null,"content":"","keywords":null,"title":"Number","uri":"https://rmoff.github.io/tag/number/"},{"categories":null,"content":"","keywords":null,"title":"Oaktable World","uri":"https://rmoff.github.io/categories/oaktable-world/"},{"categories":null,"content":"","keywords":null,"title":"Oaktable World","uri":"https://rmoff.github.io/tag/oaktable-world/"},{"categories":null,"content":"","keywords":null,"title":"Obiee","uri":"https://rmoff.github.io/categories/obiee/"},{"categories":null,"content":"","keywords":null,"title":"Obiee","uri":"https://rmoff.github.io/tag/obiee/"},{"categories":null,"content":"","keywords":null,"title":"Obiee12c","uri":"https://rmoff.github.io/categories/obiee12c/"},{"categories":null,"content":"","keywords":null,"title":"Obiee12c","uri":"https://rmoff.github.io/tag/obiee12c/"},{"categories":null,"content":"","keywords":null,"title":"Obihackers","uri":"https://rmoff.github.io/categories/obihackers/"},{"categories":null,"content":"","keywords":null,"title":"Obihackers","uri":"https://rmoff.github.io/tag/obihackers/"},{"categories":null,"content":"","keywords":null,"title":"Offset","uri":"https://rmoff.github.io/categories/offset/"},{"categories":null,"content":"","keywords":null,"title":"Offset","uri":"https://rmoff.github.io/tag/offset/"},{"categories":null,"content":"","keywords":null,"title":"Ogg","uri":"https://rmoff.github.io/categories/ogg/"},{"categories":null,"content":"","keywords":null,"title":"Ogg","uri":"https://rmoff.github.io/tag/ogg/"},{"categories":null,"content":"","keywords":null,"title":"Ogg 15051","uri":"https://rmoff.github.io/categories/ogg-15051/"},{"categories":null,"content":"","keywords":null,"title":"Ogg 15051","uri":"https://rmoff.github.io/tag/ogg-15051/"},{"categories":null,"content":"","keywords":null,"title":"Oow","uri":"https://rmoff.github.io/categories/oow/"},{"categories":null,"content":"","keywords":null,"title":"Oow","uri":"https://rmoff.github.io/tag/oow/"},{"categories":null,"content":"","keywords":null,"title":"Openworld","uri":"https://rmoff.github.io/categories/openworld/"},{"categories":null,"content":"","keywords":null,"title":"Openworld","uri":"https://rmoff.github.io/tag/openworld/"},{"categories":null,"content":"","keywords":null,"title":"Oracle","uri":"https://rmoff.github.io/categories/oracle/"},{"categories":null,"content":"","keywords":null,"title":"Oracle","uri":"https://rmoff.github.io/tag/oracle/"},{"categories":null,"content":"","keywords":null,"title":"Ova","uri":"https://rmoff.github.io/categories/ova/"},{"categories":null,"content":"","keywords":null,"title":"Ova","uri":"https://rmoff.github.io/tag/ova/"},{"categories":null,"content":"","keywords":null,"title":"Palmers","uri":"https://rmoff.github.io/categories/palmers/"},{"categories":null,"content":"","keywords":null,"title":"Palmers","uri":"https://rmoff.github.io/tag/palmers/"},{"categories":null,"content":"","keywords":null,"title":"Pandoc","uri":"https://rmoff.github.io/categories/pandoc/"},{"categories":null,"content":"","keywords":null,"title":"Pandoc","uri":"https://rmoff.github.io/tag/pandoc/"},{"categories":null,"content":"","keywords":null,"title":"Paper","uri":"https://rmoff.github.io/categories/paper/"},{"categories":null,"content":"","keywords":null,"title":"Paper","uri":"https://rmoff.github.io/tag/paper/"},{"categories":null,"content":"","keywords":null,"title":"Patch","uri":"https://rmoff.github.io/categories/patch/"},{"categories":null,"content":"","keywords":null,"title":"Patch","uri":"https://rmoff.github.io/tag/patch/"},{"categories":null,"content":"","keywords":null,"title":"Paw","uri":"https://rmoff.github.io/categories/paw/"},{"categories":null,"content":"","keywords":null,"title":"Paw","uri":"https://rmoff.github.io/tag/paw/"},{"categories":null,"content":"","keywords":null,"title":"Pebcak","uri":"https://rmoff.github.io/categories/pebcak/"},{"categories":null,"content":"","keywords":null,"title":"Pebcak","uri":"https://rmoff.github.io/tag/pebcak/"},{"categories":null,"content":"","keywords":null,"title":"Peco","uri":"https://rmoff.github.io/categories/peco/"},{"categories":null,"content":"","keywords":null,"title":"Peco","uri":"https://rmoff.github.io/tag/peco/"},{"categories":null,"content":"","keywords":null,"title":"Podcasts","uri":"https://rmoff.github.io/categories/podcasts/"},{"categories":null,"content":"","keywords":null,"title":"Podcasts","uri":"https://rmoff.github.io/tag/podcasts/"},{"categories":null,"content":"","keywords":null,"title":"Postman","uri":"https://rmoff.github.io/categories/postman/"},{"categories":null,"content":"","keywords":null,"title":"Postman","uri":"https://rmoff.github.io/tag/postman/"},{"categories":null,"content":"","keywords":null,"title":"Posts","uri":"https://rmoff.github.io/post/"},{"categories":null,"content":"","keywords":null,"title":"Presenting","uri":"https://rmoff.github.io/categories/presenting/"},{"categories":null,"content":"","keywords":null,"title":"Presenting","uri":"https://rmoff.github.io/tag/presenting/"},{"categories":null,"content":"","keywords":null,"title":"Process Substitution","uri":"https://rmoff.github.io/categories/process-substitution/"},{"categories":null,"content":"","keywords":null,"title":"Process Substitution","uri":"https://rmoff.github.io/tag/process-substitution/"},{"categories":null,"content":"","keywords":null,"title":"Proxmox","uri":"https://rmoff.github.io/categories/proxmox/"},{"categories":null,"content":"","keywords":null,"title":"Proxmox","uri":"https://rmoff.github.io/tag/proxmox/"},{"categories":null,"content":"","keywords":null,"title":"Pygmentize","uri":"https://rmoff.github.io/categories/pygmentize/"},{"categories":null,"content":"","keywords":null,"title":"Pygmentize","uri":"https://rmoff.github.io/tag/pygmentize/"},{"categories":null,"content":"","keywords":null,"title":"Pygments","uri":"https://rmoff.github.io/categories/pygments/"},{"categories":null,"content":"","keywords":null,"title":"Pygments","uri":"https://rmoff.github.io/tag/pygments/"},{"categories":null,"content":"","keywords":null,"title":"Python","uri":"https://rmoff.github.io/categories/python/"},{"categories":null,"content":"","keywords":null,"title":"Python","uri":"https://rmoff.github.io/tag/python/"},{"categories":null,"content":"","keywords":null,"title":"Qcow2","uri":"https://rmoff.github.io/categories/qcow2/"},{"categories":null,"content":"","keywords":null,"title":"Qcow2","uri":"https://rmoff.github.io/tag/qcow2/"},{"categories":null,"content":"","keywords":null,"title":"Qemu","uri":"https://rmoff.github.io/categories/qemu/"},{"categories":null,"content":"","keywords":null,"title":"Qemu","uri":"https://rmoff.github.io/tag/qemu/"},{"categories":null,"content":"","keywords":null,"title":"Quandl","uri":"https://rmoff.github.io/categories/quandl/"},{"categories":null,"content":"","keywords":null,"title":"Quandl","uri":"https://rmoff.github.io/tag/quandl/"},{"categories":null,"content":"","keywords":null,"title":"R","uri":"https://rmoff.github.io/categories/r/"},{"categories":null,"content":"","keywords":null,"title":"R","uri":"https://rmoff.github.io/tag/r/"},{"categories":null,"content":"","keywords":null,"title":"Raw","uri":"https://rmoff.github.io/categories/raw/"},{"categories":null,"content":"","keywords":null,"title":"Raw","uri":"https://rmoff.github.io/tag/raw/"},{"categories":null,"content":"","keywords":null,"title":"Reading","uri":"https://rmoff.github.io/categories/reading/"},{"categories":null,"content":"","keywords":null,"title":"Reading","uri":"https://rmoff.github.io/tag/reading/"},{"categories":null,"content":"","keywords":null,"title":"Readprocmeminfofile","uri":"https://rmoff.github.io/categories/readprocmeminfofile/"},{"categories":null,"content":"","keywords":null,"title":"Readprocmeminfofile","uri":"https://rmoff.github.io/tag/readprocmeminfofile/"},{"categories":null,"content":"","keywords":null,"title":"Regression Testing","uri":"https://rmoff.github.io/categories/regression-testing/"},{"categories":null,"content":"","keywords":null,"title":"Regression Testing","uri":"https://rmoff.github.io/tag/regression-testing/"},{"categories":null,"content":"","keywords":null,"title":"Replica Set","uri":"https://rmoff.github.io/categories/replica-set/"},{"categories":null,"content":"","keywords":null,"title":"Replica Set","uri":"https://rmoff.github.io/tag/replica-set/"},{"categories":null,"content":"","keywords":null,"title":"Rest","uri":"https://rmoff.github.io/categories/rest/"},{"categories":null,"content":"","keywords":null,"title":"Rest","uri":"https://rmoff.github.io/tag/rest/"},{"categories":null,"content":"","keywords":null,"title":"Rest Api","uri":"https://rmoff.github.io/categories/rest-api/"},{"categories":null,"content":"","keywords":null,"title":"Rest Api","uri":"https://rmoff.github.io/tag/rest-api/"},{"categories":null,"content":"","keywords":null,"title":"Rhel","uri":"https://rmoff.github.io/categories/rhel/"},{"categories":null,"content":"","keywords":null,"title":"Rhel","uri":"https://rmoff.github.io/tag/rhel/"},{"categories":null,"content":"","keywords":null,"title":"Rickroll","uri":"https://rmoff.github.io/categories/rickroll/"},{"categories":null,"content":"","keywords":null,"title":"Rj45","uri":"https://rmoff.github.io/categories/rj45/"},{"categories":null,"content":"","keywords":null,"title":"Rj45","uri":"https://rmoff.github.io/tag/rj45/"},{"categories":null,"content":"","keywords":null,"title":"Root","uri":"https://rmoff.github.io/categories/root/"},{"categories":null,"content":"","keywords":null,"title":"Root","uri":"https://rmoff.github.io/tag/root/"},{"categories":null,"content":"","keywords":null,"title":"Router","uri":"https://rmoff.github.io/categories/router/"},{"categories":null,"content":"","keywords":null,"title":"Router","uri":"https://rmoff.github.io/tag/router/"},{"categories":null,"content":"","keywords":null,"title":"Rpd","uri":"https://rmoff.github.io/categories/rpd/"},{"categories":null,"content":"","keywords":null,"title":"Rpd","uri":"https://rmoff.github.io/tag/rpd/"},{"categories":null,"content":"","keywords":null,"title":"Running","uri":"https://rmoff.github.io/categories/running/"},{"categories":null,"content":"","keywords":null,"title":"Running","uri":"https://rmoff.github.io/tag/running/"},{"categories":null,"content":"","keywords":null,"title":"S3","uri":"https://rmoff.github.io/categories/s3/"},{"categories":null,"content":"","keywords":null,"title":"S3","uri":"https://rmoff.github.io/tag/s3/"},{"categories":null,"content":"","keywords":null,"title":"San Francisco","uri":"https://rmoff.github.io/categories/san-francisco/"},{"categories":null,"content":"","keywords":null,"title":"San Francisco","uri":"https://rmoff.github.io/tag/san-francisco/"},{"categories":null,"content":"","keywords":null,"title":"Sawserver","uri":"https://rmoff.github.io/categories/sawserver/"},{"categories":null,"content":"","keywords":null,"title":"Sawserver","uri":"https://rmoff.github.io/tag/sawserver/"},{"categories":null,"content":"","keywords":null,"title":"Schedule","uri":"https://rmoff.github.io/categories/schedule/"},{"categories":null,"content":"","keywords":null,"title":"Schedule","uri":"https://rmoff.github.io/tag/schedule/"},{"categories":null,"content":"","keywords":null,"title":"Schema Registry","uri":"https://rmoff.github.io/categories/schema-registry/"},{"categories":null,"content":"","keywords":null,"title":"Schema Registry","uri":"https://rmoff.github.io/tag/schema-registry/"},{"categories":null,"content":"","keywords":null,"title":"Schemaprojectorexception","uri":"https://rmoff.github.io/categories/schemaprojectorexception/"},{"categories":null,"content":"","keywords":null,"title":"Schemaprojectorexception","uri":"https://rmoff.github.io/tag/schemaprojectorexception/"},{"categories":null,"content":"","keywords":null,"title":"Screen","uri":"https://rmoff.github.io/categories/screen/"},{"categories":null,"content":"","keywords":null,"title":"Screen","uri":"https://rmoff.github.io/tag/screen/"},{"categories":null,"content":"","keywords":null,"title":"Sed","uri":"https://rmoff.github.io/categories/sed/"},{"categories":null,"content":"","keywords":null,"title":"Sed","uri":"https://rmoff.github.io/tag/sed/"},{"categories":null,"content":"","keywords":null,"title":"Slack","uri":"https://rmoff.github.io/categories/slack/"},{"categories":null,"content":"","keywords":null,"title":"Slack","uri":"https://rmoff.github.io/tag/slack/"},{"categories":null,"content":"","keywords":null,"title":"Spark","uri":"https://rmoff.github.io/categories/spark/"},{"categories":null,"content":"","keywords":null,"title":"Spark","uri":"https://rmoff.github.io/tag/spark/"},{"categories":null,"content":"","keywords":null,"title":"Sparksql","uri":"https://rmoff.github.io/categories/sparksql/"},{"categories":null,"content":"","keywords":null,"title":"Sparksql","uri":"https://rmoff.github.io/tag/sparksql/"},{"categories":null,"content":"","keywords":null,"title":"Speaking","uri":"https://rmoff.github.io/categories/speaking/"},{"categories":null,"content":"","keywords":null,"title":"Speaking","uri":"https://rmoff.github.io/tag/speaking/"},{"categories":null,"content":"","keywords":null,"title":"Spelling","uri":"https://rmoff.github.io/categories/spelling/"},{"categories":null,"content":"","keywords":null,"title":"Spelling","uri":"https://rmoff.github.io/tag/spelling/"},{"categories":null,"content":"","keywords":null,"title":"Squashfs","uri":"https://rmoff.github.io/categories/squashfs/"},{"categories":null,"content":"","keywords":null,"title":"Squashfs","uri":"https://rmoff.github.io/tag/squashfs/"},{"categories":null,"content":"","keywords":null,"title":"Start.cmd","uri":"https://rmoff.github.io/categories/start.cmd/"},{"categories":null,"content":"","keywords":null,"title":"Start.cmd","uri":"https://rmoff.github.io/tag/start.cmd/"},{"categories":null,"content":"","keywords":null,"title":"Strava","uri":"https://rmoff.github.io/categories/strava/"},{"categories":null,"content":"","keywords":null,"title":"Strava","uri":"https://rmoff.github.io/tag/strava/"},{"categories":null,"content":"","keywords":null,"title":"Stream","uri":"https://rmoff.github.io/categories/stream/"},{"categories":null,"content":"","keywords":null,"title":"Stream","uri":"https://rmoff.github.io/tag/stream/"},{"categories":null,"content":"","keywords":null,"title":"Stream Processing","uri":"https://rmoff.github.io/categories/stream-processing/"},{"categories":null,"content":"","keywords":null,"title":"Stream Processing","uri":"https://rmoff.github.io/tag/stream-processing/"},{"categories":null,"content":"","keywords":null,"title":"Streaming","uri":"https://rmoff.github.io/categories/streaming/"},{"categories":null,"content":"","keywords":null,"title":"Streaming","uri":"https://rmoff.github.io/tag/streaming/"},{"categories":null,"content":"","keywords":null,"title":"Streaming Etl","uri":"https://rmoff.github.io/categories/streaming-etl/"},{"categories":null,"content":"","keywords":null,"title":"Streaming Etl","uri":"https://rmoff.github.io/tag/streaming-etl/"},{"categories":null,"content":"","keywords":null,"title":"Sudo","uri":"https://rmoff.github.io/categories/sudo/"},{"categories":null,"content":"","keywords":null,"title":"Sudo","uri":"https://rmoff.github.io/tag/sudo/"},{"categories":null,"content":"","keywords":null,"title":"Suet Pudding","uri":"https://rmoff.github.io/categories/suet-pudding/"},{"categories":null,"content":"","keywords":null,"title":"Suet Pudding","uri":"https://rmoff.github.io/tag/suet-pudding/"},{"categories":null,"content":"","keywords":null,"title":"Swapfree","uri":"https://rmoff.github.io/categories/swapfree/"},{"categories":null,"content":"","keywords":null,"title":"Swapfree","uri":"https://rmoff.github.io/tag/swapfree/"},{"categories":null,"content":"","keywords":null,"title":"Swingbench","uri":"https://rmoff.github.io/categories/swingbench/"},{"categories":null,"content":"","keywords":null,"title":"Swingbench","uri":"https://rmoff.github.io/tag/swingbench/"},{"categories":null,"content":"","keywords":null,"title":"Syntax Highlighting","uri":"https://rmoff.github.io/categories/syntax-highlighting/"},{"categories":null,"content":"","keywords":null,"title":"Syntax Highlighting","uri":"https://rmoff.github.io/tag/syntax-highlighting/"},{"categories":null,"content":"","keywords":null,"title":"Sysdig","uri":"https://rmoff.github.io/categories/sysdig/"},{"categories":null,"content":"","keywords":null,"title":"Sysdig","uri":"https://rmoff.github.io/tag/sysdig/"},{"categories":null,"content":"","keywords":null,"title":"Table","uri":"https://rmoff.github.io/categories/table/"},{"categories":null,"content":"","keywords":null,"title":"Table","uri":"https://rmoff.github.io/tag/table/"},{"categories":null,"content":"","keywords":null,"title":"Tag","uri":"https://rmoff.github.io/tag/"},{"categories":null,"content":"See talks","keywords":null,"title":"Talks","uri":"https://rmoff.github.io/presentations/"},{"categories":null,"content":"You can find most of my slide decks on Speaker Deck. Below are links to particular talks I\u0026rsquo;ve done recently, along with recordings where available.\nATM Fraud detection with Kafka and KSQL  üìñ Slides üëæ Code üìΩ Recording  Embrace the Anarchy: Apache Kafka\u0026rsquo;s Role in Modern Data Architectures  üìΩ Recording\n Devoxx Belgium\n Rated 32 out of all the conference talks Scored \\( 4.31/5 \\), 134 votes  Oracle CODE London","keywords":null,"title":"Talks","uri":"https://rmoff.github.io/talks/"},{"categories":null,"content":"","keywords":null,"title":"Testing","uri":"https://rmoff.github.io/categories/testing/"},{"categories":null,"content":"","keywords":null,"title":"Testing","uri":"https://rmoff.github.io/tag/testing/"},{"categories":null,"content":"","keywords":null,"title":"Tftp","uri":"https://rmoff.github.io/categories/tftp/"},{"categories":null,"content":"","keywords":null,"title":"Tftp","uri":"https://rmoff.github.io/tag/tftp/"},{"categories":null,"content":"","keywords":null,"title":"Timelion","uri":"https://rmoff.github.io/categories/timelion/"},{"categories":null,"content":"","keywords":null,"title":"Timelion","uri":"https://rmoff.github.io/tag/timelion/"},{"categories":null,"content":"","keywords":null,"title":"Timestamp","uri":"https://rmoff.github.io/categories/timestamp/"},{"categories":null,"content":"","keywords":null,"title":"Timestamp","uri":"https://rmoff.github.io/tag/timestamp/"},{"categories":null,"content":"","keywords":null,"title":"Tools","uri":"https://rmoff.github.io/categories/tools/"},{"categories":null,"content":"","keywords":null,"title":"Tools","uri":"https://rmoff.github.io/tag/tools/"},{"categories":null,"content":"","keywords":null,"title":"Topbeat","uri":"https://rmoff.github.io/categories/topbeat/"},{"categories":null,"content":"","keywords":null,"title":"Topbeat","uri":"https://rmoff.github.io/tag/topbeat/"},{"categories":null,"content":"","keywords":null,"title":"Twitter","uri":"https://rmoff.github.io/categories/twitter/"},{"categories":null,"content":"","keywords":null,"title":"Twitter","uri":"https://rmoff.github.io/tag/twitter/"},{"categories":null,"content":"","keywords":null,"title":"Uas","uri":"https://rmoff.github.io/categories/uas/"},{"categories":null,"content":"","keywords":null,"title":"Uas","uri":"https://rmoff.github.io/tag/uas/"},{"categories":null,"content":"","keywords":null,"title":"Ubiquiti","uri":"https://rmoff.github.io/categories/ubiquiti/"},{"categories":null,"content":"","keywords":null,"title":"Ubiquiti","uri":"https://rmoff.github.io/tag/ubiquiti/"},{"categories":null,"content":"","keywords":null,"title":"Ubnt","uri":"https://rmoff.github.io/categories/ubnt/"},{"categories":null,"content":"","keywords":null,"title":"Ubnt","uri":"https://rmoff.github.io/tag/ubnt/"},{"categories":null,"content":"","keywords":null,"title":"Ukoug","uri":"https://rmoff.github.io/categories/ukoug/"},{"categories":null,"content":"","keywords":null,"title":"Ukoug","uri":"https://rmoff.github.io/tag/ukoug/"},{"categories":null,"content":"","keywords":null,"title":"Uploadrpd","uri":"https://rmoff.github.io/categories/uploadrpd/"},{"categories":null,"content":"","keywords":null,"title":"Uploadrpd","uri":"https://rmoff.github.io/tag/uploadrpd/"},{"categories":null,"content":"","keywords":null,"title":"Ups","uri":"https://rmoff.github.io/categories/ups/"},{"categories":null,"content":"","keywords":null,"title":"Ups","uri":"https://rmoff.github.io/tag/ups/"},{"categories":null,"content":"","keywords":null,"title":"Usb","uri":"https://rmoff.github.io/categories/usb/"},{"categories":null,"content":"","keywords":null,"title":"Usb","uri":"https://rmoff.github.io/tag/usb/"},{"categories":null,"content":"","keywords":null,"title":"Usenix","uri":"https://rmoff.github.io/categories/usenix/"},{"categories":null,"content":"","keywords":null,"title":"Usenix","uri":"https://rmoff.github.io/tag/usenix/"},{"categories":null,"content":"","keywords":null,"title":"User Groups","uri":"https://rmoff.github.io/categories/user-groups/"},{"categories":null,"content":"","keywords":null,"title":"User Groups","uri":"https://rmoff.github.io/tag/user-groups/"},{"categories":null,"content":"","keywords":null,"title":"Vgscan","uri":"https://rmoff.github.io/categories/vgscan/"},{"categories":null,"content":"","keywords":null,"title":"Vgscan","uri":"https://rmoff.github.io/tag/vgscan/"},{"categories":null,"content":"","keywords":null,"title":"Vi","uri":"https://rmoff.github.io/categories/vi/"},{"categories":null,"content":"","keywords":null,"title":"Vi","uri":"https://rmoff.github.io/tag/vi/"},{"categories":null,"content":"","keywords":null,"title":"Virtualbox","uri":"https://rmoff.github.io/categories/virtualbox/"},{"categories":null,"content":"","keywords":null,"title":"Virtualbox","uri":"https://rmoff.github.io/tag/virtualbox/"},{"categories":null,"content":"","keywords":null,"title":"Visual Analyzer","uri":"https://rmoff.github.io/categories/visual-analyzer/"},{"categories":null,"content":"","keywords":null,"title":"Visual Analyzer","uri":"https://rmoff.github.io/tag/visual-analyzer/"},{"categories":null,"content":"","keywords":null,"title":"Vm","uri":"https://rmoff.github.io/categories/vm/"},{"categories":null,"content":"","keywords":null,"title":"Vm","uri":"https://rmoff.github.io/tag/vm/"},{"categories":null,"content":"","keywords":null,"title":"Vmdk","uri":"https://rmoff.github.io/categories/vmdk/"},{"categories":null,"content":"","keywords":null,"title":"Vmdk","uri":"https://rmoff.github.io/tag/vmdk/"},{"categories":null,"content":"","keywords":null,"title":"Vmware","uri":"https://rmoff.github.io/categories/vmware/"},{"categories":null,"content":"","keywords":null,"title":"Vmware","uri":"https://rmoff.github.io/tag/vmware/"},{"categories":null,"content":"","keywords":null,"title":"Vmware Esxi","uri":"https://rmoff.github.io/categories/vmware-esxi/"},{"categories":null,"content":"","keywords":null,"title":"Vmware Esxi","uri":"https://rmoff.github.io/tag/vmware-esxi/"},{"categories":null,"content":"","keywords":null,"title":"Web Service","uri":"https://rmoff.github.io/categories/web-service/"},{"categories":null,"content":"","keywords":null,"title":"Web Service","uri":"https://rmoff.github.io/tag/web-service/"},{"categories":null,"content":"","keywords":null,"title":"Window","uri":"https://rmoff.github.io/categories/window/"},{"categories":null,"content":"","keywords":null,"title":"Window","uri":"https://rmoff.github.io/tag/window/"},{"categories":null,"content":"","keywords":null,"title":"Wireless","uri":"https://rmoff.github.io/categories/wireless/"},{"categories":null,"content":"","keywords":null,"title":"Wireless","uri":"https://rmoff.github.io/tag/wireless/"},{"categories":null,"content":"","keywords":null,"title":"Wls","uri":"https://rmoff.github.io/categories/wls/"},{"categories":null,"content":"","keywords":null,"title":"Wls","uri":"https://rmoff.github.io/tag/wls/"},{"categories":null,"content":"","keywords":null,"title":"Wlst","uri":"https://rmoff.github.io/categories/wlst/"},{"categories":null,"content":"","keywords":null,"title":"Wlst","uri":"https://rmoff.github.io/tag/wlst/"},{"categories":null,"content":"","keywords":null,"title":"Wrangling","uri":"https://rmoff.github.io/categories/wrangling/"},{"categories":null,"content":"","keywords":null,"title":"Wrangling","uri":"https://rmoff.github.io/tag/wrangling/"},{"categories":null,"content":"","keywords":null,"title":"Xargs","uri":"https://rmoff.github.io/categories/xargs/"},{"categories":null,"content":"","keywords":null,"title":"Xargs","uri":"https://rmoff.github.io/tag/xargs/"},{"categories":null,"content":"","keywords":null,"title":"Xsa","uri":"https://rmoff.github.io/categories/xsa/"},{"categories":null,"content":"","keywords":null,"title":"Xsa","uri":"https://rmoff.github.io/tag/xsa/"},{"categories":null,"content":"","keywords":null,"title":"Xstream","uri":"https://rmoff.github.io/categories/xstream/"},{"categories":null,"content":"","keywords":null,"title":"Xstream","uri":"https://rmoff.github.io/tag/xstream/"},{"categories":null,"content":"","keywords":null,"title":"Yarn","uri":"https://rmoff.github.io/categories/yarn/"},{"categories":null,"content":"","keywords":null,"title":"Yarn","uri":"https://rmoff.github.io/tag/yarn/"},{"categories":null,"content":"","keywords":null,"title":"York","uri":"https://rmoff.github.io/categories/york/"},{"categories":null,"content":"","keywords":null,"title":"York","uri":"https://rmoff.github.io/tag/york/"},{"categories":null,"content":"","keywords":null,"title":"Zookeeper","uri":"https://rmoff.github.io/categories/zookeeper/"},{"categories":null,"content":"","keywords":null,"title":"Zookeeper","uri":"https://rmoff.github.io/tag/zookeeper/"},{"categories":null,"content":"","keywords":null,"title":"rmoff\u0026rsquo;s random ramblings","uri":"https://rmoff.github.io/"}]