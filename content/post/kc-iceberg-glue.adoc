---
draft: false
title: 'Writing to Apache Iceberg on S3 using Kafka Connect with Glue catalog'
date: "2025-06-30T15:36:21Z"
image: "/images/2025/06/h_IMG_0592.webp"
thumbnail: "/images/2025/06/t_IMG_0587.webp"
credit: "https://bsky.app/profile/rmoff.net"
categories:
- Apache Iceberg
- Apache Kafka
- Kafka Connect
---

:source-highlighter: rouge
:icons: font
:rouge-css: style
:rouge-style: monokai

Without wanting to mix my temperature metaphors, Iceberg is the new hawtness, and getting data into it from other places is a common task.
I link:/2025/06/24/writing-to-apache-iceberg-on-s3-using-flink-sql-with-glue-catalog/[wrote previously about using Flink SQL to do this], and today I'm going to look at doing the same using Kafka Connect.
I'm going to use AWS's Glue Data Catalog, but the sink also works with other Iceberg catalogs.

<!--more-->

image::/images/2025/06/kc.excalidraw.png[]

Kafka Connect is a framework for data integration, and is part of Apache Kafka.
There is a rich ecosystem of connectors for getting data in and out of Kafka, and Kafka Connect itself provides a set of features that you'd expect for a resilient data integration platform, including scaling, schema handling, restarts, serialisation, and more.

The Apache Iceberg connector for Kafka Connect was https://www.tabular.io/blog/intro-kafka-connect/[originally created by folk at Tabular] and has subsequently been https://github.com/apache/iceberg/pull/8701#issue-1922301136[contributed to] https://iceberg.apache.org/docs/nightly/kafka-connect/[the Apache Iceberg project] (via a brief stint on https://github.com/databricks/iceberg-kafka-connect[a Databricks repo] following the Tabular acquisition).

For the purposes of this blog I'm still using the Tabular version since it has a pre-built binary available on https://www.confluent.io/hub/tabular/iceberg-kafka-connect[Confluent Hub] which makes it easier to install.
If you want to use the Apache Iceberg version you currently https://iceberg.apache.org/docs/nightly/kafka-connect/#installation[need to build the connector yourself].
There is https://github.com/apache/iceberg/issues/10745[work underway] to make it available on Confluent Hub.

== Setup

TIP: You can find the Docker Compose for this article https://github.com/rmoff/examples/tree/main/iceberg/kafka-kafkaconnect-aws[here]

We'll start by populating a Kafka topic with some dummy data.
I'm using JSON to serialise it; bear in mind that an explicitly-declared schema stored in a Schema Registry and the data serialised with something like Avro is often a better approach.

[source,bash]
----
$ echo '{"order_id": "001", "customer_id": "cust_123", "product": "laptop", "quantity": 1, "price": 999.99}
{"order_id": "002", "customer_id": "cust_456", "product": "mouse", "quantity": 2, "price": 25.50}
{"order_id": "003", "customer_id": "cust_789", "product": "keyboard", "quantity": 1, "price": 75.00}
{"order_id": "004", "customer_id": "cust_321", "product": "monitor", "quantity": 1, "price": 299.99}
{"order_id": "005", "customer_id": "cust_654", "product": "headphones", "quantity": 1, "price": 149.99}' | docker compose exec -T kcat kcat -P -b broker:9092 -t orders
----

Now we need to install the connector into our Kafka Connect worker.
I'm running a https://hub.docker.com/r/confluentinc/cp-kafka-connect[Kafka Connect Docker image] provided by Confluent because it provides the https://docs.confluent.io/platform/7.5/connect/confluent-hub/client.html#install-components-with-c-hub-client[`confluent-hub` CLI tool] which is a handy way for installing pre-built connectors and saves me having to do it myself.
It's worth noting that the `confluent-hub` CLI is being deprecated in favour of the broader `confluent` CLI tool and `confluent connect plugin install` to install connectors.

[source,bash]
----
$ confluent-hub install --no-prompt tabular/iceberg-kafka-connect:0.6.19
----

[TIP]
====
If you're using Docker you can bake this in at runtime https://github.com/confluentinc/demo-scene/blob/master/connect-cluster/docker-compose-scenario02.yml#L97-L107[like this].
====

Let's check that the connector is installed.
We can use the Kafka Connect REST API for this and the `/connector-plugins` endpoint:

[source,bash]
----
$ curl -s localhost:8083/connector-plugins|jq '.[].class'
"io.tabular.iceberg.connect.IcebergSinkConnector"
"org.apache.kafka.connect.mirror.MirrorCheckpointConnector"
"org.apache.kafka.connect.mirror.MirrorHeartbeatConnector"
"org.apache.kafka.connect.mirror.MirrorSourceConnector"
----

(Note that it's `io.tabular` and not `org.apache`, since we're using the Tabular version of the connector for now).

REST APIs are all very well, but a nicer way of managing Kafka Connect is https://github.com/kcctl/kcctl[**kcctl**].
This is a CLI client built for Kafka Connect.
On the Mac it's a simple install from Brew: `brew install kcctl/tap/kcctl`

Once you've install kcctl, configure it to point to the Kafka Connect worker cluster:

[source,bash]
----
$ kcctl config set-context local --cluster http://localhost:8083
----

Now we can easily inspect the cluster:

[source,bash]
----
$ kcctl info
URL:               http://localhost:8083
Version:           8.0.0-ccs
Commit:            42dc8a94fe8a158bfc3241b5a93a1adde9973507
Kafka Cluster ID:  5L6g3nShT-eMCtK--X86sw
----

We can also look at the sink connectors installed (which is a subset of those shown above):

[source,bash]
----
$ kcctl get plugins --types=sink

 TYPE   CLASS                                             VERSION
 sink   io.tabular.iceberg.connect.IcebergSinkConnector   1.5.2-kc-0.6.19
----

== Configuring the Apache Iceberg Kafka Connect sink

Now let's instantiate our Iceberg sink.
The https://iceberg.apache.org/docs/nightly/kafka-connect/#initial-setup[docs] are pretty good, and include details of how to use different catalogs.
I'm going to configure the sink thus:

* Read messages from the `orders` topic
* Write them to the Iceberg table `foo.kc.orders`
* Use the AWS Glue Data Catalog to store metadata
** I've passed my local AWS credentials as environment variables to the Kafka Connect docker container.
This is not a secure way of doing things, but suffices plenty for these sandbox testing purposes.
* Store the Iceberg files on S3 in the `rmoff-lakehouse` bucket under the `/01` path

Using kcctl it looks like this:

[source,bash]
----
$ kcctl apply -f - <<EOF
{
  "name": "iceberg-sink-kc_orders",
  "config": {
    "connector.class": "io.tabular.iceberg.connect.IcebergSinkConnector",
    "topics": "orders",
    "iceberg.tables": "foo.kc.orders",
    "iceberg.catalog.catalog-impl": "org.apache.iceberg.aws.glue.GlueCatalog",
    "iceberg.catalog.warehouse": "s3://rmoff-lakehouse/01/",
    "iceberg.catalog.io-impl": "org.apache.iceberg.aws.s3.S3FileIO"
  }
}
EOF
----

Check if it worked:

[source,bash]
----
$ kcctl get connectors

 NAME                     TYPE   STATE     TASKS
 iceberg-sink-kc_orders   sink   RUNNING   0: FAILED

$ kcctl describe connector iceberg-sink-kc_orders
Name:       iceberg-sink-kc_orders
Type:       sink
State:      RUNNING
Worker ID:  kafka-connect:8083
Config:
  connector.class:               io.tabular.iceberg.connect.IcebergSinkConnector
  iceberg.catalog.catalog-impl:  org.apache.iceberg.aws.glue.GlueCatalog
  iceberg.catalog.io-impl:       org.apache.iceberg.aws.s3.S3FileIO
  iceberg.catalog.warehouse:     s3://rmoff-lakehouse/00/
  iceberg.tables:                foo.kc.orders
  name:                          iceberg-sink-kc_orders
  topics:                        orders
Tasks:
  0:
    State:      FAILED
    Worker ID:  kafka-connect:8083
    Trace:      org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
        at
[…]
      Caused by: org.apache.kafka.connect.errors.DataException: JsonConverter with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.
[…]
----

So, no dice on the first attempt.
(Note also the confusing fact that the _connector_ has a state of `RUNNING` whilst the _task_ is `FAILED`).

The error is to do with how Kafka Connect handles deserialising messages from Kafka topics.
It's reading JSON, but expecting to find `schema` and `payload` elements within it—and these aren't there.
https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/#json-message-without-expected-schema-payload-structure[This blog post] explains the issue in more detail.

To fix it we'll change the connctor configuration which we can do easily with kcctl's `patch`:

[source,bash]
----
$ kcctl patch connector iceberg-sink-kc_orders \
    -s key.converter=org.apache.kafka.connect.json.JsonConverter \
    -s key.converter.schemas.enable=false \
    -s value.converter=org.apache.kafka.connect.json.JsonConverter \
    -s value.converter.schemas.enable=false
----

Check the connector state again:

[source,bash]
----
$ kcctl describe connector iceberg-sink-kc_orders
Name:       iceberg-sink-kc_orders
Type:       sink
State:      RUNNING
Worker ID:  kafka-connect:8083
Config:
  connector.class:                 io.tabular.iceberg.connect.IcebergSinkConnector
  iceberg.catalog.catalog-impl:    org.apache.iceberg.aws.glue.GlueCatalog
  iceberg.catalog.io-impl:         org.apache.iceberg.aws.s3.S3FileIO
  iceberg.catalog.warehouse:       s3://rmoff-lakehouse/01/
  iceberg.tables:                  foo.kc.orders
  key.converter:                   org.apache.kafka.connect.json.JsonConverter
  key.converter.schemas.enable:    false
  name:                            iceberg-sink-kc_orders
  topics:                          orders
  value.converter:                 org.apache.kafka.connect.json.JsonConverter
  value.converter.schemas.enable:  false
Tasks:
  0:
    State:      FAILED
[…]
      Caused by: org.apache.iceberg.exceptions.NoSuchTableException: Invalid table identifier: foo.kc.orders
----

This time the error is entirely self-inflicted.
Hot off my blog post about https://rmoff.net/2025/06/24/writing-to-apache-iceberg-on-s3-using-flink-sql-with-glue-catalog/[doing this in Flink SQL] I had in my mind that the table needed a three part qualification; `catalog.database.table`.
In fact, we only need to specify `database.table`.
In addition I've realised that the table doesn't exist already, and by default the connector won't automagically create it—so let's fix that too.

[source,bash]
----
$ kcctl patch connector iceberg-sink-kc_orders \
    -s iceberg.tables=kc.orders \
    -s iceberg.tables.auto-create-enabled=true
----

We're getting closer, but not quite there yet:

[source,bash]
----
[…]
    Caused by: software.amazon.awssdk.services.glue.model.EntityNotFoundException: Database kc not found. (Service: Glue, Status Code: 400, Request ID: 16a25fcf-01be-44e9-ba67-cc71431f3945)
----

Let's see what databases we _do_ have:

[source,bash]
----
$ aws glue get-databases --region us-east-1 --query 'DatabaseList[].Name' --output table

+--------------------+
|    GetDatabases    |
+--------------------+
|  default_database  |
|  my_glue_db        |
|  new_glue_db       |
|  rmoff_db          |
+--------------------+
----

OK, so let's use a database that does exist (`rmoff_db`):

[source,bash]
----
$ kcctl patch connector iceberg-sink-kc_orders \
    -s iceberg.tables=rmoff_db.orders
----

Now we're up and running :)

[source,bash]
----
$ kcctl describe connector iceberg-sink-kc_orders
Name:       iceberg-sink-kc_orders
Type:       sink
State:      RUNNING
Worker ID:  kafka-connect:8083
Config:
  connector.class:                     io.tabular.iceberg.connect.IcebergSinkConnector
  iceberg.catalog.catalog-impl:        org.apache.iceberg.aws.glue.GlueCatalog
  iceberg.catalog.io-impl:             org.apache.iceberg.aws.s3.S3FileIO
  iceberg.catalog.warehouse:           s3://rmoff-lakehouse/01/
  iceberg.tables:                      rmoff_db.orders
  iceberg.tables.auto-create-enabled:  true
  key.converter:                       org.apache.kafka.connect.json.JsonConverter
  key.converter.schemas.enable:        false
  name:                                iceberg-sink-kc_orders
  topics:                              orders
  value.converter:                     org.apache.kafka.connect.json.JsonConverter
  value.converter.schemas.enable:      false
Tasks:
  0:
    State:      RUNNING
    Worker ID:  kafka-connect:8083
Topics:
  orders
----

== Examining the Iceberg table

Now we'll have a look at the Iceberg table.

The table has been registered in the Glue Data Catalog:

[source,bash]
----
$ aws glue get-tables \
    --region us-east-1 --database-name rmoff_db \
    --query 'TableList[].Name' --output table

+----------------+
|    GetTables   |
+----------------+
|  orders        |
+----------------+
----

And there's something in the S3 bucket:
[source,bash]
----
$ aws s3 --recursive ls s3://rmoff-lakehouse/01
2025-06-30 16:44:39       1320 01/rmoff_db.db/orders/metadata/00000-bcbeeafa-4556-4a52-92ee-5dbc34d35d6b.metadata.json
----

However, this is just the table's Iceberg metadata is there—but nothing else.
That's because Kafka Connect won't flush the data to storage straight away; by default it's every 5 minutes.
The configuration that controls this is `iceberg.control.commit.interval-ms`.

So, if we wait long enough, we'll see some data:

[source,bash]
----
$ aws s3 --recursive ls s3://rmoff-lakehouse/01
2025-06-30 16:51:35       1635 01/rmoff_db.db/orders/data/00001-1751298279338-409ff5c8-244f-4104-8b81-dfe47fcbb2b3-00001.parquet
2025-06-30 16:44:39       1320 01/rmoff_db.db/orders/metadata/00000-bcbeeafa-4556-4a52-92ee-5dbc34d35d6b.metadata.json
2025-06-30 16:55:09       2524 01/rmoff_db.db/orders/metadata/00001-e8341cee-cf17-4255-bcf1-6e87cf41bbf3.metadata.json
2025-06-30 16:55:08       6950 01/rmoff_db.db/orders/metadata/cbe2651d-7c83-4465-a2e1-d92bb3e0b61d-m0.avro
2025-06-30 16:55:09       4233 01/rmoff_db.db/orders/metadata/snap-6069858821353147927-1-cbe2651d-7c83-4465-a2e1-d92bb3e0b61d.avro
----

Alternatively we can be impatient (and inefficient, if we were to use this for real as you'd get a ton of small files as a result) and override it:

[source,bash]
----
# Commit files to Iceberg every 30 seconds
$ kcctl patch connector iceberg-sink-kc_orders \
    -s iceberg.control.commit.interval-ms=30000
----

Now let's have a look at this data that we've written.
The absolute joy of Iceberg is the freedom that it gives you by decoupling storage from engine.
This means that we can write the data with one engine (here, Kafka Connect), and read it from another.
Let's use DuckDB.
Because, quack.

DuckDB https://duckdb.org/docs/stable/core_extensions/iceberg/amazon_sagemaker_lakehouse.html#connecting-to-amazon-sagemaker-lakehouse[supports] AWS Glue Data Catalog for Iceberg metadata.
I had https://github.com/duckdb/duckdb-iceberg/issues/265#issuecomment-3009061597[some trouble] with it, but found a https://github.com/duckdb/duckdb-iceberg/issues/265#issuecomment-2959813611[useful workaround] (yay open source).
There's also a comprehensive https://tobilg.com/using-amazon-sagemaker-lakehouse-with-duckdb[blog post] from Tobias Müller
on how to get it to work with a ton of IAM, ARN, and WTF (I think I made the last one up)—probably useful if you need to get this to work with any semblance of security.

So, first we create an `S3` secret in DuckDB to provide our AWS credentials, which I'm doing via https://duckdb.org/docs/stable/core_extensions/httpfs/s3api.html#credential_chain-provider[`credential_chain`] which will read them from my local environment variables.

[source,sql]
----
🟡◗ CREATE SECRET iceberg_secret (
        TYPE S3,
        PROVIDER credential_chain
    );
----

Then we attach the Glue data catalog as a new database to the DuckDB session.
Here, `1234` is my AWS account id (which you can get with `aws sts get-caller-identity --query Account`).

[source,sql]
----
🟡◗ ATTACH '1234' AS glue_catalog (
        TYPE iceberg,
        ENDPOINT_TYPE glue
    );
----

Once you've done this you should be able to list the table(s) in your Glue Data Catalog:

[source,sql]
----
🟡◗ SHOW DATABASES;
┌───────────────┐
│ database_name │
│    varchar    │
├───────────────┤
│ glue_catalog  │
│ memory        │
└───────────────┘

🟡◗ SELECT * FROM information_schema.tables
    WHERE table_catalog = 'glue_catalog'
      AND table_schema='rmoff_db';
100% ▕████████████████████████████████████████████████████████████▏
┌───────────────┬──────────────┬──────────────────┬────────────┬
│ table_catalog │ table_schema │    table_name    │ table_type │
│    varchar    │   varchar    │     varchar      │  varchar   │
├───────────────┼──────────────┼──────────────────┼────────────┼
│ glue_catalog  │ rmoff_db     │ orders           │ BASE TABLE │
└───────────────┴──────────────┴──────────────────┴────────────┴
----

Terminology-wise, a _catalog_ in AWS Glue Data Catalog is a _database_ in DuckDB (`SHOW DATABASES`), and also a _catalog_ (`table_catalog`).
A Glue _database_ is a DuckDB _schema_.
And a table is a table in both :)

Let's finish this section by checking that the data we wrote to Kafka is indeed in Iceberg.

Here's the source data read from the Kafka topic:

[source,bash]
----
$ docker compose exec -it kcat kcat -b broker:9092 -C -t orders
{"order_id": "001", "customer_id": "cust_123", "product": "laptop", "quantity": 1, "price": 999.99}
{"order_id": "002", "customer_id": "cust_456", "product": "mouse", "quantity": 2, "price": 25.50}
{"order_id": "003", "customer_id": "cust_789", "product": "keyboard", "quantity": 1, "price": 75.00}
{"order_id": "004", "customer_id": "cust_321", "product": "monitor", "quantity": 1, "price": 299.99}
{"order_id": "005", "customer_id": "cust_654", "product": "headphones", "quantity": 1, "price": 149.99}
----

and now the Iceberg table:

[source,sql]
----
🟡◗ USE glue_catalog.rmoff_db;
Run Time (s): real 0.539 user 0.036459 sys 0.006006
🟡◗ SELECT * FROM orders;
┌────────────┬──────────┬────────┬─────────────┬──────────┐
│  product   │ quantity │ price  │ customer_id │ order_id │
│  varchar   │  int64   │ double │   varchar   │ varchar  │
├────────────┼──────────┼────────┼─────────────┼──────────┤
│ laptop     │        1 │ 999.99 │ cust_123    │ 001      │
│ mouse      │        2 │   25.5 │ cust_456    │ 002      │
│ keyboard   │        1 │   75.0 │ cust_789    │ 003      │
│ monitor    │        1 │ 299.99 │ cust_321    │ 004      │
│ headphones │        1 │ 149.99 │ cust_654    │ 005      │
└────────────┴──────────┴────────┴─────────────┴──────────┘
Run Time (s): real 2.752 user 0.209454 sys 0.082746
----

Write another row of data to the Kafka topic (`order_id`: `006`):

[source,bash]
----
$ echo '{"order_id": "006", "customer_id": "cust_987", "product": "webcam", "quantity": 1, "price": 89.99}' | docker compose exec -T kcat kcat -P -b broker:9092 -t orders
----

Now wait 30 seconds (or whatever `iceberg.control.commit.interval-ms` is set to), and check the Iceberg table:

[source,sql]
----
🟡◗ SELECT * FROM orders;
┌────────────┬──────────┬────────┬─────────────┬──────────┐
│  product   │ quantity │ price  │ customer_id │ order_id │
│  varchar   │  int64   │ double │   varchar   │ varchar  │
├────────────┼──────────┼────────┼─────────────┼──────────┤
│ webcam     │        1 │  89.99 │ cust_987    │ 006      │ <1>
│ laptop     │        1 │ 999.99 │ cust_123    │ 001      │
│ mouse      │        2 │   25.5 │ cust_456    │ 002      │
│ keyboard   │        1 │   75.0 │ cust_789    │ 003      │
│ monitor    │        1 │ 299.99 │ cust_321    │ 004      │
│ headphones │        1 │ 149.99 │ cust_654    │ 005      │
└────────────┴──────────┴────────┴─────────────┴──────────┘
Run Time (s): real 2.567 user 0.125818 sys 0.031565
----
<1> The new row of data 🎉

== Schemas

Now that we've got the basic connection between Kafka and Iceberg using Kafka Connect working, let's look at it in a bit more detail.
The first thing that warrants a bit of attention is the schema of the data.

Here's the first record of data from our Kafka topic:

[source,javascript]
----
{
    "order_id": "001",
    "customer_id": "cust_123",
    "product": "laptop",
    "quantity": 1,
    "price": 999.99
}
----

Eyeballing it, you and I can probably guess at the data types of the schema.
Quantity is an integer, probably.
Price, a decimal (unless you don't realise it's a currency and guess that it's a float or double).
Product is obviously a character field.
What about the order ID though?
It looks numeric, but has leading zeros; so a character field also?

My point is, there is no *declared schema*, only an inferred one.
What does it look like written to Iceberg?

[source,bash]
----
$ aws glue get-table --region us-east-1 --database-name rmoff_db --name orders \
    --query 'Table.StorageDescriptor.Columns[].{Name:Name,Type:Type}' --output table

---------------------------
|        GetTable         |
+--------------+----------+
|     Name     |  Type    |
+--------------+----------+
|  product     |  string  |
|  quantity    |  bigint  |
|  price       |  double  |
|  customer_id |  string  |
|  order_id    |  string  |
+--------------+----------+
----

Not bad—only the `price` being stored as a `DOUBLE` is wrong.

What about if we were to use a timestamp in the source data?
And a boolean?

Here's a new dataset in a Kafka topic.
It's roughly based on click behaviour.

[source,javascript]
----
{
    "click_ts": "2023-02-01T14:30:25Z",
    "ad_cost": "1.50",
    "is_conversion": "true",
    "user_id": "001234567890"
}
----

Using the same Kafka Connect approach as above:

[source,bash]
----
$ kcctl apply -f - <<EOF
{
  "name": "iceberg-sink-kc_clicks",
  "config": {
    "connector.class": "io.tabular.iceberg.connect.IcebergSinkConnector",
    "topics": "clicks",
    "iceberg.tables": "rmoff_db.clicks",
    "iceberg.tables.auto-create-enabled": "true",
    "iceberg.catalog.catalog-impl": "org.apache.iceberg.aws.glue.GlueCatalog",
    "iceberg.catalog.warehouse": "s3://rmoff-lakehouse/01/",
    "iceberg.catalog.io-impl": "org.apache.iceberg.aws.s3.S3FileIO",
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "key.converter.schemas.enable": "false",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": "false",
    "iceberg.control.commit.interval-ms": "30000"
  }
}
EOF
----

it ends up like this in Iceberg:

[source,bash]
----
$ ❯ aws glue get-table --region us-east-1 --database-name rmoff_db --name clicks\
    --query 'Table.StorageDescriptor.Columns[].{Name:Name,Type:Type}' --output table

-----------------------------
|         GetTable          |
+----------------+----------+
|      Name      |  Type    |
+----------------+----------+
|  click_ts      |  string  |
|  ad_cost       |  string  |
|  user_id       |  string  |
|  is_conversion |  string  |
+----------------+----------+
----

Here we start to see the real flaw if we just rely on inferred schemas.
Holding a currency as a string?
Wat.
Storing a timestamp as a string?
Gross.
Using a string to hold a boolean?
Fine, until someone decides to put a value other than `true` or `false` in it. Or `True`. Or `TRuE`. And so on.

Data types exist for a reason, and part of good data pipeline hygiene is making use of them.

=== Enough of the lecturing…How do I use an explicit schema with Kafka Connect?

==== JSON with embedded schema

On option is https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/#json-schemas[embedding the schema directly in the message].
This is actually what the `JsonConverter` was defaulting to in the first example above and through an error because we'd not done it.
Here's what the above `clicks` record looks like with embedded schema:

[source,javascript]
----
{
  "schema": {
    "type": "struct",
    "fields": [
      {
        "field": "click_ts",
        "type": "int64",
        "name": "org.apache.kafka.connect.data.Timestamp",
        "version": 1,
        "optional": false
      },
      {
        "field": "ad_cost",
        "type": "bytes",
        "name": "org.apache.kafka.connect.data.Decimal",
        "version": 1,
        "parameters": {
          "scale": "2"
        },
        "optional": false
      },
      {
        "field": "is_conversion",
        "type": "boolean",
        "optional": false
      },
      {
        "field": "user_id",
        "type": "string",
        "optional": false
      }
    ]
  },
  "payload": {
    "click_ts": 1675258225000,
    "ad_cost": "1.50",
    "is_conversion": true,
    "user_id": "001234567890"
  }
}
----

Even though our Kafka Connect worker is defaulting to using it, I'm going to explicitly configure `schemas.enable` just for clarity:

[source,bash]
----
kcctl apply -f - <<EOF
{
  "name": "iceberg-sink-kc_clicks_schema",
  "config": {
    "connector.class": "io.tabular.iceberg.connect.IcebergSinkConnector",
    "topics": "clicks_with_schema",
    "iceberg.tables": "rmoff_db.clicks_embedded_schema",
    "iceberg.tables.auto-create-enabled": "true",
    "iceberg.catalog.catalog-impl": "org.apache.iceberg.aws.glue.GlueCatalog",
    "iceberg.catalog.warehouse": "s3://rmoff-lakehouse/01/",
    "iceberg.catalog.io-impl": "org.apache.iceberg.aws.s3.S3FileIO",
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "key.converter.schemas.enable": "true",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": "true",
    "iceberg.control.commit.interval-ms": "30000"
  }
}
EOF
----

The first time I try it, it fails:

[source,bash]
----
org.apache.kafka.connect.errors.DataException: Invalid bytes for Decimal field
com.fasterxml.jackson.databind.exc.InvalidFormatException: Cannot access contents of TextNode as binary due to broken Base64 encoding: Illegal character '.' (code 0x2e) in base64 content
----

That's because the `ad_cost` field is defined as a logical `Decimal` type, but physical stored as `bytes`, so I need to write it as that in the topic:

[source,javascript]
----
[…]
  },
  "payload": {
    "click_ts": 1675258225000,
    "ad_cost": "AJY=", <1>
    "is_conversion": true,
    "user_id": "001234567890"
  }
}
----
[NOTE]
====
Where on earth do I get `AJY=` from?
I'll let https://claude.ai/[Claude] explain:

For decimal 1.50 with scale 2, we need to ensure proper signed integer encoding:

. *Unscale*: 1.50 × 10² = 150
. *Convert to signed bytes*: 150 as a positive integer needs to be encoded as `[0, 150]` (2 bytes) or use proper big-endian encoding
. *Base64 encode*: The bytes `[0, 150]` encode to `"AJY="`
====

With the connector restarted reading from a fresh topic with this newly-encoded decimal value in it, things look good in Iceberg:

[source,sql]
----
🟡◗ SELECT * FROM clicks_embedded_schema;
┌──────────────────────────┬───────────────┬───────────────┬──────────────┐
│         click_ts         │    ad_cost    │ is_conversion │   user_id    │
│ timestamp with time zone │ decimal(38,2) │    boolean    │   varchar    │
├──────────────────────────┼───────────────┼───────────────┼──────────────┤
│ 2023-02-01 13:30:25+00   │          1.50 │ true          │ 001234567890 │ <1>
----
<1> Proper data types, yay!

*BUT*…this is a pretty heavy way of doing things.
Bytes might be cheap, but do we really want to spend over 80% of the message on sending the full schema definition with _every single record_?

This is where a Schema Registry comes in.

==== Schema Registry

A schema registry is basically what it says on the tin.
A registry, of schemas.

Instead of passing the full schema each time (like above), a client will have a _reference_ to the schema in the message, and then retrieve the actual schema from the registry.

The most well known of the schema registries in the Kafka ecosystem is Confluent's https://github.com/confluentinc/schema-registry[Schema Registry].
I'll show you shortly how it is used automatically within a pipeline, but first I'm going to demonstrate it's "manual" use.

The schema from the `clicks` above looks like this:

[source,javascript]
----
{
  "type": "record",
  "name": "ClickEvent",
  "fields": [
    {
      "name": "click_ts",
      "type": { "type": "long", "logicalType": "timestamp-millis" }
    },
    {
      "name": "ad_cost",
      "type": { "type": "bytes", "logicalType": "decimal", "precision": 10, "scale": 2 }
    },
    {
      "name": "is_conversion",
      "type": "boolean"
    },
    {
      "name": "user_id",
      "type": "string"
    }
  ]
}"
----

Send this to Schema Registry using the REST API:

[source,bash]
----
$ http POST localhost:8081/subjects/clicks-value/versions \
  Content-Type:application/vnd.schemaregistry.v1+json \
  schema='{"type":"record","name":"ClickEvent","fields":[{"name":"click_ts","type":{"type":"long","logicalType":"timestamp-millis"}},{"name":"ad_cost","type":{"type":"bytes","logicalType":"decimal","precision":10,"scale":2}},{"name":"is_conversion","type":"boolean"},{"name":"user_id","type":"string"}]}'
----

This will return the ID that the schema has been assigned.

Now send the message to Kafka, specifying `value.schema.id` as the schema ID returned in the step above:

[source,bash]
----
$ printf '{"click_ts": 1675258225000, "ad_cost": "1.50", "is_conversion": true, "user_id": "001234567890"}\n' | \
    docker compose exec -T kafka-connect kafka-avro-console-producer \
                        --bootstrap-server broker:9092 \
                        --topic clicks_registry \
                        --property schema.registry.url=http://schema-registry:8081 \
                        --property value.schema.id=1
----

What we now have is a Kafka topic with a message that holds _just_ the payload plus a _pointer_ to the schema.
It's the best of both worlds; a small message footprint, but a fully-defined schema available for any consumer to use.

[NOTE]
====
An Avro-serialised message is smaller than a JSON one holding the same data:

[source,bash]
----
$ docker compose exec -T kcat kcat -C -b broker:9092 -t clicks_registry
Ѕ1.50001234567890

$ docker compose exec -T kcat kcat -C -b broker:9092 -t clicks_registry -c1 | wc -c
31

$ docker compose exec -T kcat kcat -C -b broker:9092 -t clicks
{"click_ts": "2023-02-01T14:30:25Z", "ad_cost": "1.50", "is_conversion": "true", "user_id": "001234567890"}

$ docker compose exec -T kcat kcat -C -b broker:9092 -t clicks -c1 | wc -c
108
----
====

Let's finish off by sending this Avro data over to Iceberg:

[source,bash]
----
$ kcctl apply -f - <<EOF
{
  "name": "iceberg-sink-clicks-registry",
  "config": {
    "connector.class": "io.tabular.iceberg.connect.IcebergSinkConnector",
    "topics": "clicks_registry",
    "iceberg.tables": "rmoff_db.clicks_schema_registry",
    "iceberg.tables.auto-create-enabled": "true",
    "iceberg.catalog.catalog-impl": "org.apache.iceberg.aws.glue.GlueCatalog",
    "iceberg.catalog.warehouse": "s3://rmoff-lakehouse/01/",
    "iceberg.catalog.io-impl": "org.apache.iceberg.aws.s3.S3FileIO",
    "key.converter": "io.confluent.connect.avro.AvroConverter",
    "key.converter.schema.registry.url": "http://schema-registry:8081",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter.schema.registry.url": "http://schema-registry:8081",
    "iceberg.control.commit.interval-ms": "30000"
  }
}
EOF
----

The data lands in Iceberg with its data types looking good:

[source,sql]
----
🟡◗ SELECT * FROM clicks_schema_registry;
┌──────────────────────────┬───────────────┬───────────────┬──────────────┐
│         click_ts         │    ad_cost    │ is_conversion │   user_id    │
│ timestamp with time zone │ decimal(38,2) │    boolean    │   varchar    │
├──────────────────────────┼───────────────┼───────────────┼──────────────┤
│ 2023-02-01 13:30:25+00   │    8251118.56 │ true          │ 001234567890 │
----

But…what's this?
For some reason `ad_cost` is `8251118.56` even though the source data was `1.50`.

.Decimals…again
[NOTE]
====
Similar to the `Decimal` issue above when I embedded the schema in a JSON message, providing a decimal value in Avro also requires special attention.
In this case it's the Kafka producer that I'm using that needs to be persuaded to serialise it correctly.
This time I'll let Gemini explain:

To represent the decimal `1.50` for a `bytes` field with a `Decimal` logical type and a scale of 2, you must provide the value as `{"ad_cost": "\\u0000\\u0096"}`. Here's why:

* **Unscaled Integer**: The `Decimal` logical type is stored as a raw `bytes` array representing an unscaled integer. For a value of `1.50` and a `scale` of `2`, the unscaled integer is `1.50 * 10^2 = 150`.
* **Signed Bytes**: Avro's decimal representation uses signed, big-endian bytes. The integer `150` is `0x96` in hexadecimal. However, a single byte `0x96` has its most significant bit set, causing it to be interpreted as a negative number in two's complement.
* **Positive Number Padding**: To ensure the number is treated as positive `150`, a `0x00` padding byte must be prepended, resulting in the two-byte sequence `[0x00, 0x96]`.
* **JSON String Encoding**: The `kafka-avro-console-producer` requires this byte sequence to be provided as a JSON string using unicode escapes, which is `"\u0000\u0096"`.
* **Shell Escaping**: Your shell will interpret and consume the single backslashes. To pass the literal escape sequences to the producer's JSON parser, you must escape the backslashes themselves, resulting in `{"ad_cost": "\\u0000\\u0096"}`.
====

With the serialisation of the decimal value corrected thus:

[source,bash]
----
printf '{"click_ts": 1675258225000, "ad_cost": "\\u0000\\u0096" ,"is_conversion": true, "user_id": "001234567890"}\n' | \
    docker compose exec -T kafka-connect kafka-avro-console-producer \
                        --bootstrap-server broker:9092 \
                        --topic clicks_registry \
                        --property schema.registry.url=http://schema-registry:8081 \
                        --property value.schema.id=1
----

I finally got the expected value showing in Iceberg:

[source,sql]
----
🟡◗ SELECT * FROM clicks_schema_registry;
┌──────────────────────────┬───────────────┬───────────────┬──────────────┐
│         click_ts         │    ad_cost    │ is_conversion │   user_id    │
│ timestamp with time zone │ decimal(38,2) │    boolean    │   varchar    │
├──────────────────────────┼───────────────┼───────────────┼──────────────┤
│ 2023-02-01 13:30:25+00   │          1.50 │ true          │ 001234567890 │
----

== Postgres to Iceberg via Kafka Connect

Let's put this into practice now.
I'm going to use Kafka Connect with the Debezium connector to get data from Postgres and then write it to Iceberg with the same sink connector we've used above.

First, create and populate Postgres table:

[source,sql]
----
CREATE TABLE clicks (
    click_ts TIMESTAMP WITH TIME ZONE,
    ad_cost DECIMAL(38,2),
    is_conversion BOOLEAN,
    user_id VARCHAR
);

INSERT INTO clicks (click_ts, ad_cost, is_conversion, user_id)
    VALUES ('2023-02-01 13:30:25+00', 1.50, true, '001234567890');
----

Then check we've got the Debezium connector installed on our Kafka Connect worker:

[source,bash]
----
$ kcctl get plugins --types=source

 TYPE     CLASS                                                       VERSION
 source   io.debezium.connector.postgresql.PostgresConnector          3.1.2.Final
----

and create a Debezium source connector:

[source,bash]
----
$ kcctl apply -f - <<EOF
{
  "name": "postgres-clicks-source",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "database.hostname": "postgres",
    "database.port": "5432",
    "database.user": "postgres",
    "database.password": "Welcome123",
    "database.dbname": "postgres",
    "table.include.list": "public.clicks",
    "topic.prefix": "dbz"
  }
}
EOF
----

Using kcctl we can see that the connector is running, and writing data to a topic:

[source,bash]
----
$ kcctl describe connector postgres-clicks-source
Name:       postgres-clicks-source
Type:       source
State:      RUNNING
[…]
Topics:
  dbz.public.clicks
----

If we take a look at the topic we can quickly see a mistake I've made in the configuration of the connector:

[source,bash]
----
$  docker compose exec -T kcat kcat -b broker:9092 -C -t dbz.public.clicks -c1

{"schema":{"type":"struct","fields":[{"type":"struct","fields":[{"type":"string","optional":true,"name":"io.debezium.time.ZonedTimestamp","version":1,"field":"click_ts"},{"type":"bytes","optional":true,"name":"org.apache.kafka.connect.data.Decimal","version":1,"parameters":{"scale":"2","connect.decimal.precision":"38"},"field":"ad_cost"},{"type":"boolean","optional":true,"field":"is_conversion"},{"type":"string","optional":true,"field":"user_id"}],"optional":true,"name":"dbz.public.clicks.Value","field":"before"},{"type":"struct","fields":[{"type":"string","optional":true,"name":"io.debezium.time.ZonedTimestamp","version":1,"field":"click_ts"},{"type":"bytes","optional":true,"name
[…]
----

It's using the `JsonConverter` with an embeded schema.
That's not what we want, so let's create a new version of the connector.
It's important to create a new version, because the existing one won't re-read messages from the topic just because we change its configuration because logically it has processed that data already.
We also need to make sure we write to a different topic; writing JSON and Avro to the same Kafka topic is a recipe for distaster (or at least, wailing and gnashing of teeth when a sink connector spectacularly fails to read the messages).

[source,bash]
----
$ kcctl delete connector postgres-clicks-source

$ kcctl apply -f - <<EOF
{
  "name": "postgres-clicks-source-avro",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "database.hostname": "postgres",
    "database.port": "5432",
    "database.user": "postgres",
    "database.password": "Welcome123",
    "database.dbname": "postgres",
    "table.include.list": "public.clicks",
    "topic.prefix": "dbz-avro",
    "key.converter": "io.confluent.connect.avro.AvroConverter",
    "key.converter.schema.registry.url": "http://schema-registry:8081",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter.schema.registry.url": "http://schema-registry:8081"
  }
}
EOF
----

Now we can see the Avro data in the topic:

[source,bash]
----
$ docker compose exec -T kcat kcat -b broker:9092 -C -t dbz-avro.public.clicks -c1

62023-02-01T13:30:25.000000Z0012345678903.1.2.Finalpostgresqldbz-avroe
firstpostgres"[null,"34511440"]Ђӻ0
                                  public
                                        clicks
                                               reʷӻ0
----

To deserialise it correctly we use `-s avro` as above, and we see that https://debezium.io/documentation/reference/stable/transformations/event-flattening.html#_change_event_structure[the payload from Debezium] is more complex than a simple message:

[source,bash]
----
$ docker compose exec -T kcat kcat -C -b broker:9092 -t dbz-avro.public.clicks \
                        -s avro -r http://schema-registry:8081 -c1 | jq '.'
----

[source,javascript]
----
{
  "before": null,
  "after": {
    "Value": {
      "click_ts": {
        "string": "2023-02-01T13:30:25.000000Z"
      },
      "ad_cost": {
        "bytes": ""
      },
      "is_conversion": {
        "boolean": true
      },
      "user_id": {
        "string": "001234567890"
      }
    }
  },
  "source": {
    "version": "3.1.2.Final",
    "connector": "postgresql",
    "name": "dbz-avro",
    "ts_ms": 1751447315595,
    "snapshot": {
      "string": "first"
    },
    "db": "postgres",
    […]
----

Debezium, and any good CDC tool in general, doesn't just capture the current state of a row; it captures _changes_.
Since this is the initial snapshot, we have a blank `before` section, the payload in `after` (i.e. current state), and then some metadata (`source`).

You _might_ want all of this raw change data sent to Iceberg, but more likely is that you just want the current state of the record.
To do this you can use a Kafka Connect Single Message Transformation (SMT).
Both Iceberg and Debezium ship with their own SMTs to do this.
Iceberg has https://iceberg.apache.org/docs/nightly/kafka-connect/#debeziumtransform[`DebeziumTransform`] and Debezium  https://debezium.io/documentation/reference/stable/transformations/event-flattening.html[`ExtractNewRecordState`].
The differences between them that I can tell are:

* The Iceberg one is marked experimental, whilst the Debezium one has been used in production for years
* The Iceberg one adds CDC metadata fields (operation type, offset, etc) along with the record state, whilst to do this with the Debezium one you'd need to chain several other SMTs for the same effect.

Let's try the Iceberg one, which we'll configure as part of the new sink connector itself:

[source,bash]
----
$ kcctl apply -f - <<EOF
{
  "name": "iceberg-sink-postgres-clicks",
  "config": {
    "connector.class": "io.tabular.iceberg.connect.IcebergSinkConnector",
    "topics": "dbz-avro.public.clicks",
    "iceberg.tables": "rmoff_db.postgres_clicks",
    "iceberg.tables.auto-create-enabled": "true",
    "iceberg.catalog.catalog-impl": "org.apache.iceberg.aws.glue.GlueCatalog",
    "iceberg.catalog.warehouse": "s3://rmoff-lakehouse/01/",
    "iceberg.catalog.io-impl": "org.apache.iceberg.aws.s3.S3FileIO",
    "key.converter": "io.confluent.connect.avro.AvroConverter",
    "key.converter.schema.registry.url": "http://schema-registry:8081",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter.schema.registry.url": "http://schema-registry:8081",
    "iceberg.control.commit.interval-ms": "30000",
    "transforms": "dbz",
    "transforms.dbz.type": "io.tabular.iceberg.connect.transforms.DebeziumTransform"
  }
}
EOF
----

Here's the resulting Iceberg table:

[source,sql]
----
🟡◗ describe postgres_clicks;
┌───────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────┬
│  column_name  │                                           column_type                                            │
│    varchar    │                                             varchar                                              │
├───────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────┼
│ click_ts      │ VARCHAR                                                                                          │
│ ad_cost       │ DECIMAL(38,2)                                                                                    │
│ is_conversion │ BOOLEAN                                                                                          │
│ user_id       │ VARCHAR                                                                                          │
│ _cdc          │ STRUCT(op VARCHAR, ts TIMESTAMP WITH TIME ZONE, "offset" BIGINT, source VARCHAR, target VARCHAR) │
└───────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────┴
----

and data:

[source,sql]
----
🟡◗ SELECT * FROM postgres_clicks;
┌─────────────────────────────┬───────────────┬───────────────┬──────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│          click_ts           │    ad_cost    │ is_conversion │   user_id    │                                                     _cdc                                                     │
│           varchar           │ decimal(38,2) │    boolean    │   varchar    │       struct(op varchar, ts timestamp with time zone, "offset" bigint, source varchar, target varchar)       │
├─────────────────────────────┼───────────────┼───────────────┼──────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ 2023-02-01T13:30:25.000000Z │          1.50 │ true          │ 001234567890 │ {'op': I, 'ts': '2025-07-02 10:08:35.608+01', 'offset': 0, 'source': public.clicks, 'target': public.clicks} │
----

=== Data Type Fun: Timestamps

One data type issue this time—pun intended.
The `click_ts` should be a timestamp, but is showing up as a string in Iceberg.
To understand where this is occurring, I'll start by checking the schema that Debezium wrote to the Schema Registry when it wrote the data to Kafka:

[source,bash]
----
$ http http://localhost:8081/subjects/dbz-avro.public.clicks-value/versions/latest | \
    jq '.schema | fromjson'
----

[source,javascript]
----
[…]
    {
        "name": "click_ts",
        "type": [
        "null",
        {
            "type": "string",
            "connect.version": 1,
            "connect.name": "io.debezium.time.ZonedTimestamp"
        }
        ],
        "default": null
    },
[…]
----

Per https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-basic-types[the docs], it's stored as a `string`, but using the Kafka Connect logical type `io.debezium.time.ZonedTimestamp`.

Let's have a look at the link:/2020/12/17/twelve-days-of-smt-day-8-timestampconverter/[TimestampConverter SMT].
This will hopefully let us convert the `string` type (which holds the timestamp) into a logical `Timestamp` type as part of the sink connector.

[source,bash]
----
$ kcctl apply -f - <<EOF
{
  "name": "iceberg-sink-postgres-clicks-new",
  "config": {
    "connector.class": "io.tabular.iceberg.connect.IcebergSinkConnector",
    "topics": "dbz-avro.public.clicks",
    "iceberg.tables": "rmoff_db.postgres_clicks",
    "iceberg.tables.auto-create-enabled": "true",
    "iceberg.catalog.catalog-impl": "org.apache.iceberg.aws.glue.GlueCatalog",
    "iceberg.catalog.warehouse": "s3://rmoff-lakehouse/01/",
    "iceberg.catalog.io-impl": "org.apache.iceberg.aws.s3.S3FileIO",
    "key.converter": "io.confluent.connect.avro.AvroConverter",
    "key.converter.schema.registry.url": "http://schema-registry:8081",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter.schema.registry.url": "http://schema-registry:8081",
    "iceberg.control.commit.interval-ms": "30000",
    "transforms": "dbz,convert_ts",
    "transforms.dbz.type": "io.tabular.iceberg.connect.transforms.DebeziumTransform",
    "transforms.convert_ts.type" : "org.apache.kafka.connect.transforms.TimestampConverter\$Value",
    "transforms.convert_ts.field" : "click_ts",
    "transforms.convert_ts.format": "yyyy-MM-dd'T'HH:mm:ss.SSSSSS'Z'",
    "transforms.convert_ts.target.type": "Timestamp"
  }
}
EOF
----

NOTE: The order of the transformations is important; for the `convert_ts` transform to work the `click_ts` field needs to have been unnested, which is what the `dbz` transform does first.

With the initial `postgres_clicks` Iceberg table deleted, and the new version of the connector running (so as to make sure that the table gets recreated with-hopefully—the correct schema), we see this in Iceberg:

[source,sql]
----
🟡◗ describe postgres_clicks;
┌───────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────┬─────────┬─────────┬─────────┬─────────┐
│  column_name  │                                           column_type                                            │  null   │   key   │ default │  extra  │
│    varchar    │                                             varchar                                              │ varchar │ varchar │ varchar │ varchar │
├───────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────┼─────────┼─────────┼─────────┼─────────┤
│ click_ts      │ TIMESTAMP WITH TIME ZONE                                                                         │ YES     │ NULL    │ NULL    │ NULL    │
│ ad_cost       │ DECIMAL(38,2)                                                                                    │ YES     │ NULL    │ NULL    │ NULL    │
│ is_conversion │ BOOLEAN                                                                                          │ YES     │ NULL    │ NULL    │ NULL    │
│ user_id       │ VARCHAR                                                                                          │ YES     │ NULL    │ NULL    │ NULL    │
│ _cdc          │ STRUCT(op VARCHAR, ts TIMESTAMP WITH TIME ZONE, "offset" BIGINT, source VARCHAR, target VARCHAR) │ YES     │ NULL    │ NULL    │ NULL    │
└───────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────┴─────────┴─────────┴─────────┴─────────┘

🟡◗ select click_ts, ad_cost, is_conversion, user_id from postgres_clicks2;
┌──────────────────────────┬───────────────┬───────────────┬──────────────┐
│         click_ts         │    ad_cost    │ is_conversion │   user_id    │
│ timestamp with time zone │ decimal(38,2) │    boolean    │   varchar    │
├──────────────────────────┼───────────────┼───────────────┼──────────────┤
│ 2023-02-01 13:30:25+00   │          1.50 │ true          │ 001234567890 │
└──────────────────────────┴───────────────┴───────────────┴──────────────┘
----

Compare this to the Postgres source:

[source,sql]
----
postgres=# \d clicks
                           Table "public.clicks"
    Column     |           Type           | Collation | Nullable | Default
---------------+--------------------------+-----------+----------+---------
 click_ts      | timestamp with time zone |           |          |
 ad_cost       | numeric(38,2)            |           |          |
 is_conversion | boolean                  |           |          |
 user_id       | character varying        |           |          |

postgres=# select * from clicks;
        click_ts        | ad_cost | is_conversion |   user_id
------------------------+---------+---------------+--------------
 2023-02-01 13:30:25+00 |    1.50 | t             | 001234567890
----

Perfect!

[TIP]
====
If you're using `TIMESTAMP` instead of `TIMESTAMP WITH TIME ZONE` in Postgres then Debezium will store this as

[source,javascript]
----
{
    "type": "long",
    "connect.version": 1,
    "connect.name": "io.debezium.time.MicroTimestamp"
}
----

and the Iceberg Kafka Connect sink write it, by default, as a `BIGINT` to Iceberg (matching the `long` logical type in the schema).

You can use the same `TimestampConverter` trick as above, instead specifying `unix.precision` so that the transform treats the source value as an epoch value, converting it into a timestamp:

[source,javascript]
----

----
"transforms.convert_ts.type"          : "org.apache.kafka.connect.transforms.TimestampConverter\$Value",
"transforms.convert_ts.field"         : "click_ts",
"transforms.convert_ts.unix.precision": "microseconds",
"transforms.convert_ts.target.type"   : "Timestamp"
----

The only problem is that this ends up in Iceberg as a `TIMESTAMP WITH TIME ZONE`—i.e. _includes_ time zone, even though the source doesn't.
====


schema evolution
n:1 flow

== Appendix: Debugging

You can increase the log level of the Kafka Connect worker for specific components:

[source,bash]
----
http PUT localhost:8083/admin/loggers/org.apache.iceberg.metrics Content-Type:application/json level=TRACE
http PUT localhost:8083/admin/loggers/org.apache.iceberg.aws Content-Type:application/json level=TRACE
----

See link:/2020/01/16/changing-the-logging-level-for-kafka-connect-dynamically/[Changing the Logging Level for Kafka Connect Dynamically] for more detail.

It can be useful for inspection of SMTs:

[source,bash]
----
curl -s -X PUT http://localhost:8083/admin/loggers/org.apache.kafka.connect.runtime.TransformationChain -H "Content-Type:application/json" -d '{"level": "TRACE"}'
----

You'll then see in the logs something like this:

[source,]
----
Applying transformation io.tabular.iceberg.connect.transforms.DebeziumTransform to
    SinkRecord{kafkaOffset=2, timestampType=CreateTime, originalTopic=dbz-avro.public.clicks_no_tz, originalKafkaPartition=1, originalKafkaOffset=2}
    ConnectRecord{topic='dbz-avro.public.clicks_no_tz', kafkaPartition=1, key=null, keySchema=null,
        value=Struct{after=Struct{click_ts=1675258225000000,ad_cost=1.50,is_conversion=true,user_id=001234567890},source=Struct{version=3.1.2.Final,connector=postgresql,name=dbz-avro,ts_ms=1751471423083,snapshot=false,db=postgres,sequence=["34643256","34643544"],ts_us=1751471423083360,ts_ns=1751471423083360000,schema=public,table=clicks_no_tz,txId=780,lsn=34643544},op=c,ts_ms=1751471423553,ts_us=1751471423553059,ts_ns=1751471423553059129},
        valueSchema=Schema{dbz-avro.public.clicks_no_tz.Envelope:STRUCT}, timestamp=1751471423743, headers=ConnectHeaders(headers=)}


Applying transformation org.apache.kafka.connect.transforms.TimestampConverter$Value to
    SinkRecord{kafkaOffset=2, timestampType=CreateTime, originalTopic=dbz-avro.public.clicks_no_tz, originalKafkaPartition=1, originalKafkaOffset=2}
    ConnectRecord{topic='dbz-avro.public.clicks_no_tz', kafkaPartition=1, key=null, keySchema=null,
        value=Struct{click_ts=1675258225000000,ad_cost=1.50,is_conversion=true,user_id=001234567890,_cdc=Struct{op=I,ts=Wed Jul 02 15:50:23 GMT 2025,offset=2,source=public.clicks_no_tz,target=public.clicks_no_tz}},
        valueSchema=Schema{dbz-avro.public.clicks_no_tz.Value:STRUCT}, timestamp=1751471423743, headers=ConnectHeaders(headers=)}
----


== Appendix: Documentation and Links

* https://github.com/apache/iceberg/tree/main/kafka-connect[Apache Iceberg - Kafka Connect sink]
* https://iceberg.apache.org/docs/nightly/kafka-connect/[Apache Iceberg - Kafka Connect sink docs]
* https://www.confluent.io/hub/tabular/iceberg-kafka-connect[Apache Iceberg sink on Confluent Hub]

== Appendix: Version problems

I saw this error from the connector:

[source,]
----
java.lang.NoSuchMethodError: 'org.apache.kafka.clients.admin.ListConsumerGroupOffsetsOptions org.apache.kafka.clients.admin.ListConsumerGroupOffsetsOptions.requireStable(boolean)'
        at io.tabular.iceberg.connect.channel.CommitterImpl.fetchStableConsumerOffsets(CommitterImpl.java:116)
        at io.tabular.iceberg.connect.channel.CommitterImpl.<init>(CommitterImpl.java:97)
        at io.tabular.iceberg.connect.channel.CommitterImpl.<init>(CommitterImpl.java:70)
        at io.tabular.iceberg.connect.channel.CommitterImpl.<init>(CommitterImpl.java:62)
        at io.tabular.iceberg.connect.channel.TaskImpl.<init>(TaskImpl.java:37)
        at io.tabular.iceberg.connect.IcebergSinkTask.open(IcebergSinkTask.java:56)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.openPartitions(WorkerSinkTask.java:637)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.access$1200(WorkerSinkTask.java:72)
----

This happened with cp-kafka-connect `7.2.15`.
Switching to 8.0.0 resolved the problem.
